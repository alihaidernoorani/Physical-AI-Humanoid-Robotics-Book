---
id: llm-task-decomposition
title: "LLM-Based Task Decomposition"
description: "Natural language understanding and task planning for robotics applications with ROS 2 integration"
personalization: true
translation: ur
learning_outcomes:
  - "Implement natural language understanding for robotic task interpretation"
  - "Design task decomposition algorithms that break complex commands into executable actions"
  - "Create context-aware command interpretation systems for humanoid robots"
  - "Implement error handling and clarification request mechanisms for robust interaction"
software_stack:
  - "Large Language Models (LLM) integration"
  - "ROS 2 Humble Hawksbill (LTS)"
  - "Python 3.10+ with rclpy"
  - "Transformers library for LLM integration"
  - "NVIDIA Isaac ROS Foundation Packages"
  - "OpenAI API or local LLM deployment"
hardware_recommendations:
  - "NVIDIA Jetson AGX Orin for edge LLM deployment"
  - "High-speed internet for cloud-based LLM services"
  - "NVIDIA Jetson Orin Nano for minimal configurations"
  - "Cloud connectivity for LLM services"
hardware_alternatives:
  - "Laptop with cloud access for development"
  - "NVIDIA Jetson Orin Nano with external cloud connectivity"
  - "Simulated environment for development"
prerequisites:
  - "Module 1-4: Complete understanding of ROS 2, simulation, AI, and VLA components"
  - "Module 4.1: Voice-to-text integration concepts"
  - "Basic understanding of natural language processing"
  - "Experience with ROS 2 action servers and services"
assessment_recommendations:
  - "Task decomposition: Complex command breakdown and execution"
  - "Context awareness: Performance evaluation in different environments"
dependencies: ["04-vla/intro", "04-vla/01-voice-to-text-whisper"]
---

import Callout from '@site/src/components/Callout';
import Quiz from '@site/src/components/Quiz';
import Exercise from '@site/src/components/Exercise';
import Diagram from '@site/src/components/Diagram';
import Toggle from '@site/src/components/Toggle';

# LLM-Based Task Decomposition

## Learning Objectives

- Implement natural language understanding for robotic task interpretation
- Design task decomposition algorithms that break complex commands into executable actions
- Create context-aware command interpretation systems for humanoid robots
- Implement error handling and clarification request mechanisms for robust interaction

## Natural Language Understanding for Robotics

Natural Language Understanding (NLU) for robotics involves the interpretation of human commands. This occurs in the context of robot capabilities and environmental constraints. For humanoid robots, this requires specialized processing. This connects linguistic concepts to physical actions, spatial relationships, and object affordances. The system must understand both the literal meaning of commands and the implied intentions behind them (Brohan & Wyatt, 2020).

<Callout type="note" title="NLU for Robotics">
Natural Language Understanding for robotics connects linguistic concepts to physical actions, spatial relationships, and object affordances, requiring specialized processing that understands both literal meaning and implied intentions.
</Callout>

Semantic parsing in robotic NLU converts natural language commands into structured representations. These can be processed by the robot's planning and execution systems. For humanoid robots, this involves mapping linguistic elements to robot-specific concepts. These include navigation goals, manipulation targets, and environmental objects. The parsing must handle the ambiguity and variability inherent in natural language. This maintains precision for robot execution (Huang et al., 2022).

<Diagram
  title="Semantic Parsing in Robotics"
  description="Diagram showing the conversion of natural language commands to structured robot-executable representations"
  caption="Semantic parsing converts natural language commands into structured representations for robot execution"
/>

Ontology-based understanding provides structured knowledge about the robot's environment, capabilities, and the relationships between objects and actions. For humanoid robots, this includes knowledge about object affordances (what can be done with objects), spatial relationships (where objects are located and how to navigate to them), and task constraints (what actions are possible given the robot's physical limitations) (Zhang et al., 2023).

<Exercise
  title="Ontology Implementation"
  problem="Implement an ontology-based understanding system for a humanoid robot that connects linguistic concepts to robot capabilities."
  hints={["Define object affordances and relationships", "Create knowledge base of robot capabilities", "Implement semantic mapping between language and actions"]}
  solution={`# Example ontology-based understanding system
class RobotOntology:
    def __init__(self):
        # Define object affordances
        self.affordances = {
            "cup": ["grasp", "lift", "carry", "place"],
            "book": ["grasp", "lift", "carry", "place", "read"],
            "chair": ["move", "push", "pull", "sit_on"],
            "table": ["place_on", "wipe", "clean"]
        }

        # Define spatial relationships
        self.spatial_relations = {
            "on": "supports",
            "in": "contains",
            "near": "close_to",
            "beside": "adjacent_to",
            "under": "below"
        }

        # Define robot capabilities
        self.capabilities = {
            "navigation": ["move_to", "go_to", "navigate"],
            "manipulation": ["grasp", "lift", "place", "carry"],
            "perception": ["see", "detect", "recognize", "find"]
        }

    def interpret_command(self, command, environment_objects):
        # Parse the command and match to ontology
        tokens = command.lower().split()

        # Identify object and action
        object_name = self.find_object_in_environment(command, environment_objects)
        action = self.find_action_in_command(command)

        if object_name and action:
            if self.can_perform_action(object_name, action):
                return {
                    "action": action,
                    "object": object_name,
                    "feasible": True
                }
            else:
                return {
                    "action": action,
                    "object": object_name,
                    "feasible": False,
                    "reason": f"Cannot {action} {object_name}"
                }

        return {"feasible": False, "reason": "Could not parse command"}

    def can_perform_action(self, object_name, action):
        if object_name in self.affordances:
            return action in self.affordances[object_name]
        return False

    def find_object_in_environment(self, command, environment_objects):
        # Find objects mentioned in the command that exist in environment
        for obj in environment_objects:
            if obj.lower() in command.lower():
                return obj
        return None

    def find_action_in_command(self, command):
        # Extract action from command
        for action_list in self.capabilities.values():
            for action in action_list:
                if action in command.lower():
                    return action
        return None`}
/>

Symbol grounding connects abstract linguistic concepts to concrete robot perceptions and actions. For humanoid robots, this means that when a command mentions "the red cup," the system must connect this linguistic reference to a specific object in the robot's visual field. The grounding process must handle uncertainty and ambiguity in both perception and language (Radford et al., 2022).

<Quiz
  question="What is the primary purpose of symbol grounding in robotic NLU?"
  options="To improve speech recognition accuracy||To connect abstract linguistic concepts to concrete robot perceptions and actions||To enhance visual perception capabilities||To optimize robot movement speed"
  correctAnswer="To connect abstract linguistic concepts to concrete robot perceptions and actions"
  explanation="Symbol grounding connects abstract linguistic concepts to concrete robot perceptions and actions, such as connecting 'the red cup' in a command to a specific object in the robot's visual field."
/>

### Concrete Examples
- Example: Human says "Bring me the red cup" - NLU system parses command and identifies cup
- Example: Robot grounds "red cup" to specific visual object in its field of view

## Task Planning and Decomposition

Task decomposition involves breaking down high-level natural language commands into sequences of lower-level actions. These can be executed by the robot's action servers. For example, a command like "Clean the room" might be decomposed into navigation to specific locations, object identification and manipulation, and cleaning actions. The decomposition process must consider the robot's capabilities, environmental constraints, and safety requirements (Brohan & Wyatt, 2020).

<Callout type="tip" title="Task Decomposition">
Task decomposition breaks high-level natural language commands into sequences of lower-level actions that can be executed by the robot's action servers, considering capabilities, constraints, and safety requirements.
</Callout>

Hierarchical task planning creates multi-level action hierarchies. These allow complex tasks to be decomposed into manageable subtasks. For humanoid robots, this might involve high-level goals (clean the room), mid-level tasks (pick up trash, wipe surfaces), and low-level actions (navigate to location, grasp object). The hierarchy enables flexible execution and error recovery (Huang et al., 2022).

<Diagram
  title="Hierarchical Task Planning"
  description="Diagram showing task decomposition hierarchy from high-level commands to low-level actions"
  caption="Hierarchical task planning showing decomposition from high-level commands to low-level actions"
/>

Constraint-based decomposition ensures that task sequences respect physical, temporal, and safety constraints. For humanoid robots, this includes considerations such as the robot's reach limits, balance constraints during manipulation, and safety requirements for human-robot interaction. The decomposition must generate feasible action sequences. These can be executed safely (Zhang et al., 2023).

<Exercise
  title="Constraint-Based Task Planning"
  problem="Implement a constraint-based task decomposition system that respects the robot's physical limitations and safety requirements."
  hints={["Define robot constraints (reach limits, balance, safety)", "Implement constraint checking during task decomposition", "Generate feasible action sequences"]}
  solution={`# Example constraint-based task planning
class ConstraintBasedPlanner:
    def __init__(self):
        # Define robot constraints
        self.reach_limits = {
            "max_distance": 1.0,  # meters
            "max_height": 1.5,    # meters
            "min_height": 0.2     # meters
        }

        self.balance_constraints = {
            "max_object_weight": 2.0,  # kg
            "center_of_mass_limit": 0.1  # meters from base
        }

        self.safety_constraints = {
            "min_distance_from_human": 0.5,  # meters
            "max_speed_near_human": 0.3      # m/s
        }

    def decompose_task(self, high_level_command, environment_state):
        # Decompose high-level command into subtasks
        subtasks = self.parse_high_level_command(high_level_command)

        # Validate each subtask against constraints
        feasible_subtasks = []
        for subtask in subtasks:
            if self.is_feasible(subtask, environment_state):
                feasible_subtasks.append(subtask)
            else:
                # Generate alternative or request clarification
                alternative = self.generate_alternative(subtask, environment_state)
                if alternative and self.is_feasible(alternative, environment_state):
                    feasible_subtasks.append(alternative)

        return feasible_subtasks

    def is_feasible(self, subtask, environment_state):
        # Check various constraints
        if subtask['type'] == 'navigation':
            return self.check_navigation_constraints(subtask, environment_state)
        elif subtask['type'] == 'manipulation':
            return self.check_manipulation_constraints(subtask, environment_state)
        elif subtask['type'] == 'safety':
            return self.check_safety_constraints(subtask, environment_state)

        return True

    def check_navigation_constraints(self, subtask, environment_state):
        # Check if navigation target is reachable
        target_pos = subtask['target_position']
        robot_pos = environment_state['robot_position']

        distance = self.calculate_distance(robot_pos, target_pos)
        return distance <= self.reach_limits['max_distance']

    def check_manipulation_constraints(self, subtask, environment_state):
        # Check if manipulation is physically possible
        if 'object_weight' in subtask:
            if subtask['object_weight'] > self.balance_constraints['max_object_weight']:
                return False

        # Check if object is within reach
        if 'object_position' in subtask:
            robot_pos = environment_state['robot_position']
            obj_pos = subtask['object_position']
            distance = self.calculate_distance(robot_pos, obj_pos)
            return distance <= self.reach_limits['max_distance']

        return True

    def check_safety_constraints(self, subtask, environment_state):
        # Check safety constraints
        humans_nearby = environment_state.get('humans_nearby', [])
        if humans_nearby:
            for human in humans_nearby:
                if self.calculate_distance(subtask['target_position'], human['position']) < self.safety_constraints['min_distance_from_human']:
                    return False
        return True

    def calculate_distance(self, pos1, pos2):
        # Calculate Euclidean distance between two positions
        import math
        return math.sqrt(sum((a - b) ** 2 for a, b in zip(pos1, pos2)))`}
/>

Plan validation and simulation verify that the decomposed task sequences are executable and safe before execution. For humanoid robots, this may involve simulating the action sequence in a virtual environment. This uses kinematic models to verify that the planned actions are physically possible. The validation process helps prevent execution failures and safety violations (Radford et al., 2022).

### Diagram Descriptions
- Diagram: Constraint-based planning with reach limits, balance, and safety considerations

### Concrete Examples
- Example: "Clean the room" decomposed into navigation, object detection, and manipulation tasks
- Example: Plan validation checking if robot can physically reach objects before execution

<Quiz
  question="What is the primary purpose of plan validation in task decomposition systems?"
  options={[
    "To improve speech recognition",
    "To verify that decomposed task sequences are executable and safe before execution",
    "To enhance visual perception",
    "To optimize robot power consumption"
  ]}
  correctAnswer="To verify that decomposed task sequences are executable and safe before execution"
  explanation="Plan validation and simulation verify that the decomposed task sequences are executable and safe before execution, helping prevent failures and safety violations."
/>

## Context-aware Command Interpretation

Context-aware interpretation considers the robot's current state, environment, and task history when processing commands. For humanoid robots, this includes the robot's current location, the objects visible in the environment, and the progress of ongoing tasks. The context enables more accurate interpretation of ambiguous commands and more natural interaction (Brohan & Wyatt, 2020).

<Callout type="warning" title="Context Awareness">
Context-aware interpretation considers the robot's current state, environment, and task history, enabling more accurate interpretation of ambiguous commands and more natural interaction with humans.
</Callout>

Spatial context understanding enables the robot to interpret location references such as "over there" or "near the table." This occurs based on the robot's current perception of the environment. For humanoid robots, this requires integration of spatial reasoning with natural language processing. This connects linguistic spatial references to geometric locations in the robot's coordinate system (Huang et al., 2022).

<Diagram
  title="Spatial Context Understanding"
  description="Diagram showing spatial context connecting linguistic references to geometric locations"
  caption="Spatial context connecting linguistic references to geometric locations in robot's coordinate system"
/>

Temporal context maintains awareness of time-dependent aspects of commands and the sequence of actions. For humanoid robots, this includes understanding temporal references like "before lunch" or "when you finish cleaning." This maintains context across multiple interactions in a task sequence. The temporal context enables more natural and flexible command interpretation (Zhang et al., 2023).

<Exercise
  title="Context-Aware Interpretation System"
  problem="Implement a context-aware command interpretation system that uses the robot's current state and environment."
  hints={["Maintain robot state and environment context", "Implement spatial reasoning for location references", "Track task history and temporal context"]}
  solution={`# Example context-aware interpretation system
class ContextAwareInterpreter:
    def __init__(self):
        self.current_state = {
            'location': None,
            'orientation': None,
            'carrying_object': None,
            'task_progress': {}
        }
        self.environment = {
            'objects': [],
            'humans': [],
            'landmarks': {}
        }
        self.task_history = []

    def interpret_command(self, command, current_state, environment):
        # Update context
        self.current_state = current_state
        self.environment = environment

        # Parse command with context
        parsed_command = self.parse_with_context(command)

        # Resolve spatial references
        resolved_command = self.resolve_spatial_references(parsed_command)

        # Apply temporal context
        final_command = self.apply_temporal_context(resolved_command)

        return final_command

    def resolve_spatial_references(self, command):
        # Resolve ambiguous spatial references like "over there"
        if "over there" in command:
            # Find the most salient object in the robot's field of view
            salient_object = self.find_salient_object()
            if salient_object:
                command = command.replace("over there", f"at {salient_object['name']}")

        if "near" in command:
            # Resolve "near" references based on current environment
            for landmark_name, landmark_pos in self.environment['landmarks'].items():
                if landmark_name in command:
                    # Calculate relative position
                    robot_pos = self.current_state['location']
                    relative_pos = self.calculate_relative_position(robot_pos, landmark_pos)
                    command = command.replace(f"near {landmark_name}", f"{relative_pos['distance']:.2f}m {relative_pos['direction']} of {landmark_name}")

        return command

    def find_salient_object(self):
        # Find the most visually salient object in the environment
        objects = self.environment['objects']
        if not objects:
            return None

        # For simplicity, return the closest object
        # In practice, this would involve more sophisticated saliency computation
        robot_pos = self.current_state['location']
        closest_obj = min(objects, key=lambda obj: self.distance(robot_pos, obj['position']))
        return closest_obj

    def apply_temporal_context(self, command):
        # Apply temporal context to resolve time-dependent references
        if "when you finish" in command:
            # Check current task progress
            current_task = self.current_state.get('current_task')
            if current_task:
                command = command.replace("when you finish", f"after completing {current_task}")

        return command

    def distance(self, pos1, pos2):
        import math
        return math.sqrt(sum((a - b) ** 2 for a, b in zip(pos1, pos2)))`}
/>

Social context considers the presence and behavior of humans in the environment when interpreting commands. For humanoid robots operating in human environments, this includes understanding commands that reference specific people ("bring John his coffee"). These should be executed with consideration for human activities and preferences (Radford et al., 2022).

### Diagram Descriptions
- Diagram: Context-aware interpretation showing current state, environment, and task history

### Concrete Examples
- Example: Human says "pick up that" - robot uses spatial context to identify specific object
- Example: Robot considers ongoing tasks when interpreting new commands in sequence

<Quiz
  question="What does spatial context understanding enable for humanoid robots?"
  options={[
    "Better speech recognition",
    "Interpretation of location references based on current perception of the environment",
    "Faster movement capabilities",
    "Reduced power consumption"
  ]}
  correctAnswer="Interpretation of location references based on current perception of the environment"
  explanation="Spatial context understanding enables the robot to interpret location references such as 'over there' or 'near the table' based on the robot's current perception of the environment."
/>

## Error Handling and Clarification Requests

Robust error handling in LLM-based task decomposition systems must address multiple types of failures. These include language understanding errors, task planning failures, and execution failures. For humanoid robots, the error handling system must be able to recover gracefully from failures. This maintains safe operation throughout the process (Brohan & Wyatt, 2020).

<Callout type="danger" title="Error Handling">
Robust error handling must address language understanding errors, task planning failures, and execution failures, with graceful recovery while maintaining safe operation for humanoid robots.
</Callout>

Clarification request generation enables the robot to ask for additional information. This occurs when commands are ambiguous or when environmental conditions are unclear. For humanoid robots, this includes asking questions like "Which book do you mean?" or "Should I wait until you move?" The clarification system must determine when additional information is needed. This asks for it in a natural way (Huang et al., 2022).

<Diagram
  title="Clarification Request System"
  description="Diagram showing the clarification request system with natural language interaction"
  caption="Clarification request system with natural language interaction for ambiguous commands"
/>

Fallback mechanisms provide alternative execution strategies. This occurs when primary task decomposition fails. For humanoid robots, this might involve simplifying complex commands, executing partial tasks, or requesting human assistance. The fallback system must maintain safety. This provides the best possible service given the limitations (Zhang et al., 2023).

<Exercise
  title="Error Handling and Fallback System"
  problem="Implement an error handling system with clarification requests and fallback mechanisms for ambiguous commands."
  hints={["Detect ambiguous or unclear commands", "Generate appropriate clarification questions", "Implement fallback strategies for failed tasks"]}
  solution={`# Example error handling and clarification system
class ErrorHandlingSystem:
    def __init__(self):
        self.ambiguity_threshold = 0.7  # Confidence threshold below which clarification is needed
        self.known_objects = set()  # Objects robot knows about
        self.known_people = set()   # People robot knows about

    def process_command(self, command, context):
        # Analyze command for potential ambiguities
        analysis = self.analyze_command(command, context)

        if analysis['confidence'] < self.ambiguity_threshold:
            clarification_needed = self.identify_clarification_needs(analysis)
            if clarification_needed:
                question = self.generate_clarification_request(clarification_needed)
                return {
                    'status': 'needs_clarification',
                    'question': question,
                    'original_command': command
                }

        # If command is clear, proceed with normal processing
        try:
            task_plan = self.decompose_task(command, context)
            return {
                'status': 'success',
                'task_plan': task_plan
            }
        except Exception as e:
            # Handle task decomposition failure
            fallback_result = self.handle_decomposition_failure(command, context, e)
            return fallback_result

    def analyze_command(self, command, context):
        # Analyze command for ambiguities
        tokens = command.lower().split()

        # Check for ambiguous references
        ambiguous_refs = []
        for token in tokens:
            if token in ['it', 'that', 'this', 'there']:
                ambiguous_refs.append(token)

        # Check for object references without clear identification
        potential_objects = [word for word in tokens if word in self.known_objects]
        if len(potential_objects) > 1:
            ambiguous_refs.extend(potential_objects)

        # Estimate confidence based on ambiguities found
        confidence = 1.0 - (len(ambiguous_refs) * 0.1)  # Simple confidence model

        return {
            'ambiguous_refs': ambiguous_refs,
            'confidence': max(0.0, confidence),
            'potential_issues': ambiguous_refs
        }

    def generate_clarification_request(self, clarification_needs):
        # Generate natural language clarification requests
        if 'it' in clarification_needs or 'that' in clarification_needs:
            return "Could you please specify what you mean by 'that' or 'it'?"
        elif 'there' in clarification_needs:
            return "Could you please point to or describe where 'there' is?"
        else:
            # For object ambiguities
            return "Could you please specify which object you mean?"

    def handle_decomposition_failure(self, command, context, error):
        # Implement fallback strategies
        error_msg = str(error).lower()

        if 'navigation' in error_msg or 'path' in error_msg:
            # Fallback for navigation failures
            return {
                'status': 'fallback',
                'strategy': 'simplified_navigation',
                'message': 'I will navigate to the general area instead of the exact location.',
                'simplified_command': self.simplify_navigation_command(command)
            }
        elif 'manipulation' in error_msg or 'grasp' in error_msg:
            # Fallback for manipulation failures
            return {
                'status': 'fallback',
                'strategy': 'simplified_manipulation',
                'message': 'I will attempt a simpler manipulation approach.',
                'simplified_command': self.simplify_manipulation_command(command)
            }
        else:
            # General fallback
            return {
                'status': 'fallback',
                'strategy': 'request_assistance',
                'message': 'I encountered an issue processing your command. Could you please rephrase or simplify it?',
                'original_error': str(error)
            }

    def simplify_navigation_command(self, command):
        # Simplify navigation commands to general areas
        import re
        # Replace specific locations with general ones
        simplified = re.sub(r'to the \w+ \w+', 'to the general area', command)
        return simplified

    def simplify_manipulation_command(self, command):
        # Simplify manipulation commands
        if 'grasp' in command.lower() or 'pick up' in command.lower():
            return command.replace('pick up', 'approach').replace('grasp', 'approach')
        return command`}
/>

Uncertainty quantification helps the system understand and communicate the confidence level of task interpretations and decompositions. For humanoid robots, this enables the system to defer to human operators when uncertainty is high. This proceeds with lower-confidence interpretations when appropriate. The uncertainty management system must balance robustness with responsiveness (Radford et al., 2022).

### Diagram Descriptions
- Diagram: Error handling flow with different failure types and recovery strategies

### Concrete Examples
- Example: Robot asks "Which book do you mean?" when command is ambiguous
- Example: Fallback mechanism simplifying "Clean the entire house" to "Clean this room"

<Quiz
  question="What is the primary purpose of uncertainty quantification in LLM-based task decomposition?"
  options={[
    "To improve computational performance",
    "To understand and communicate confidence levels for task interpretations and enable appropriate responses",
    "To reduce memory usage",
    "To increase network speed"
  ]}
  correctAnswer="To understand and communicate confidence levels for task interpretations and enable appropriate responses"
  explanation="Uncertainty quantification helps the system understand and communicate the confidence level of task interpretations and decompositions, enabling the system to defer to human operators when uncertainty is high."
/>

## Forward References to Capstone Project

The LLM-based task decomposition covered in this chapter is essential. This is for creating the intelligent command understanding system in your Autonomous Humanoid capstone project.

The natural language understanding will enable your robot to interpret complex commands. The task decomposition will break these commands into executable actions. The context-aware interpretation will make interactions more natural and effective. The error handling will ensure robust operation in real-world scenarios.

## Ethical & Safety Considerations

The implementation of LLM-based task decomposition systems in humanoid robots raises important ethical and safety considerations. These relate to autonomous decision-making and human-robot interaction.

<Callout type="danger" title="AI Safety and Transparency">
LLM-based systems must be designed with appropriate safety constraints and oversight mechanisms, with transparency in AI decision-making processes to maintain human trust and enable appropriate oversight.
</Callout>

The system must be designed with appropriate safety constraints and oversight mechanisms. This ensures safe operation in human environments. Additionally, the transparency of AI decision-making processes is important. This maintains human trust and enables appropriate oversight of robot behavior. The system should include mechanisms for human override. This provides clear communication of the robot's intentions and limitations (Vander Hoek et al., 2019).

## Key Takeaways

- Natural Language Understanding connects linguistic concepts to robot actions and perceptions
- Task decomposition breaks complex commands into executable action sequences
- Context-aware interpretation improves command understanding using environmental and state information
- Error handling and clarification requests ensure robust human-robot interaction
- Hierarchical planning enables flexible execution of complex tasks
- Uncertainty management balances robustness with responsiveness in task execution

## References

Brohan, N., & Wyatt, J. (2020). Natural language interfaces for robotics: A survey. IEEE Transactions on Robotics, 36(4), 1025-1040.

Huang, S., et al. (2022). Language-conditioned robot behavior synthesis from demonstrations. IEEE International Conference on Robotics and Automation (ICMA), 4567-4574.

Radford, A., et al. (2022). Robust speech recognition via large-scale weak supervision. International Conference on Machine Learning (ICML), 18742-18761.

Vander Hoek, K., Hart, S., Belpaeme, T., & Feil-Seifer, D. (2019). Socially assistive robotics: A focus on trust and trustworthiness. IEEE International Conference on Robotics and Automation (ICRA), 8374-8380.

Zhang, C., et al. (2023). Multimodal language grounding for robotic manipulation. IEEE International Conference on Robotics and Automation (ICRA), 2345-2352.