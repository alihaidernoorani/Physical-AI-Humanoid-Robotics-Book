"use strict";(globalThis.webpackChunktextbook_physical_ai=globalThis.webpackChunktextbook_physical_ai||[]).push([[269],{1302(e,n,t){t(6540),t(4848)},4028(e,n,t){t.r(n),t.d(n,{assets:()=>m,contentTitle:()=>u,default:()=>f,frontMatter:()=>d,metadata:()=>a,toc:()=>p});const a=JSON.parse('{"id":"vla/grounding-language-ros2","title":"Language-Action Grounding","description":"Mapping natural language commands to ROS 2 action servers and feedback mechanisms for humanoid robotics","source":"@site/docs/vla/03-grounding-language-ros2.mdx","sourceDirName":"vla","slug":"/vla/grounding-language-ros2","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/vla/grounding-language-ros2","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla/03-grounding-language-ros2.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"grounding-language-ros2","title":"Language-Action Grounding","description":"Mapping natural language commands to ROS 2 action servers and feedback mechanisms for humanoid robotics","personalization":true,"translation":"ur","learning_outcomes":["Implement language-to-action mapping systems for humanoid robot control","Design and implement ROS 2 action servers for language-driven tasks","Create feedback and confirmation mechanisms for natural human-robot interaction","Integrate multi-modal command execution with vision-language-action systems"],"software_stack":["ROS 2 Humble Hawksbill (LTS)","Python 3.10+ with rclpy","NVIDIA Isaac ROS Foundation Packages","Transformers library for language processing","OpenCV for vision integration","Isaac ROS GEMs for perception processing"],"hardware_recommendations":["NVIDIA Jetson AGX Orin for integrated processing","High-quality microphone array for voice input","RGB-D camera for visual perception","IMU for balance feedback"],"hardware_alternatives":["Laptop with discrete GPU for development","NVIDIA Jetson Orin Nano for minimal configurations","Simulated environment for development"],"prerequisites":["Module 1-4: Complete understanding of ROS 2, simulation, AI, and VLA components","Module 4.1: Voice-to-text integration concepts","Module 4.2: LLM-based task decomposition","Experience with ROS 2 action servers and services","Basic understanding of natural language processing"],"assessment_recommendations":["Integration test: End-to-end voice command to action execution","Feedback mechanism: Performance evaluation of confirmation systems"],"dependencies":["04-vla/intro","04-vla/01-voice-to-text-whisper","04-vla/02-llm-task-decomposition"]},"sidebar":"tutorialSidebar","previous":{"title":"LLM-Based Task Decomposition","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/vla/llm-task-decomposition"},"next":{"title":"Capstone: End-to-End Autonomous Humanoid","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/vla/capstone-end-to-end"}}');var i=t(4848),o=t(8453),s=t(8844),r=t(7589),c=t(6212),l=t(7639);t(1302);const d={id:"grounding-language-ros2",title:"Language-Action Grounding",description:"Mapping natural language commands to ROS 2 action servers and feedback mechanisms for humanoid robotics",personalization:!0,translation:"ur",learning_outcomes:["Implement language-to-action mapping systems for humanoid robot control","Design and implement ROS 2 action servers for language-driven tasks","Create feedback and confirmation mechanisms for natural human-robot interaction","Integrate multi-modal command execution with vision-language-action systems"],software_stack:["ROS 2 Humble Hawksbill (LTS)","Python 3.10+ with rclpy","NVIDIA Isaac ROS Foundation Packages","Transformers library for language processing","OpenCV for vision integration","Isaac ROS GEMs for perception processing"],hardware_recommendations:["NVIDIA Jetson AGX Orin for integrated processing","High-quality microphone array for voice input","RGB-D camera for visual perception","IMU for balance feedback"],hardware_alternatives:["Laptop with discrete GPU for development","NVIDIA Jetson Orin Nano for minimal configurations","Simulated environment for development"],prerequisites:["Module 1-4: Complete understanding of ROS 2, simulation, AI, and VLA components","Module 4.1: Voice-to-text integration concepts","Module 4.2: LLM-based task decomposition","Experience with ROS 2 action servers and services","Basic understanding of natural language processing"],assessment_recommendations:["Integration test: End-to-end voice command to action execution","Feedback mechanism: Performance evaluation of confirmation systems"],dependencies:["04-vla/intro","04-vla/01-voice-to-text-whisper","04-vla/02-llm-task-decomposition"]},u="Language-Action Grounding",m={},p=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Mapping Language to ROS 2 Actions",id:"mapping-language-to-ros-2-actions",level:2},{value:"Concrete Examples",id:"concrete-examples",level:3},{value:"Action Server Implementation",id:"action-server-implementation",level:2},{value:"Diagram Descriptions",id:"diagram-descriptions",level:3},{value:"Concrete Examples",id:"concrete-examples-1",level:3},{value:"Feedback and Confirmation Mechanisms",id:"feedback-and-confirmation-mechanisms",level:2},{value:"Diagram Descriptions",id:"diagram-descriptions-1",level:3},{value:"Concrete Examples",id:"concrete-examples-2",level:3},{value:"Multi-modal Command Execution",id:"multi-modal-command-execution",level:2},{value:"Diagram Descriptions",id:"diagram-descriptions-2",level:3},{value:"Concrete Examples",id:"concrete-examples-3",level:3},{value:"Forward References to Capstone Project",id:"forward-references-to-capstone-project",level:2},{value:"Concrete Examples",id:"concrete-examples-4",level:3},{value:"Ethical &amp; Safety Considerations",id:"ethical--safety-considerations",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function g(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"language-action-grounding",children:"Language-Action Grounding"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implement language-to-action mapping systems for humanoid robot control"}),"\n",(0,i.jsx)(n.li,{children:"Design and implement ROS 2 action servers for language-driven tasks"}),"\n",(0,i.jsx)(n.li,{children:"Create feedback and confirmation mechanisms for natural human-robot interaction"}),"\n",(0,i.jsx)(n.li,{children:"Integrate multi-modal command execution with vision-language-action systems"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"mapping-language-to-ros-2-actions",children:"Mapping Language to ROS 2 Actions"}),"\n",(0,i.jsx)(n.p,{children:"Language-to-action mapping forms the critical bridge between natural language understanding and robot execution in humanoid systems. This process involves converting the structured output of language processing systems into specific ROS 2 action calls that control the robot's behavior. For humanoid robots, this mapping must handle the complexity of natural language and ensures safe and appropriate robot responses."}),"\n",(0,i.jsx)(s.A,{type:"tip",title:"Language-to-Action Mapping",children:(0,i.jsx)(n.p,{children:"Language-to-action mapping connects natural language understanding to robot execution, converting structured language processing output into specific ROS 2 action calls that control robot behavior."})}),"\n",(0,i.jsx)(n.p,{children:'Semantic mapping connects the concepts identified in natural language commands to specific robot capabilities and environmental objects. For humanoid robots, this includes mapping spatial references ("the table near the window") to geometric locations, action references ("pick up") to specific manipulation capabilities, and object references ("the red cup") to identified objects in the robot\'s perception system.'}),"\n",(0,i.jsx)(l.A,{title:"Semantic Mapping Process",description:"Diagram showing the semantic mapping process connecting natural language concepts to robot capabilities and environmental objects",caption:"Semantic mapping connects natural language concepts to robot capabilities and environmental objects"}),"\n",(0,i.jsx)(n.p,{children:"Action selection algorithms determine which ROS 2 actions are most appropriate for executing the interpreted commands. For humanoid robots, this involves considering the robot's current state, available capabilities, environmental constraints, and safety requirements. The selection process must ensure that chosen actions are executable and safe and achieve the intended goal."}),"\n",(0,i.jsx)(c.A,{title:"Action Selection Algorithm",problem:"Implement an action selection algorithm that determines appropriate ROS 2 actions for natural language commands.",hints:["Consider robot state, capabilities, and constraints","Implement safety validation before action selection","Map language concepts to specific action servers"],solution:"# Example action selection algorithm\nclass ActionSelector:\n  def __init__(self):\n      self.robot_capabilities = {\n          'navigation': ['move_base', 'navigate_to_pose'],\n          'manipulation': ['pick_place', 'grasp_object', 'place_object'],\n          'perception': ['detect_objects', 'recognize_objects', 'find_surface']\n      }\n\n      self.spatial_actions = {\n          'go to': 'move_base',\n          'navigate to': 'navigate_to_pose',\n          'move to': 'move_base'\n      }\n\n      self.manipulation_actions = {\n          'pick up': 'grasp_object',\n          'grasp': 'grasp_object',\n          'place': 'place_object',\n          'put': 'place_object'\n      }\n\n  def select_actions(self, interpreted_command, robot_state, environment):\n      # Parse the interpreted command to identify action type and parameters\n      action_type = self.identify_action_type(interpreted_command)\n\n      if action_type == 'navigation':\n          return self.select_navigation_action(interpreted_command, robot_state, environment)\n      elif action_type == 'manipulation':\n          return self.select_manipulation_action(interpreted_command, robot_state, environment)\n      elif action_type == 'perception':\n          return self.select_perception_action(interpreted_command, robot_state, environment)\n      else:\n          return self.select_generic_action(interpreted_command, robot_state, environment)\n\n  def identify_action_type(self, command):\n      # Identify the primary action type from the command\n      command_lower = command.lower()\n\n      # Check for navigation keywords\n      navigation_keywords = ['go to', 'navigate', 'move to', 'travel to', 'walk to']\n      for keyword in navigation_keywords:\n          if keyword in command_lower:\n              return 'navigation'\n\n      # Check for manipulation keywords\n      manipulation_keywords = ['pick up', 'grasp', 'place', 'put', 'take', 'get', 'lift']\n      for keyword in manipulation_keywords:\n          if keyword in command_lower:\n              return 'manipulation'\n\n      # Check for perception keywords\n      perception_keywords = ['see', 'find', 'detect', 'look for', 'recognize', 'identify']\n      for keyword in perception_keywords:\n          if keyword in command_lower:\n              return 'perception'\n\n      return 'generic'\n\n  def select_navigation_action(self, command, robot_state, environment):\n      # Extract target location from command\n      target_location = self.extract_location(command, environment)\n\n      # Validate that the location is reachable\n      if self.is_location_reachable(target_location, robot_state):\n          return {\n              'action_server': 'move_base',\n              'goal': {\n                  'target_pose': target_location,\n                  'frame_id': 'map'\n              },\n              'validation': 'location_reachable'\n          }\n      else:\n          return {\n              'action_server': None,\n              'error': 'Location not reachable',\n              'suggestion': 'Find alternative location or path'\n          }\n\n  def select_manipulation_action(self, command, robot_state, environment):\n      # Extract object and action from command\n      target_object = self.extract_object(command, environment)\n      manipulation_action = self.extract_manipulation_action(command)\n\n      # Validate that the object is manipulable\n      if self.is_object_manipulable(target_object, robot_state, environment):\n          return {\n              'action_server': manipulation_action,\n              'goal': {\n                  'object': target_object,\n                  'pose': target_object.get('pose', None)\n              },\n              'validation': 'object_manipulable'\n          }\n      else:\n          return {\n              'action_server': None,\n              'error': 'Object not manipulable',\n              'suggestion': 'Find alternative object or action'\n          }\n\n  def is_location_reachable(self, location, robot_state):\n      # Check if location is reachable based on robot constraints\n      # This is a simplified check - real implementation would use navigation planning\n      robot_pos = robot_state.get('position', [0, 0, 0])\n      distance = self.calculate_distance(robot_pos, location['position'])\n      return distance < 10.0  # Assume max reachable distance is 10m\n\n  def is_object_manipulable(self, obj, robot_state, environment):\n      # Check if object can be manipulated based on robot capabilities\n      if 'pose' not in obj:\n          return False  # Object position unknown\n\n      obj_pos = obj['pose']['position']\n      robot_pos = robot_state.get('position', [0, 0, 0])\n      distance = self.calculate_distance(robot_pos, obj_pos)\n\n      # Check if object is within reach\n      if distance > 1.0:  # Assume max reach is 1m\n          return False\n\n      # Check object properties\n      if obj.get('weight', 0) > 2.0:  # Max weight 2kg\n          return False\n\n      return True\n\n  def calculate_distance(self, pos1, pos2):\n      import math\n      return math.sqrt(sum((a - b) ** 2 for a, b in zip(pos1, pos2)))"}),"\n",(0,i.jsx)(n.p,{children:"Constraint validation ensures that selected actions are feasible given the robot's current state and environmental conditions. For humanoid robots, this includes checking reach constraints, balance requirements, and safety margins before executing actions. The validation process prevents the robot from attempting impossible or unsafe actions that occur based on language commands."}),"\n",(0,i.jsx)(r.A,{question:"What is the primary purpose of constraint validation in language-to-action mapping?",options:"To improve speech recognition accuracy||To ensure selected actions are feasible given robot state and environmental conditions||To enhance visual perception capabilities||To optimize robot movement speed",correctAnswer:"To ensure selected actions are feasible given robot state and environmental conditions",explanation:"Constraint validation ensures that selected actions are feasible given the robot's current state and environmental conditions, checking reach constraints, balance requirements, and safety margins before execution."}),"\n",(0,i.jsx)(n.h3,{id:"concrete-examples",children:"Concrete Examples"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'Example: "Bring me the red cup" maps to navigation and manipulation action sequence'}),"\n",(0,i.jsx)(n.li,{children:"Example: Constraint validation checking if robot can reach the identified red cup before grasping"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"action-server-implementation",children:"Action Server Implementation"}),"\n",(0,i.jsx)(n.p,{children:"ROS 2 action server implementation for language-driven tasks requires specialized design considerations that account for the variable nature of natural language commands and the need for robust error handling. The action servers must be able to handle commands with varying complexity and provide appropriate feedback during execution."}),"\n",(0,i.jsx)(s.A,{type:"note",title:"Action Server Design",children:(0,i.jsx)(n.p,{children:"Action servers for language-driven tasks must handle variable command complexity, provide robust error handling, and offer appropriate feedback during execution to support natural human-robot interaction."})}),"\n",(0,i.jsx)(n.p,{children:'Hierarchical action servers organize complex tasks into manageable subtasks that can be executed independently while maintaining overall task coordination. For humanoid robots, this might involve high-level action servers for complex behaviors (like "clean the room") that coordinate multiple lower-level action servers (navigation, manipulation, perception). The hierarchy enables flexible execution and error recovery.'}),"\n",(0,i.jsx)(l.A,{title:"Hierarchical Action Server Architecture",description:"Diagram showing hierarchical action server architecture with high-level and low-level coordination",caption:"Hierarchical action server architecture with high-level and low-level coordination"}),"\n",(0,i.jsx)(n.p,{children:"Stateful action servers maintain context across multiple steps of complex tasks. This allows for multi-turn interactions and task resumption after interruptions. For humanoid robots, this is essential for tasks that require multiple steps that may be interrupted by environmental changes or user commands. The state management must handle both successful completion and failure scenarios."}),"\n",(0,i.jsx)(c.A,{title:"Stateful Action Server Implementation",problem:"Implement a stateful action server that maintains context across multiple steps of complex tasks.",hints:["Design state management for task progression","Implement interruption handling and resumption","Include state persistence and recovery mechanisms"],solution:"# Example stateful action server implementation\nimport rclpy\nfrom rclpy.action import ActionServer\nfrom rclpy.node import Node\nfrom rclpy.callback_groups import ReentrantCallbackGroup\nfrom rclpy.executors import MultiThreadedExecutor\nimport threading\nfrom enum import Enum\n\nclass TaskState(Enum):\n  IDLE = 0\n  INITIALIZING = 1\n  EXECUTING = 2\n  PAUSED = 3\n  FAILED = 4\n  COMPLETED = 5\n\nclass StatefulActionServer(Node):\n  def __init__(self):\n      super().__init__('stateful_action_server')\n\n      # Use reentrant callback group to handle multiple requests\n      callback_group = ReentrantCallbackGroup()\n\n      # Create action server with state management\n      self._action_server = ActionServer(\n          self,\n          YourActionType,  # Replace with actual action type\n          'stateful_task',\n          self.execute_callback,\n          callback_group=callback_group\n      )\n\n      # Task state management\n      self.current_task_state = TaskState.IDLE\n      self.task_context = {}\n      self.task_lock = threading.RLock()\n\n  def execute_callback(self, goal_handle):\n      with self.task_lock:\n          self.get_logger().info('Received new task goal')\n\n          # Initialize task state\n          self.current_task_state = TaskState.INITIALIZING\n          self.task_context = {\n              'goal': goal_handle.request,\n              'progress': 0.0,\n              'current_step': 0,\n              'total_steps': 0,\n              'subtasks': [],\n              'results': []\n          }\n\n          # Parse and decompose the goal into steps\n          self.task_context['subtasks'] = self.parse_goal_into_subtasks(goal_handle.request)\n          self.task_context['total_steps'] = len(self.task_context['subtasks'])\n\n          # Execute subtasks sequentially\n          self.current_task_state = TaskState.EXECUTING\n\n          for i, subtask in enumerate(self.task_context['subtasks']):\n              if goal_handle.is_cancel_requested:\n                  goal_handle.canceled()\n                  self.current_task_state = TaskState.IDLE\n                  return YourActionResult(result='canceled')\n\n              # Update progress\n              self.task_context['current_step'] = i\n              self.task_context['progress'] = (i / len(self.task_context['subtasks'])) * 100.0\n\n              # Publish feedback\n              feedback_msg = YourActionFeedback()\n              feedback_msg.current_step = i\n              feedback_msg.total_steps = len(self.task_context['subtasks'])\n              feedback_msg.progress = self.task_context['progress']\n              feedback_msg.status = f'Executing subtask {i+1} of {len(self.task_context[\"subtasks\"])}'\n\n              goal_handle.publish_feedback(feedback_msg)\n\n              # Execute the subtask\n              try:\n                  subtask_result = self.execute_subtask(subtask)\n                  self.task_context['results'].append(subtask_result)\n\n                  if not subtask_result.success:\n                      self.get_logger().error(f'Subtask {i} failed: {subtask_result.message}')\n                      self.current_task_state = TaskState.FAILED\n                      goal_handle.abort()\n                      return YourActionResult(result='subtask_failed', message=subtask_result.message)\n\n              except Exception as e:\n                  self.get_logger().error(f'Error executing subtask {i}: {str(e)}')\n                  self.current_task_state = TaskState.FAILED\n                  goal_handle.abort()\n                  return YourActionResult(result='execution_error', message=str(e))\n\n          # Task completed successfully\n          self.current_task_state = TaskState.COMPLETED\n          goal_handle.succeed()\n\n          result = YourActionResult()\n          result.success = True\n          result.message = f'Completed {len(self.task_context[\"results\"])} subtasks successfully'\n          result.results = self.task_context['results']\n\n          return result\n\n  def parse_goal_into_subtasks(self, goal):\n      # Parse the high-level goal into executable subtasks\n      # This would use the LLM-based task decomposition from Module 4.2\n      subtasks = []\n\n      # Example: \"Bring me the red cup\" -> [navigate_to_cup, grasp_cup, navigate_to_user, place_cup]\n      if 'bring' in goal.command.lower() or 'get' in goal.command.lower():\n          # Decompose into navigation, grasping, and delivery subtasks\n          subtasks = [\n              {'type': 'navigation', 'target': self.find_object_location('red cup')},\n              {'type': 'manipulation', 'action': 'grasp', 'object': 'red cup'},\n              {'type': 'navigation', 'target': self.get_user_location()},\n              {'type': 'manipulation', 'action': 'place', 'location': 'delivery_position'}\n          ]\n\n      return subtasks\n\n  def execute_subtask(self, subtask):\n      # Execute a single subtask and return result\n      if subtask['type'] == 'navigation':\n          return self.execute_navigation_subtask(subtask)\n      elif subtask['type'] == 'manipulation':\n          return self.execute_manipulation_subtask(subtask)\n      else:\n          return self.execute_generic_subtask(subtask)\n\n  def execute_navigation_subtask(self, subtask):\n      # Execute navigation subtask\n      # This would interface with navigation action servers\n      return {'success': True, 'message': 'Navigation completed', 'data': subtask}\n\n  def execute_manipulation_subtask(self, subtask):\n      # Execute manipulation subtask\n      # This would interface with manipulation action servers\n      return {'success': True, 'message': 'Manipulation completed', 'data': subtask}\n\n  def execute_generic_subtask(self, subtask):\n      # Execute generic subtask\n      return {'success': True, 'message': 'Subtask completed', 'data': subtask}\n\n  def pause_task(self):\n      with self.task_lock:\n          if self.current_task_state == TaskState.EXECUTING:\n              self.current_task_state = TaskState.PAUSED\n              return True\n          return False\n\n  def resume_task(self):\n      with self.task_lock:\n          if self.current_task_state == TaskState.PAUSED:\n              self.current_task_state = TaskState.EXECUTING\n              return True\n          return False\n\n  def get_task_state(self):\n      with self.task_lock:\n          return {\n              'state': self.current_task_state,\n              'context': self.task_context.copy()\n          }\n\ndef main(args=None):\n  rclpy.init(args=args)\n  node = StatefulActionServer()\n\n  executor = MultiThreadedExecutor()\n  executor.add_node(node)\n\n  try:\n      executor.spin()\n  except KeyboardInterrupt:\n      pass\n  finally:\n      node.destroy_node()\n      rclpy.shutdown()\n\nif __name__ == '__main__':\n  main()"}),"\n",(0,i.jsx)(n.p,{children:"Asynchronous execution patterns allow action servers to handle long-running tasks and remain responsive to new commands or safety-critical interruptions. For humanoid robots, this includes the ability to preempt ongoing actions when new commands are received or when safety conditions require immediate attention. The execution pattern must balance task completion with responsiveness and safety."}),"\n",(0,i.jsx)(n.h3,{id:"diagram-descriptions",children:"Diagram Descriptions"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Diagram: Stateful action server maintaining context across multiple task steps"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"concrete-examples-1",children:"Concrete Examples"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'Example: High-level "clean the room" server coordinating navigation and manipulation servers'}),"\n",(0,i.jsx)(n.li,{children:"Example: Stateful server maintaining task context when interrupted by user command"}),"\n"]}),"\n",(0,i.jsx)(r.A,{question:"What is a key benefit of hierarchical action servers for humanoid robots?",options:["To reduce computational requirements","To organize complex tasks into manageable subtasks while maintaining overall task coordination","To improve audio quality","To increase network speed"],correctAnswer:"To organize complex tasks into manageable subtasks while maintaining overall task coordination",explanation:"Hierarchical action servers organize complex tasks into manageable subtasks that can be executed independently while maintaining overall task coordination, enabling flexible execution and error recovery."}),"\n",(0,i.jsx)(n.h2,{id:"feedback-and-confirmation-mechanisms",children:"Feedback and Confirmation Mechanisms"}),"\n",(0,i.jsx)(n.p,{children:"Feedback mechanisms provide users with clear information about the robot's understanding of commands and indicate progress toward task completion. For humanoid robots, this includes visual, auditory, and haptic feedback that helps maintain trust and enables safe human-robot collaboration. The feedback system must be designed to provide appropriate information and avoids overwhelming users."}),"\n",(0,i.jsx)(s.A,{type:"warning",title:"Feedback Design",children:(0,i.jsx)(n.p,{children:"Feedback mechanisms must provide clear information about robot understanding and task progress while avoiding overwhelming users, helping maintain trust and enabling safe human-robot collaboration."})}),"\n",(0,i.jsx)(n.p,{children:"Confirmation requests allow the robot to verify understanding of commands before executing potentially significant actions. For humanoid robots, this includes asking for confirmation before executing commands that involve moving to different locations, manipulating objects, or performing actions that might affect the environment. The confirmation system must balance safety with efficiency."}),"\n",(0,i.jsx)(l.A,{title:"Feedback Mechanisms",description:"Diagram showing feedback mechanisms with visual, auditory, and haptic outputs for user information",caption:"Feedback mechanisms with visual, auditory, and haptic outputs for user information"}),"\n",(0,i.jsx)(n.p,{children:'Progress reporting provides continuous updates on task execution and enables users to understand the robot\'s current state and estimated completion time. For humanoid robots, this includes reporting on intermediate steps of complex tasks such as "I\'m going to the kitchen to get the cup" or "I\'m cleaning the table now." The reporting system must be informative and avoids being disruptive.'}),"\n",(0,i.jsx)(c.A,{title:"Feedback and Confirmation System",problem:"Implement a feedback and confirmation system for natural human-robot interaction.",hints:["Design appropriate feedback for different task stages","Implement confirmation requests for significant actions","Create progress reporting mechanisms"],solution:"# Example feedback and confirmation system\nclass FeedbackConfirmationSystem:\n  def __init__(self):\n      self.feedback_levels = {\n          'minimal': 0,  # Only critical feedback\n          'normal': 1,   # Standard feedback\n          'verbose': 2   # Detailed feedback\n      }\n      self.current_feedback_level = 'normal'\n\n      self.confirmation_triggers = {\n          'high_risk': ['move_to_unfamiliar_area', 'manipulate_fragile_object', 'approach_human'],\n          'medium_risk': ['navigate_busy_area', 'manipulate_heavy_object'],\n          'low_risk': []  # Usually don't need confirmation\n      }\n\n  def provide_feedback(self, event_type, data):\n      # Provide appropriate feedback based on event type and feedback level\n      if self.current_feedback_level == 'minimal' and event_type not in ['error', 'critical']:\n          return\n\n      feedback_message = self.generate_feedback_message(event_type, data)\n\n      if event_type == 'task_start':\n          self.speak(f\"Starting task: {feedback_message}\")\n          self.indicate_status('working')\n      elif event_type == 'task_progress':\n          progress = data.get('progress', 0)\n          self.speak(f\"Task progress: {progress}% - {feedback_message}\")\n      elif event_type == 'task_complete':\n          self.speak(f\"Task completed: {feedback_message}\")\n          self.indicate_status('idle')\n      elif event_type == 'error':\n          self.speak(f\"Error occurred: {feedback_message}\")\n          self.indicate_status('error')\n      elif event_type == 'confirmation_needed':\n          self.request_confirmation(feedback_message)\n\n  def generate_feedback_message(self, event_type, data):\n      # Generate appropriate feedback message based on event type\n      if event_type == 'task_start':\n          return f\"I will {data.get('task_description', 'perform the requested action')}\"\n      elif event_type == 'task_progress':\n          action = data.get('current_action', 'performing action')\n          return f\"I am currently {action}\"\n      elif event_type == 'task_complete':\n          return f\"I have completed {data.get('task_description', 'the requested task')}\"\n      elif event_type == 'error':\n          return f\"{data.get('error_message', 'An error occurred')}\"\n      elif event_type == 'confirmation_needed':\n          return f\"{data.get('request', 'Do you want me to proceed?')}\"\n\n      return \"Processing...\"\n\n  def request_confirmation(self, action_description):\n      # Request confirmation for significant actions\n      confirmation_question = f\"Should I proceed with: {action_description}?\"\n      self.speak(confirmation_question)\n\n      # Wait for user response (this would integrate with voice input system)\n      user_response = self.wait_for_user_response()\n\n      if user_response and self.is_affirmative_response(user_response):\n          return True\n      else:\n          self.speak(\"Action cancelled by user.\")\n          return False\n\n  def is_confirmation_needed(self, action, context):\n      # Determine if confirmation is needed based on action and context\n      risk_level = self.assess_risk_level(action, context)\n\n      if risk_level == 'high':\n          return True\n      elif risk_level == 'medium':\n          # For medium risk, consider user preferences\n          return self.should_request_confirmation_for_medium_risk()\n      else:\n          return False\n\n  def assess_risk_level(self, action, context):\n      # Assess risk level of an action based on context\n      action_type = action.get('type', '')\n      environment = context.get('environment', {})\n\n      # High risk actions\n      if action_type in self.confirmation_triggers['high_risk']:\n          return 'high'\n\n      # Check for environmental factors\n      humans_nearby = environment.get('humans_nearby', [])\n      if humans_nearby and action_type in self.confirmation_triggers['medium_risk']:\n          return 'medium'\n\n      # Check action properties\n      if action.get('fragile_object', False):\n          return 'high'\n      elif action.get('heavy_object', False):\n          return 'medium'\n\n      return 'low'\n\n  def wait_for_user_response(self):\n      # Wait for user response (integrate with voice input system)\n      # This is a placeholder - would connect to ASR system\n      return \"yes\"  # Placeholder response\n\n  def is_affirmative_response(self, response):\n      # Check if user response is affirmative\n      affirmative_keywords = ['yes', 'sure', 'okay', 'go ahead', 'proceed', 'correct']\n      return any(keyword in response.lower() for keyword in affirmative_keywords)\n\n  def speak(self, message):\n      # Output speech message (placeholder - would connect to TTS system)\n      print(f\"Robot says: {message}\")\n\n  def indicate_status(self, status):\n      # Indicate robot status through LEDs, display, etc.\n      print(f\"Robot status: {status}\")"}),"\n",(0,i.jsx)(n.p,{children:"Error communication and recovery mechanisms inform users when tasks cannot be completed as requested and provide alternatives or request clarification. For humanoid robots, this includes explaining why a task failed and suggesting alternative approaches. The error communication must be clear and helpful and maintains user confidence in the system."}),"\n",(0,i.jsx)(n.h3,{id:"diagram-descriptions-1",children:"Diagram Descriptions"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Diagram: Confirmation request system with verification before significant actions"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"concrete-examples-2",children:"Concrete Examples"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'Example: Robot says "I\'m going to the kitchen to get the cup" during task execution'}),"\n",(0,i.jsx)(n.li,{children:'Example: Confirmation request "Should I really clean the messy desk?" before proceeding'}),"\n"]}),"\n",(0,i.jsx)(r.A,{question:"What is the primary purpose of confirmation requests in human-robot interaction?",options:["To improve computational performance","To verify understanding of commands before executing potentially significant actions","To reduce memory usage","To increase network speed"],correctAnswer:"To verify understanding of commands before executing potentially significant actions",explanation:"Confirmation requests allow the robot to verify understanding of commands before executing potentially significant actions, helping to ensure safety and user intent alignment."}),"\n",(0,i.jsx)(n.h2,{id:"multi-modal-command-execution",children:"Multi-modal Command Execution"}),"\n",(0,i.jsx)(n.p,{children:"Multi-modal command execution integrates language understanding with visual perception and other sensory modalities, which enables more robust and flexible interaction. For humanoid robots, this means that language commands can be disambiguated using visual context and actions can be selected based on both linguistic and perceptual information."}),"\n",(0,i.jsx)(s.A,{type:"tip",title:"Multi-modal Integration",children:(0,i.jsx)(n.p,{children:"Multi-modal command execution combines language understanding with visual perception and other sensory modalities, enabling more robust and flexible interaction by disambiguating commands using visual context."})}),"\n",(0,i.jsx)(n.p,{children:"Visual grounding enhances language understanding by connecting linguistic references to visual observations. For humanoid robots, this enables the robot to identify specific objects mentioned in commands and matches linguistic descriptions with visual observations. The system can use color, shape, size, and location information to disambiguates object references."}),"\n",(0,i.jsx)(l.A,{title:"Visual Grounding Process",description:"Diagram showing visual grounding connecting linguistic descriptions to visual observations and object identification",caption:"Visual grounding connecting linguistic descriptions to visual observations and object identification"}),"\n",(0,i.jsx)(n.p,{children:"Perceptual confirmation validates that the robot's interpretation of commands matches the current environment. For humanoid robots, this might involve confirming that a requested object is visible before attempting to manipulate it and verifies that a requested location is accessible before navigating there. The confirmation process reduces errors and improves task success rates."}),"\n",(0,i.jsx)(c.A,{title:"Multi-modal Integration System",problem:"Implement a multi-modal integration system that combines language understanding with visual perception.",hints:["Integrate visual object detection with language processing","Implement perceptual confirmation mechanisms","Create visual grounding for object references"],solution:"# Example multi-modal integration system\nimport cv2\nimport numpy as np\nfrom typing import List, Dict, Any\n\nclass MultiModalIntegration:\n  def __init__(self):\n      self.object_detector = self.initialize_object_detector()\n      self.language_processor = self.initialize_language_processor()\n      self.spatial_reasoner = self.initialize_spatial_reasoner()\n\n  def initialize_object_detector(self):\n      # Initialize object detection system (e.g., using Isaac ROS GEMs or custom detector)\n      # This is a placeholder - would use actual object detection system\n      return {'initialized': True}\n\n  def initialize_language_processor(self):\n      # Initialize language processing system\n      # This would connect to the LLM-based processing from Module 4.2\n      return {'initialized': True}\n\n  def initialize_spatial_reasoner(self):\n      # Initialize spatial reasoning system\n      return {\n          'reference_frames': ['map', 'robot', 'camera'],\n          'spatial_relations': ['near', 'far', 'left', 'right', 'in_front', 'behind']\n      }\n\n  def process_multimodal_command(self, command: str, visual_data: Any):\n      # Process command using both language and visual information\n      language_analysis = self.analyze_language(command)\n      visual_analysis = self.analyze_visual(visual_data)\n\n      # Integrate information from both modalities\n      integrated_result = self.integrate_modalities(language_analysis, visual_analysis)\n\n      # Resolve ambiguities using visual context\n      resolved_command = self.resolve_ambiguities(integrated_result, visual_analysis)\n\n      return resolved_command\n\n  def analyze_language(self, command: str):\n      # Analyze the linguistic content of the command\n      analysis = {\n          'objects': self.extract_object_references(command),\n          'actions': self.extract_action_references(command),\n          'spatial_refs': self.extract_spatial_references(command),\n          'descriptors': self.extract_descriptors(command)\n      }\n      return analysis\n\n  def extract_object_references(self, command: str) -> List[str]:\n      # Extract object references from command\n      # This is simplified - real implementation would use NLP\n      import re\n      # Look for object references like \"red cup\", \"book\", \"table\", etc.\n      object_patterns = [\n          r'(?:thes+|as+|ans+)?(w+s+w+)s+(?:cup|bottle|book|box|object)',  # \"red cup\"\n          r'(?:thes+|as+|ans+)?(w+)s+(?:cup|bottle|book|box|object)',        # \"cup\"\n          r'(?:thes+|as+|ans+)?(w+)s+(?:table|chair|desk|shelf)'             # \"table\"\n      ]\n\n      objects = []\n      for pattern in object_patterns:\n          matches = re.findall(pattern, command.lower())\n          objects.extend(matches)\n\n      # Add simple object names\n      simple_objects = ['cup', 'bottle', 'book', 'box', 'table', 'chair', 'desk', 'shelf']\n      for obj in simple_objects:\n          if obj in command.lower():\n              objects.append(obj)\n\n      return list(set(objects))  # Remove duplicates\n\n  def extract_action_references(self, command: str) -> List[str]:\n      # Extract action references from command\n      actions = []\n      action_keywords = [\n          'pick up', 'grasp', 'take', 'get', 'bring', 'go to', 'move to',\n          'navigate to', 'place', 'put', 'set', 'find', 'locate', 'see'\n      ]\n\n      command_lower = command.lower()\n      for action in action_keywords:\n          if action in command_lower:\n              actions.append(action)\n\n      return actions\n\n  def extract_spatial_references(self, command: str) -> List[str]:\n      # Extract spatial references from command\n      spatial_refs = []\n      spatial_keywords = ['near', 'by', 'next to', 'on', 'at', 'in', 'over there', 'there']\n\n      command_lower = command.lower()\n      for ref in spatial_keywords:\n          if ref in command_lower:\n              spatial_refs.append(ref)\n\n      return spatial_refs\n\n  def extract_descriptors(self, command: str) -> List[str]:\n      # Extract descriptive attributes (color, size, etc.)\n      descriptors = []\n      color_keywords = ['red', 'blue', 'green', 'yellow', 'black', 'white', 'large', 'small', 'big', 'little']\n\n      command_lower = command.lower()\n      words = command_lower.split()\n      for word in words:\n          if word in color_keywords:\n              descriptors.append(word)\n\n      return descriptors\n\n  def analyze_visual(self, visual_data: Any):\n      # Analyze visual data to identify objects and spatial relationships\n      # This is a placeholder - would use actual computer vision system\n      visual_analysis = {\n          'detected_objects': [\n              {'name': 'red cup', 'position': [1.0, 2.0, 0.0], 'confidence': 0.95},\n              {'name': 'blue book', 'position': [1.5, 2.0, 0.0], 'confidence': 0.89},\n              {'name': 'wooden table', 'position': [1.2, 1.8, 0.0], 'confidence': 0.92}\n          ],\n          'spatial_relations': self.compute_spatial_relations(),\n          'environment_map': {'accessible_areas': [], 'obstacles': []}\n      }\n      return visual_analysis\n\n  def compute_spatial_relations(self):\n      # Compute spatial relations between objects\n      # This would use actual spatial reasoning based on object positions\n      return {\n          'red cup': {'on': 'wooden table'},\n          'blue book': {'on': 'wooden table'},\n          'wooden table': {'in': 'kitchen'}\n      }\n\n  def integrate_modalities(self, language_analysis: Dict, visual_analysis: Dict):\n      # Integrate information from language and visual analysis\n      integrated = {\n          'language': language_analysis,\n          'visual': visual_analysis,\n          'matches': self.find_matches(language_analysis, visual_analysis),\n          'ambiguities': self.identify_ambiguities(language_analysis, visual_analysis)\n      }\n      return integrated\n\n  def find_matches(self, language_analysis: Dict, visual_analysis: Dict):\n      # Find matches between linguistic references and visual observations\n      matches = []\n\n      for lang_obj in language_analysis['objects']:\n          for vis_obj in visual_analysis['detected_objects']:\n              # Simple string matching for demonstration\n              # In practice, this would use more sophisticated matching\n              if lang_obj.lower() in vis_obj['name'].lower() or vis_obj['name'].lower() in lang_obj.lower():\n                  matches.append({\n                      'language_ref': lang_obj,\n                      'visual_obj': vis_obj,\n                      'confidence': vis_obj['confidence']\n                  })\n\n      return matches\n\n  def identify_ambiguities(self, language_analysis: Dict, visual_analysis: Dict):\n      # Identify ambiguities that need visual resolution\n      ambiguities = []\n\n      # Check for multiple objects matching a description\n      for lang_obj in language_analysis['objects']:\n          matching_objects = []\n          for vis_obj in visual_analysis['detected_objects']:\n              if lang_obj.lower() in vis_obj['name'].lower() or vis_obj['name'].lower() in lang_obj.lower():\n                  matching_objects.append(vis_obj)\n\n          if len(matching_objects) > 1:\n              ambiguities.append({\n                  'reference': lang_obj,\n                  'candidate_objects': matching_objects,\n                  'type': 'object_ambiguity'\n              })\n\n      return ambiguities\n\n  def resolve_ambiguities(self, integrated_result: Dict, visual_analysis: Dict):\n      # Resolve ambiguities using visual context\n      resolved = {\n          'command': integrated_result['language'],\n          'target_object': None,\n          'target_location': None,\n          'action': integrated_result['language']['actions'][0] if integrated_result['language']['actions'] else None\n      }\n\n      # If there are ambiguities, use spatial context to resolve them\n      if integrated_result['ambiguities']:\n          for ambiguity in integrated_result['ambiguities']:\n              if ambiguity['type'] == 'object_ambiguity':\n                  # Use spatial relations to resolve ambiguity\n                  spatial_context = self.get_spatial_context(integrated_result['language'])\n                  resolved['target_object'] = self.select_object_by_context(\n                      ambiguity['candidate_objects'],\n                      spatial_context,\n                      visual_analysis\n                  )\n      else:\n          # Use the first match if no ambiguities\n          if integrated_result['matches']:\n              resolved['target_object'] = integrated_result['matches'][0]['visual_obj']\n\n      return resolved\n\n  def get_spatial_context(self, language_analysis: Dict):\n      # Extract spatial context from language\n      return {\n          'spatial_refs': language_analysis['spatial_refs'],\n          'descriptors': language_analysis['descriptors']\n      }\n\n  def select_object_by_context(self, candidate_objects: List[Dict], spatial_context: Dict, visual_analysis: Dict):\n      # Select the most appropriate object based on spatial context\n      if spatial_context['spatial_refs']:\n          # If there are spatial references, use them to filter candidates\n          for ref in spatial_context['spatial_refs']:\n              for obj in candidate_objects:\n                  # Check spatial relations\n                  relations = visual_analysis['spatial_relations']\n                  if obj['name'] in relations:\n                      for rel_type, related_obj in relations[obj['name']].items():\n                          if ref in rel_type or rel_type in ref:\n                              return obj\n\n      # If spatial context doesn't help, use descriptors\n      if spatial_context['descriptors']:\n          for desc in spatial_context['descriptors']:\n              for obj in candidate_objects:\n                  if desc.lower() in obj['name'].lower():\n                      return obj\n\n      # If still ambiguous, return the highest confidence object\n      return max(candidate_objects, key=lambda x: x['confidence'])"}),"\n",(0,i.jsx)(n.p,{children:"Adaptive execution adjusts action parameters based on real-time perception and environmental feedback. For humanoid robots, this includes adjusting grasp positions based on actual object poses, modifying navigation paths based on dynamic obstacles, and adapting task execution based on changing environmental conditions. The adaptive system must maintain the intended goal and accommodates environmental variations."}),"\n",(0,i.jsx)(n.h3,{id:"diagram-descriptions-2",children:"Diagram Descriptions"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Diagram: Multi-modal integration showing language, vision, and action components working together"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"concrete-examples-3",children:"Concrete Examples"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'Example: Robot uses vision to identify "red cup" when multiple cups are present in environment'}),"\n",(0,i.jsx)(n.li,{children:"Example: Adaptive execution adjusting grasp based on actual object pose vs. expected position"}),"\n"]}),"\n",(0,i.jsx)(r.A,{question:"What is the primary benefit of multi-modal command execution for humanoid robots?",options:["To reduce computational requirements","To integrate language understanding with visual perception for more robust and flexible interaction","To improve audio quality","To increase network speed"],correctAnswer:"To integrate language understanding with visual perception for more robust and flexible interaction",explanation:"Multi-modal command execution integrates language understanding with visual perception and other sensory modalities, enabling more robust and flexible interaction by disambiguating commands using visual context."}),"\n",(0,i.jsx)(n.h2,{id:"forward-references-to-capstone-project",children:"Forward References to Capstone Project"}),"\n",(0,i.jsx)(n.p,{children:"The language-action grounding concepts covered in this chapter are essential for completing the end-to-end autonomous humanoid system in your capstone project. The language-to-action mapping will connect your LLM-based task decomposition to your robot's action execution system, while the feedback mechanisms will provide natural interaction with users. The multi-modal integration will enable your robot to combine language understanding with visual perception for robust task execution."}),"\n",(0,i.jsx)(l.A,{title:"Capstone Integration Flow",description:"Diagram showing integration flow connecting language-action grounding to capstone project components",caption:"Integration flow showing language-action grounding connecting to capstone project components"}),"\n",(0,i.jsx)(n.h3,{id:"concrete-examples-4",children:"Concrete Examples"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'Example: Capstone project implementing "Bring me the red cup" command through language-action pipeline'}),"\n",(0,i.jsx)(n.li,{children:"Example: Multi-modal integration in capstone combining voice commands with visual object recognition"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"ethical--safety-considerations",children:"Ethical & Safety Considerations"}),"\n",(0,i.jsx)(n.p,{children:"The implementation of language-action grounding systems in humanoid robots raises important ethical and safety considerations regarding autonomous decision-making and human-robot interaction. The system must be designed with appropriate safety constraints and oversight mechanisms to ensure safe operation in human environments. The confirmation and feedback mechanisms are particularly important for maintaining human awareness of robot intentions and enabling appropriate oversight. Additionally, the system should include safeguards against potentially harmful commands and provide users with clear understanding of the robot's capabilities and limitations."}),"\n",(0,i.jsx)(s.A,{type:"danger",title:"Safety and Oversight",children:(0,i.jsx)(n.p,{children:"Language-action grounding systems must include appropriate safety constraints, confirmation mechanisms, and oversight capabilities to ensure safe operation and maintain human awareness of robot intentions in human environments."})}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Language-to-action mapping connects natural language understanding to robot execution"}),"\n",(0,i.jsx)(n.li,{children:"Action server design must handle the variable nature of natural language commands"}),"\n",(0,i.jsx)(n.li,{children:"Feedback and confirmation mechanisms are essential for natural human-robot interaction"}),"\n",(0,i.jsx)(n.li,{children:"Multi-modal integration enhances robustness and flexibility of command execution"}),"\n",(0,i.jsx)(n.li,{children:"Stateful action servers enable complex, multi-step task execution"}),"\n",(0,i.jsx)(n.li,{children:"Safety validation ensures appropriate and safe robot responses to language commands"}),"\n"]})]})}function f(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(g,{...e})}):g(e)}},6212(e,n,t){t.d(n,{A:()=>o});var a=t(6540),i=t(4848);const o=({title:e,problem:n,solution:t,hints:o=[],initialCode:s="",className:r=""})=>{const[c,l]=(0,a.useState)(s),[d,u]=(0,a.useState)(!1),[m,p]=(0,a.useState)(!1),[g,f]=(0,a.useState)(!1),[h,b]=(0,a.useState)(null);return(0,i.jsxs)("div",{className:`exercise-component ${r}`,style:{border:"1px solid #ddd",borderRadius:"8px",padding:"1rem",margin:"1rem 0",backgroundColor:"#fff"},children:[(0,i.jsx)("h4",{style:{margin:"0 0 1rem 0"},children:e}),(0,i.jsxs)("div",{style:{marginBottom:"1rem"},children:[(0,i.jsx)("h5",{style:{margin:"0.5rem 0",color:"#202124"},children:"Problem:"}),(0,i.jsx)("div",{style:{padding:"0.5rem",backgroundColor:"#f9f9f9",borderRadius:"4px"},children:n})]}),o.length>0&&(0,i.jsxs)("div",{style:{marginBottom:"1rem"},children:[(0,i.jsx)("button",{onClick:()=>p(!m),style:{padding:"0.5rem 1rem",backgroundColor:"#fbbc04",color:"white",border:"none",borderRadius:"4px",cursor:"pointer",marginBottom:"0.5rem"},children:m?"Hide Hint":"Show Hint"}),m&&(0,i.jsxs)("div",{style:{padding:"0.5rem",backgroundColor:"#fef7e0",borderRadius:"4px",border:"1px solid #fbbc04"},children:[(0,i.jsx)("strong",{children:"Hint:"})," ",o[0]]})]}),(0,i.jsxs)("div",{style:{marginBottom:"1rem"},children:[(0,i.jsx)("h5",{style:{margin:"0.5rem 0",color:"#202124"},children:"Your Solution:"}),(0,i.jsx)("textarea",{value:c,onChange:e=>l(e.target.value),style:{width:"100%",minHeight:"150px",padding:"0.5rem",fontFamily:"monospace",border:"1px solid #ddd",borderRadius:"4px",fontSize:"0.9rem"},placeholder:"Write your solution here..."})]}),(0,i.jsxs)("div",{style:{display:"flex",gap:"0.5rem",marginBottom:"1rem"},children:[(0,i.jsx)("button",{onClick:()=>{b({success:!0,message:"Code executed successfully! Check your logic against the solution."}),f(!0)},style:{padding:"0.5rem 1rem",backgroundColor:"#4caf50",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Run Code"}),(0,i.jsx)("button",{onClick:()=>u(!d),style:{padding:"0.5rem 1rem",backgroundColor:"#2196f3",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:d?"Hide Solution":"Show Solution"}),(0,i.jsx)("button",{onClick:()=>{l(s),u(!1),p(!1),f(!1),b(null)},style:{padding:"0.5rem 1rem",backgroundColor:"#9e9e9e",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Reset"})]}),g&&h&&(0,i.jsxs)("div",{style:{padding:"0.75rem",borderRadius:"4px",backgroundColor:h.success?"#e8f5e9":"#ffebee",border:"1px solid "+(h.success?"#4caf50":"#f44336"),marginBottom:"1rem"},children:[(0,i.jsx)("strong",{children:"Status:"})," ",h.message]}),d&&(0,i.jsxs)("div",{style:{padding:"0.5rem",backgroundColor:"#e8f5e9",borderRadius:"4px",border:"1px solid #4caf50"},children:[(0,i.jsx)("h5",{style:{margin:"0.5rem 0",color:"#202124"},children:"Solution:"}),(0,i.jsx)("pre",{style:{padding:"0.5rem",backgroundColor:"#f1f8e9",borderRadius:"4px",overflowX:"auto",whiteSpace:"pre-wrap"},children:t})]})]})}},7589(e,n,t){t.d(n,{A:()=>o});var a=t(6540),i=t(4848);const o=({question:e,options:n,correctAnswer:t,explanation:o,className:s=""})=>{const r=Array.isArray(n)?n:n&&"string"==typeof n?n.split("||"):[],[c,l]=(0,a.useState)(null),[d,u]=(0,a.useState)(!1),[m,p]=(0,a.useState)(!1),g=e=>{if(d)return;l(e);p(e===t),u(!0)},f=e=>d?e===t?{padding:"0.75rem",margin:"0.5rem 0",border:"1px solid #4caf50",borderRadius:"4px",backgroundColor:"#e8f5e9",fontWeight:"bold"}:e===c&&e!==t?{padding:"0.75rem",margin:"0.5rem 0",border:"1px solid #f44336",borderRadius:"4px",backgroundColor:"#ffebee"}:{padding:"0.75rem",margin:"0.5rem 0",border:"1px solid #ddd",borderRadius:"4px",backgroundColor:"#f5f5f5"}:{padding:"0.75rem",margin:"0.5rem 0",cursor:"pointer",border:"1px solid #ddd",borderRadius:"4px",backgroundColor:c===e?"#e3f2fd":"#fff"};return(0,i.jsxs)("div",{className:`quiz-component ${s}`,style:{border:"1px solid #ddd",borderRadius:"8px",padding:"1rem",margin:"1rem 0",backgroundColor:"#fff"},children:[(0,i.jsx)("h4",{style:{margin:"0 0 1rem 0"},children:e}),(0,i.jsx)("div",{children:r.map((n,t)=>(0,i.jsxs)("div",{style:f(n),onClick:()=>g(n),children:[(0,i.jsx)("input",{type:"radio",name:`quiz-${e}`,value:n,checked:c===n,onChange:()=>{},disabled:d,style:{marginRight:"0.5rem"}}),n]},t))}),d&&(0,i.jsxs)("div",{style:{marginTop:"1rem",padding:"0.75rem",borderRadius:"4px",backgroundColor:m?"#e8f5e9":"#ffebee",border:"1px solid "+(m?"#4caf50":"#f44336")},children:[(0,i.jsx)("p",{style:{margin:"0.5rem 0",fontWeight:"bold"},children:m?"\u2705 Correct!":"\u274c Incorrect"}),o&&(0,i.jsxs)("p",{style:{margin:"0.5rem 0"},children:[(0,i.jsx)("strong",{children:"Explanation:"})," ",o]})]}),d?(0,i.jsx)("button",{onClick:()=>{l(null),u(!1),p(!1)},style:{marginTop:"1rem",padding:"0.5rem 1rem",backgroundColor:"#2196f3",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Try Again"}):c&&(0,i.jsx)("button",{onClick:()=>g(c),style:{marginTop:"1rem",padding:"0.5rem 1rem",backgroundColor:"#4caf50",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Submit Answer"})]})}},7639(e,n,t){t.d(n,{A:()=>i});t(6540);var a=t(4848);const i=({title:e,description:n,src:t,alt:i,caption:o,className:s=""})=>(0,a.jsxs)("div",{className:`diagram-component ${s}`,style:{textAlign:"center",margin:"1.5rem 0",padding:"1rem",border:"1px solid #eee",borderRadius:"8px",backgroundColor:"#fafafa"},children:[e&&(0,a.jsx)("h5",{style:{margin:"0 0 1rem 0",color:"#202124",fontSize:"1rem",fontWeight:"bold"},children:e}),(0,a.jsx)("div",{style:{display:"flex",justifyContent:"center",alignItems:"center",margin:"0 auto",maxWidth:"100%"},children:t?(0,a.jsx)("img",{src:t,alt:i||e||"Diagram",style:{maxWidth:"100%",height:"auto",border:"1px solid #ddd",borderRadius:"4px"}}):(0,a.jsx)("div",{style:{width:"100%",height:"200px",display:"flex",alignItems:"center",justifyContent:"center",backgroundColor:"#f5f5f5",border:"2px dashed #ccc",borderRadius:"4px",color:"#666"},children:"Diagram placeholder"})}),(n||o)&&(0,a.jsxs)("div",{style:{marginTop:"0.5rem",fontSize:"0.9rem",color:"#5f6368",textAlign:"left",padding:"0.5rem"},children:[n&&(0,a.jsx)("p",{style:{margin:"0.5rem 0"},children:n}),o&&(0,a.jsxs)("p",{style:{margin:"0.5rem 0",fontStyle:"italic"},children:[(0,a.jsx)("strong",{children:"Figure:"})," ",o]})]})]})},8453(e,n,t){t.d(n,{R:()=>s,x:()=>r});var a=t(6540);const i={},o=a.createContext(i);function s(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(o.Provider,{value:n},e.children)}},8844(e,n,t){t.d(n,{A:()=>i});t(6540);var a=t(4848);const i=({type:e="note",title:n,children:t,className:i=""})=>{const o={note:{borderLeft:"4px solid #4285f4",backgroundColor:"#f0f4ff",color:"#202124"},tip:{borderLeft:"4px solid #34a853",backgroundColor:"#f0f9ff",color:"#202124"},warning:{borderLeft:"4px solid #fbbc04",backgroundColor:"#fef7e0",color:"#202124"},danger:{borderLeft:"4px solid #ea4335",backgroundColor:"#fce8e6",color:"#202124"}},s={note:"\u2139\ufe0f",tip:"\ud83d\udca1",warning:"\u26a0\ufe0f",danger:"\u274c"},r=o[e]||o.note,c=s[e]||s.note;return(0,a.jsx)("div",{className:`callout callout-${e} ${i}`,style:{border:"1px solid",borderRadius:"4px",padding:"1rem",margin:"1rem 0",...r},children:(0,a.jsxs)("div",{style:{display:"flex",alignItems:"flex-start"},children:[(0,a.jsx)("span",{style:{fontSize:"1.2rem",marginRight:"0.5rem"},children:c}),(0,a.jsxs)("div",{children:[n&&(0,a.jsx)("h5",{style:{margin:"0 0 0.5rem 0",fontSize:"1rem",fontWeight:"bold",textTransform:"uppercase",letterSpacing:"0.5px"},children:n}),(0,a.jsx)("div",{children:t})]})]})})}}}]);