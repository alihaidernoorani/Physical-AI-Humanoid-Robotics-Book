"use strict";(globalThis.webpackChunktextbook_physical_ai=globalThis.webpackChunktextbook_physical_ai||[]).push([[440],{1306(e,n,i){i.r(n),i.d(n,{assets:()=>h,contentTitle:()=>m,default:()=>g,frontMatter:()=>d,metadata:()=>t,toc:()=>u});const t=JSON.parse('{"id":"digital-twin/unity-high-fidelity-env","title":"High-Fidelity Environments in Unity","description":"Learning Objectives","source":"@site/docs/digital-twin/03-unity-high-fidelity-env.mdx","sourceDirName":"digital-twin","slug":"/digital-twin/unity-high-fidelity-env","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/digital-twin/unity-high-fidelity-env","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/digital-twin/03-unity-high-fidelity-env.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"High-Fidelity Environments in Unity","sidebar_label":"Unity Environments"},"sidebar":"tutorialSidebar","previous":{"title":"Sensor Simulation","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/digital-twin/sensor-simulation"},"next":{"title":"Gazebo-Unity Synchronization","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/digital-twin/synchronizing-gazebo-unity"}}');var o=i(4848),s=i(8453),r=i(8844),a=i(7589),l=i(6212),c=i(7639);const d={title:"High-Fidelity Environments in Unity",sidebar_label:"Unity Environments"},m=void 0,h={},u=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Unity Robotics Package Setup",id:"unity-robotics-package-setup",level:2},{value:"Concrete Examples",id:"concrete-examples",level:3},{value:"ROS 2 Communication in Unity",id:"ros-2-communication-in-unity",level:2},{value:"Concrete Examples",id:"concrete-examples-1",level:3},{value:"Physics Simulation in Unity",id:"physics-simulation-in-unity",level:2},{value:"Concrete Examples",id:"concrete-examples-2",level:3},{value:"Environment Asset Creation",id:"environment-asset-creation",level:2},{value:"Concrete Examples",id:"concrete-examples-3",level:3},{value:"Practical Applications in Humanoid Robotics",id:"practical-applications-in-humanoid-robotics",level:2},{value:"Ethical &amp; Safety Considerations",id:"ethical--safety-considerations",level:2},{value:"Summary",id:"summary",level:2}];function p(e){const n={h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this chapter, you will:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Configure Unity with the Robotics Package for ROS 2 communication"}),"\n",(0,o.jsx)(n.li,{children:"Implement high-fidelity rendering for realistic sensor simulation"}),"\n",(0,o.jsx)(n.li,{children:"Design physics-based environments for humanoid robot interaction"}),"\n",(0,o.jsx)(n.li,{children:"Create and integrate environment assets for complex simulation scenarios"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"unity-robotics-package-setup",children:"Unity Robotics Package Setup"}),"\n",(0,o.jsx)(n.p,{children:"The Unity Robotics Package provides essential tools and components that integrate Unity simulation environments with ROS 2 systems. This enables seamless communication between Unity's high-fidelity rendering and the broader ROS ecosystem. For humanoid robotics applications, this integration enables the development of photorealistic environments that support sophisticated perception and human-robot interaction studies."}),"\n",(0,o.jsx)(r.A,{type:"tip",title:"Unity Robotics Integration",children:(0,o.jsx)(n.p,{children:"The Unity Robotics Package enables seamless communication between Unity's high-fidelity rendering and the broader ROS 2 ecosystem, making it ideal for sophisticated perception and human-robot interaction studies."})}),"\n",(0,o.jsx)(n.p,{children:"Installation and configuration of the Unity Robotics Package involves setting up components such as the ROS-TCP-Connector and ROS-TCP-Endpoint. These facilitate communication between Unity and ROS 2. The package provides pre-built components and scripts that handle the complexity of ROS message serialization and deserialization. This allows developers to focus on environment creation rather than communication infrastructure. For humanoid robots, the package enables real-time synchronization between Unity environments and ROS 2 robot control systems."}),"\n",(0,o.jsx)(l.A,{title:"Unity Robotics Package Installation",problem:"Install and configure the Unity Robotics Package in a Unity project for humanoid robot simulation.",hints:["Use the Unity Package Manager","Import the ROS-TCP-Connector package","Configure the ROS-TCP-Endpoint for communication"],solution:'// Installation steps:\n1. Open Unity Package Manager (Window > Package Manager)\n2. Click "+" > Add package from git URL\n3. Add: com.unity.robotics.ros-tcp-connector\n4. Create a new GameObject and attach ROSConnection component\n5. Configure the ROS-TCP-Endpoint with appropriate IP and port settings\n\n// Example ROSConnection usage:\nusing Unity.Robotics.ROSTCPConnector;\nusing UnityEngine;\n\npublic class HumanoidController : MonoBehaviour\n{\n  ROSConnection ros;\n\n  void Start()\n  {\n      ros = ROSConnection.GetOrCreateInstance();\n      ros.RegisterPublisher<sensor_msgs.msg.Image>("camera_image");\n  }\n}'}),"\n",(0,o.jsx)(n.p,{children:"The ROS-TCP-Connector component manages network communication between Unity and ROS 2. It handles message routing and protocol translation. For humanoid robots, this component must maintain low-latency communication. This ensures that sensor data and control commands are transmitted with minimal delay. The connector supports both publisher-subscriber and service-based communication patterns. This enables comprehensive integration with ROS 2 systems."}),"\n",(0,o.jsx)(c.A,{title:"Unity-ROS 2 Communication Architecture",description:"Diagram showing the communication flow between Unity and ROS 2 systems via the TCP connector",width:"700",height:"400"}),"\n",(0,o.jsx)(n.p,{children:"Environment synchronization components in the Unity Robotics Package handle coordination between Unity's physics simulation and ROS 2's robot state publishing. For humanoid robots, this synchronization ensures that the visual representation of the robot and environment in Unity matches the state maintained by ROS 2 systems. The synchronization components must handle both static transforms and dynamic object states using appropriate update frequencies."}),"\n",(0,o.jsx)(n.h3,{id:"concrete-examples",children:"Concrete Examples"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Example: Installing Unity Robotics Package and setting up ROS-TCP-Connector for NAO robot simulation"}),"\n",(0,o.jsx)(n.li,{children:"Example: Configuring ROS-TCP-Endpoint to connect Unity environment with ROS 2 navigation stack"}),"\n"]}),"\n",(0,o.jsx)(a.A,{question:"What is the primary purpose of the ROS-TCP-Connector component in Unity?",options:["To manage Unity's rendering pipeline","To manage network communication between Unity and ROS 2","To handle Unity's physics simulation","To create 3D models in Unity"],correctAnswer:1,explanation:"The ROS-TCP-Connector component manages network communication between Unity and ROS 2, handling message routing and protocol translation for seamless integration."}),"\n",(0,o.jsx)(n.h2,{id:"ros-2-communication-in-unity",children:"ROS 2 Communication in Unity"}),"\n",(0,o.jsx)(n.p,{children:"ROS 2 communication in Unity environments enables bidirectional data exchange between Unity's high-fidelity rendering system and the broader ROS ecosystem. For humanoid robots, this communication capability allows Unity to serve as a sophisticated sensor simulation platform. It generates realistic camera images, depth maps, and other sensor data that can be processed by ROS 2 perception nodes."}),"\n",(0,o.jsx)(r.A,{type:"note",title:"Bidirectional Communication",children:(0,o.jsx)(n.p,{children:"ROS 2 communication in Unity enables bidirectional data exchange between Unity's high-fidelity rendering and the broader ROS ecosystem, making it ideal for sensor simulation."})}),"\n",(0,o.jsx)(n.p,{children:"Message publishing from Unity to ROS 2 enables the simulation of various sensor types including cameras, LiDAR, and other perception systems. For humanoid robots, Unity can generate realistic RGB-D camera data with appropriate noise characteristics and visual effects that closely match physical sensors. The message publishing system must handle timing and synchronization requirements for real-time sensor simulation while maintaining the performance needed for interactive environments."}),"\n",(0,o.jsx)(c.A,{title:"Message Flow Between Unity and ROS 2",description:"Diagram showing the flow of messages between Unity and ROS 2 systems with publishers and subscribers",width:"700",height:"400"}),"\n",(0,o.jsx)(n.p,{children:"Service and action communication patterns in Unity environments enable more complex interactions between Unity and ROS 2 systems. For humanoid robots, these patterns can support dynamic environment modification, scenario setup, and complex simulation management. The service communication allows ROS 2 nodes to request specific actions in the Unity environment such as spawning objects, changing lighting conditions, or modifying environmental parameters."}),"\n",(0,o.jsx)(l.A,{title:"ROS 2 Publisher Implementation",problem:"Implement a ROS 2 publisher in Unity to send camera sensor data from the simulation.",hints:["Use the ROSConnection component","Create a sensor_msgs/Image message","Publish at appropriate frequency for camera data"],solution:'using Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.MessageGeneration;\nusing UnityEngine;\n\npublic class CameraPublisher : MonoBehaviour\n{\n  ROSConnection ros;\n  Camera cam;\n\n  void Start()\n  {\n      ros = ROSConnection.GetOrCreateInstance();\n      cam = GetComponent<Camera>();\n  }\n\n  void Update()\n  {\n      // Capture image from camera\n      RenderTexture currentRT = RenderTexture.active;\n      RenderTexture.active = cam.targetTexture;\n\n      Texture2D tex = new Texture2D(cam.targetTexture.width, cam.targetTexture.height);\n      tex.ReadPixels(new Rect(0, 0, cam.targetTexture.width, cam.targetTexture.height), 0, 0);\n      tex.Apply();\n\n      RenderTexture.active = currentRT;\n\n      // Convert to ROS image message and publish\n      byte[] imageData = tex.EncodeToPNG();\n      sensor_msgs.msg.Image imgMsg = new sensor_msgs.msg.Image();\n      imgMsg.data = imageData;\n      imgMsg.encoding = "png";\n      imgMsg.height = (uint)cam.targetTexture.height;\n      imgMsg.width = (uint)cam.targetTexture.width;\n\n      ros.Publish("camera/image_raw", imgMsg);\n  }\n}'}),"\n",(0,o.jsx)(n.p,{children:"Real-time performance considerations for ROS 2 communication in Unity include network bandwidth management, message serialization efficiency, and synchronization timing. For humanoid robots operating in complex environments, the communication system must handle high-frequency sensor data while maintaining the frame rates required for realistic rendering. The system must also provide appropriate buffering and interpolation to handle network latency variations."}),"\n",(0,o.jsx)(n.h3,{id:"concrete-examples-1",children:"Concrete Examples"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Example: Publishing realistic RGB-D camera data from Unity to ROS 2 perception nodes"}),"\n",(0,o.jsx)(n.li,{children:"Example: Using ROS 2 services to dynamically spawn objects in Unity simulation environment"}),"\n"]}),"\n",(0,o.jsx)(a.A,{question:"Which of the following communication patterns does Unity support with ROS 2?",options:["Publisher-subscriber only","Service-based only","Both publisher-subscriber and service-based","Neither publisher-subscriber nor service-based"],correctAnswer:2,explanation:"Unity supports both publisher-subscriber and service-based communication patterns with ROS 2, enabling comprehensive integration."}),"\n",(0,o.jsx)(n.h2,{id:"physics-simulation-in-unity",children:"Physics Simulation in Unity"}),"\n",(0,o.jsx)(n.p,{children:"Unity's physics engine provides sophisticated simulation capabilities that complement its rendering features, enabling the creation of environments where humanoid robots can interact with realistic physical objects. The physics simulation in Unity supports complex collision detection, rigid body dynamics, and constraint systems that can model real-world interactions between robots and their environment."}),"\n",(0,o.jsx)(r.A,{type:"warning",title:"Physics Accuracy",children:(0,o.jsx)(n.p,{children:"For humanoid robots, Unity's physics engine must accurately model the forces and torques involved in object manipulation, including grasp stability, object sliding, and dynamic interactions."})}),"\n",(0,o.jsx)(n.p,{children:"Rigid body dynamics in Unity physics support the simulation of objects with realistic mass, friction, and collision properties that humanoid robots can interact with during manipulation tasks. For humanoid robots, Unity's physics engine must accurately model the forces and torques involved in object manipulation including grasp stability, object sliding, and dynamic interactions. The physics parameters must be carefully tuned to match the real-world properties of the objects being simulated for realistic interaction scenarios."}),"\n",(0,o.jsx)(c.A,{title:"Unity Physics Engine Architecture",description:"Diagram showing Unity's physics engine architecture with collision detection and rigid body systems",width:"700",height:"400"}),"\n",(0,o.jsx)(n.p,{children:"Collision detection and response systems in Unity handle complex interactions between humanoid robot models and environmental objects. For humanoid robots, the collision system must efficiently handle numerous potential contacts that occur between robot limbs and environmental geometry while maintaining stable simulation behavior. The collision response must provide appropriate forces and torques that reflect realistic physical interactions for safe and effective robot operation."}),"\n",(0,o.jsx)(n.p,{children:"Joint constraint systems in Unity physics enable the simulation of complex articulated objects that are environmental mechanisms humanoid robots might encounter. For humanoid robots, this includes doors, drawers, switches, and other interactive elements that require articulated physics simulation. The joint constraints must provide realistic resistance and movement characteristics that match their real-world counterparts to ensure accurate behavior during interaction."}),"\n",(0,o.jsx)(l.A,{title:"Physics Joint Configuration",problem:"Configure a physics joint in Unity for a humanoid robot interaction with a door.",hints:["Use Unity's HingeJoint component","Configure appropriate limits and spring settings","Set proper anchor and axis properties"],solution:"using UnityEngine;\n\npublic class DoorController : MonoBehaviour\n{\n  HingeJoint hinge;\n\n  void Start()\n  {\n      hinge = GetComponent<HingeJoint>();\n\n      // Configure joint limits\n      JointLimits limits = hinge.limits;\n      limits.min = 0f;  // Closed position\n      limits.max = 90f; // Open position\n      limits.bounciness = 0.1f;  // Slight bounce\n      hinge.limits = limits;\n\n      // Configure spring for resistance\n      JointSpring spring = hinge.spring;\n      spring.spring = 5f;      // Spring strength\n      spring.damper = 2f;      // Damping\n      spring.targetPosition = 0f;  // Default to closed\n      hinge.spring = spring;\n\n      // Enable collision detection between door and robot\n      hinge.enableCollision = true;\n  }\n}"}),"\n",(0,o.jsx)(n.h3,{id:"concrete-examples-2",children:"Concrete Examples"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Example: Simulating realistic object manipulation with Unity physics for humanoid robot grippers"}),"\n",(0,o.jsx)(n.li,{children:"Example: Configuring joint constraints for interactive doors and switches in Unity environment"}),"\n"]}),"\n",(0,o.jsx)(a.A,{question:"What is the primary purpose of joint constraints in Unity physics for humanoid robots?",options:["To improve rendering quality","To enable simulation of complex articulated objects like doors and drawers","To reduce simulation performance","To create 3D models"],correctAnswer:1,explanation:"Joint constraints in Unity physics enable the simulation of complex articulated objects like doors and drawers that humanoid robots might interact with."}),"\n",(0,o.jsx)(n.h2,{id:"environment-asset-creation",children:"Environment Asset Creation"}),"\n",(0,o.jsx)(n.p,{children:"Creating high-fidelity environment assets for humanoid robot simulation requires careful attention to both visual realism and physical accuracy. For humanoid robots operating in human environments, the environment assets must include detailed architectural features, furniture, and objects that accurately represent the spaces where humanoid robots will operate. The assets must balance visual fidelity with performance requirements for real-time simulation."}),"\n",(0,o.jsx)(r.A,{type:"tip",title:"Asset Optimization",children:(0,o.jsx)(n.p,{children:"For humanoid robot simulation, environment assets must balance visual fidelity with performance requirements, using techniques like LOD systems and occlusion culling."})}),"\n",(0,o.jsx)(n.p,{children:"Architectural environment design for humanoid robots must include appropriate scale, lighting, and spatial relationships that match real-world human environments. The environments should include features such as doorways, corridors, stairs, and furniture that are arranged in realistic configurations that humanoid robots might encounter. The design must also consider the robot's sensor and mobility capabilities when creating navigation paths and interaction zones."}),"\n",(0,o.jsx)(c.A,{title:"Environment Asset Pipeline",description:"Diagram showing the pipeline from 3D modeling to Unity integration for humanoid robot simulation",width:"700",height:"400"}),"\n",(0,o.jsx)(n.p,{children:"Asset optimization techniques ensure that high-fidelity environments can run efficiently in real-time while maintaining the visual quality needed for realistic sensor simulation. For humanoid robots, this includes techniques such as level-of-detail (LOD) systems, occlusion culling, and texture streaming. These maintain performance without sacrificing the visual fidelity required for camera sensor simulation. The optimization must preserve the geometric accuracy needed for depth sensors and collision detection."}),"\n",(0,o.jsx)(l.A,{title:"LOD System Implementation",problem:"Implement a Level of Detail (LOD) system in Unity for environment assets in humanoid robot simulation.",hints:["Use Unity's LODGroup component","Create different detail levels for the same object","Configure appropriate transition distances"],solution:"using UnityEngine;\n\npublic class EnvironmentLOD : MonoBehaviour\n{\n  LODGroup lodGroup;\n\n  void Start()\n  {\n      lodGroup = GetComponent<LODGroup>();\n\n      // Define LOD levels\n      LOD[] lods = new LOD[3];\n\n      // LOD 0: High detail (distance 0-20m)\n      Renderer[] highDetailRenderers = GetComponentsInChildren<Renderer>();\n      lods[0] = new LOD(0.5f, highDetailRenderers);\n\n      // LOD 1: Medium detail (distance 20-50m)\n      Renderer[] mediumDetailRenderers = GetMediumDetailRenderers();\n      lods[1] = new LOD(0.25f, mediumDetailRenderers);\n\n      // LOD 2: Low detail (distance 50m+)\n      Renderer[] lowDetailRenderers = GetLowDetailRenderers();\n      lods[2] = new LOD(0.05f, lowDetailRenderers);\n\n      lodGroup.SetLODs(lods);\n      lodGroup.RecalculateBounds();\n  }\n\n  Renderer[] GetMediumDetailRenderers() { /* Implementation */ return new Renderer[0]; }\n  Renderer[] GetLowDetailRenderers() { /* Implementation */ return new Renderer[0]; }\n}"}),"\n",(0,o.jsx)(n.p,{children:"Interactive element design includes the creation of environmental objects that humanoid robots can manipulate or interact with during simulation. For humanoid robots, this includes objects such as doors, switches, handles, and manipulable items that support realistic interaction scenarios. The interactive elements must be designed with appropriate physics properties and visual feedback that supports both manipulation planning and human-robot interaction studies."}),"\n",(0,o.jsx)(n.h3,{id:"concrete-examples-3",children:"Concrete Examples"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Example: Creating realistic indoor apartment environment with furniture for humanoid robot navigation"}),"\n",(0,o.jsx)(n.li,{children:"Example: Designing interactive kitchen objects with physics properties for manipulation tasks"}),"\n"]}),"\n",(0,o.jsx)(a.A,{question:"What is the primary purpose of LOD (Level of Detail) systems in Unity environment assets?",options:["To improve visual quality only","To reduce visual quality only","To balance visual fidelity with performance requirements","To eliminate the need for optimization"],correctAnswer:2,explanation:"LOD systems balance visual fidelity with performance requirements by using different detail levels based on distance from the camera."}),"\n",(0,o.jsx)(n.h2,{id:"practical-applications-in-humanoid-robotics",children:"Practical Applications in Humanoid Robotics"}),"\n",(0,o.jsx)(n.p,{children:"In humanoid robotics, Unity environments are used for developing and testing complex behaviors in realistic human environments before deploying them on physical robots. The high-fidelity rendering and physics simulation capabilities of Unity enable comprehensive testing of perception, navigation, and manipulation algorithms in diverse scenarios that accurately reflect real-world conditions."}),"\n",(0,o.jsx)(n.p,{children:"When creating environments for humanoid robots, several critical considerations ensure effective and realistic results:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Selecting the appropriate level of visual and physical detail for your robot's intended tasks and application requirements"}),"\n",(0,o.jsx)(n.li,{children:"Meeting performance requirements that allow for real-time simulation without compromising visual fidelity"}),"\n",(0,o.jsx)(n.li,{children:"Ensuring seamless integration with ROS 2 communication systems for sensor and control data exchange"}),"\n",(0,o.jsx)(n.li,{children:"Incorporating accessibility and diversity considerations for inclusive robot development and testing"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"These elements work together synergistically to create high-fidelity simulation environments where you can develop and test humanoid robot capabilities safely and efficiently, with results that transfer effectively to real-world deployment scenarios."}),"\n",(0,o.jsx)(n.h2,{id:"ethical--safety-considerations",children:"Ethical & Safety Considerations"}),"\n",(0,o.jsx)(n.p,{children:"The creation of high-fidelity simulation environments for humanoid robots raises important ethical considerations that relate to the representation of human environments and the potential for bias in simulated scenarios. These considerations are fundamental to responsible robotics development and deployment."}),"\n",(0,o.jsx)(n.p,{children:"The environments must be designed to include diverse populations and accessibility considerations to ensure that humanoid robots are tested in inclusive scenarios that reflect the full range of human diversity and needs. Additionally, the realistic nature of these environments must be clearly communicated to avoid confusion between simulation and reality, particularly in research and development contexts where findings may influence real-world deployment decisions."}),"\n",(0,o.jsx)(r.A,{type:"danger",title:"Inclusive Design",children:(0,o.jsx)(n.p,{children:"High-fidelity simulation environments for humanoid robots must be designed inclusively, considering diverse populations and accessibility requirements to ensure fair and safe robot deployment."})}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"In this chapter, we've explored the fundamental concepts of high-fidelity environments in Unity and their critical applications in humanoid robotics:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Unity Robotics Package"})," enables seamless integration between Unity environments and ROS 2 systems, providing essential tools for bidirectional communication and state synchronization"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"High-fidelity rendering"})," in Unity supports realistic sensor simulation for humanoid robots, generating camera images, depth maps, and other sensor data that closely match physical sensors"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Physics simulation"})," in Unity enables realistic object interaction and manipulation scenarios with accurate modeling of forces, torques, and dynamic interactions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Environment asset creation"})," must balance visual fidelity with performance requirements using optimization techniques like LOD systems and occlusion culling"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Real-time communication systems"})," maintain synchronization between Unity and ROS 2, handling network bandwidth management and message serialization for smooth operation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Interactive environment elements"})," support comprehensive humanoid robot testing scenarios including doors, switches, and other articulated objects that require physics simulation"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"The Unity environment creation skills covered in this chapter are essential for developing the high-fidelity simulation environments for your Autonomous Humanoid capstone project. Unity-ROS 2 integration will enable realistic sensor simulation for your perception systems, allowing you to test algorithms in photorealistic conditions. The physics simulation capabilities will support manipulation and interaction scenarios with accurate modeling of forces and torques. The environment asset creation techniques will allow you to build diverse testing scenarios that validate your humanoid robot's capabilities in realistic human environments. Understanding these principles is fundamental to creating simulation environments that effectively bridge the reality gap and enable safe, reliable robot deployment."})]})}function g(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}},6212(e,n,i){i.d(n,{A:()=>s});var t=i(6540),o=i(4848);const s=({title:e,problem:n,solution:i,hints:s=[],initialCode:r="",className:a=""})=>{const[l,c]=(0,t.useState)(r),[d,m]=(0,t.useState)(!1),[h,u]=(0,t.useState)(!1),[p,g]=(0,t.useState)(!1),[y,b]=(0,t.useState)(null);return(0,o.jsxs)("div",{className:`exercise-component ${a}`,style:{border:"1px solid #ddd",borderRadius:"8px",padding:"1rem",margin:"1rem 0",backgroundColor:"#fff"},children:[(0,o.jsx)("h4",{style:{margin:"0 0 1rem 0"},children:e}),(0,o.jsxs)("div",{style:{marginBottom:"1rem"},children:[(0,o.jsx)("h5",{style:{margin:"0.5rem 0",color:"#202124"},children:"Problem:"}),(0,o.jsx)("div",{style:{padding:"0.5rem",backgroundColor:"#f9f9f9",borderRadius:"4px"},children:n})]}),s.length>0&&(0,o.jsxs)("div",{style:{marginBottom:"1rem"},children:[(0,o.jsx)("button",{onClick:()=>u(!h),style:{padding:"0.5rem 1rem",backgroundColor:"#fbbc04",color:"white",border:"none",borderRadius:"4px",cursor:"pointer",marginBottom:"0.5rem"},children:h?"Hide Hint":"Show Hint"}),h&&(0,o.jsxs)("div",{style:{padding:"0.5rem",backgroundColor:"#fef7e0",borderRadius:"4px",border:"1px solid #fbbc04"},children:[(0,o.jsx)("strong",{children:"Hint:"})," ",s[0]]})]}),(0,o.jsxs)("div",{style:{marginBottom:"1rem"},children:[(0,o.jsx)("h5",{style:{margin:"0.5rem 0",color:"#202124"},children:"Your Solution:"}),(0,o.jsx)("textarea",{value:l,onChange:e=>c(e.target.value),style:{width:"100%",minHeight:"150px",padding:"0.5rem",fontFamily:"monospace",border:"1px solid #ddd",borderRadius:"4px",fontSize:"0.9rem"},placeholder:"Write your solution here..."})]}),(0,o.jsxs)("div",{style:{display:"flex",gap:"0.5rem",marginBottom:"1rem"},children:[(0,o.jsx)("button",{onClick:()=>{b({success:!0,message:"Code executed successfully! Check your logic against the solution."}),g(!0)},style:{padding:"0.5rem 1rem",backgroundColor:"#4caf50",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Run Code"}),(0,o.jsx)("button",{onClick:()=>m(!d),style:{padding:"0.5rem 1rem",backgroundColor:"#2196f3",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:d?"Hide Solution":"Show Solution"}),(0,o.jsx)("button",{onClick:()=>{c(r),m(!1),u(!1),g(!1),b(null)},style:{padding:"0.5rem 1rem",backgroundColor:"#9e9e9e",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Reset"})]}),p&&y&&(0,o.jsxs)("div",{style:{padding:"0.75rem",borderRadius:"4px",backgroundColor:y.success?"#e8f5e9":"#ffebee",border:"1px solid "+(y.success?"#4caf50":"#f44336"),marginBottom:"1rem"},children:[(0,o.jsx)("strong",{children:"Status:"})," ",y.message]}),d&&(0,o.jsxs)("div",{style:{padding:"0.5rem",backgroundColor:"#e8f5e9",borderRadius:"4px",border:"1px solid #4caf50"},children:[(0,o.jsx)("h5",{style:{margin:"0.5rem 0",color:"#202124"},children:"Solution:"}),(0,o.jsx)("pre",{style:{padding:"0.5rem",backgroundColor:"#f1f8e9",borderRadius:"4px",overflowX:"auto",whiteSpace:"pre-wrap"},children:i})]})]})}},7589(e,n,i){i.d(n,{A:()=>s});var t=i(6540),o=i(4848);const s=({question:e,options:n,correctAnswer:i,explanation:s,className:r=""})=>{const a=Array.isArray(n)?n:n&&"string"==typeof n?n.split("||"):[],[l,c]=(0,t.useState)(null),[d,m]=(0,t.useState)(!1),[h,u]=(0,t.useState)(!1),p=e=>{if(d)return;c(e);u(e===i),m(!0)},g=e=>d?e===i?{padding:"0.75rem",margin:"0.5rem 0",border:"1px solid #4caf50",borderRadius:"4px",backgroundColor:"#e8f5e9",fontWeight:"bold"}:e===l&&e!==i?{padding:"0.75rem",margin:"0.5rem 0",border:"1px solid #f44336",borderRadius:"4px",backgroundColor:"#ffebee"}:{padding:"0.75rem",margin:"0.5rem 0",border:"1px solid #ddd",borderRadius:"4px",backgroundColor:"#f5f5f5"}:{padding:"0.75rem",margin:"0.5rem 0",cursor:"pointer",border:"1px solid #ddd",borderRadius:"4px",backgroundColor:l===e?"#e3f2fd":"#fff"};return(0,o.jsxs)("div",{className:`quiz-component ${r}`,style:{border:"1px solid #ddd",borderRadius:"8px",padding:"1rem",margin:"1rem 0",backgroundColor:"#fff"},children:[(0,o.jsx)("h4",{style:{margin:"0 0 1rem 0"},children:e}),(0,o.jsx)("div",{children:a.map((n,i)=>(0,o.jsxs)("div",{style:g(n),onClick:()=>p(n),children:[(0,o.jsx)("input",{type:"radio",name:`quiz-${e}`,value:n,checked:l===n,onChange:()=>{},disabled:d,style:{marginRight:"0.5rem"}}),n]},i))}),d&&(0,o.jsxs)("div",{style:{marginTop:"1rem",padding:"0.75rem",borderRadius:"4px",backgroundColor:h?"#e8f5e9":"#ffebee",border:"1px solid "+(h?"#4caf50":"#f44336")},children:[(0,o.jsx)("p",{style:{margin:"0.5rem 0",fontWeight:"bold"},children:h?"\u2705 Correct!":"\u274c Incorrect"}),s&&(0,o.jsxs)("p",{style:{margin:"0.5rem 0"},children:[(0,o.jsx)("strong",{children:"Explanation:"})," ",s]})]}),d?(0,o.jsx)("button",{onClick:()=>{c(null),m(!1),u(!1)},style:{marginTop:"1rem",padding:"0.5rem 1rem",backgroundColor:"#2196f3",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Try Again"}):l&&(0,o.jsx)("button",{onClick:()=>p(l),style:{marginTop:"1rem",padding:"0.5rem 1rem",backgroundColor:"#4caf50",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Submit Answer"})]})}},7639(e,n,i){i.d(n,{A:()=>o});i(6540);var t=i(4848);const o=({title:e,description:n,src:i,alt:o,caption:s,className:r=""})=>(0,t.jsxs)("div",{className:`diagram-component ${r}`,style:{textAlign:"center",margin:"1.5rem 0",padding:"1rem",border:"1px solid #eee",borderRadius:"8px",backgroundColor:"#fafafa"},children:[e&&(0,t.jsx)("h5",{style:{margin:"0 0 1rem 0",color:"#202124",fontSize:"1rem",fontWeight:"bold"},children:e}),(0,t.jsx)("div",{style:{display:"flex",justifyContent:"center",alignItems:"center",margin:"0 auto",maxWidth:"100%"},children:i?(0,t.jsx)("img",{src:i,alt:o||e||"Diagram",style:{maxWidth:"100%",height:"auto",border:"1px solid #ddd",borderRadius:"4px"}}):(0,t.jsx)("div",{style:{width:"100%",height:"200px",display:"flex",alignItems:"center",justifyContent:"center",backgroundColor:"#f5f5f5",border:"2px dashed #ccc",borderRadius:"4px",color:"#666"},children:"Diagram placeholder"})}),(n||s)&&(0,t.jsxs)("div",{style:{marginTop:"0.5rem",fontSize:"0.9rem",color:"#5f6368",textAlign:"left",padding:"0.5rem"},children:[n&&(0,t.jsx)("p",{style:{margin:"0.5rem 0"},children:n}),s&&(0,t.jsxs)("p",{style:{margin:"0.5rem 0",fontStyle:"italic"},children:[(0,t.jsx)("strong",{children:"Figure:"})," ",s]})]})]})},8453(e,n,i){i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}},8844(e,n,i){i.d(n,{A:()=>o});i(6540);var t=i(4848);const o=({type:e="note",title:n,children:i,className:o=""})=>{const s={note:{borderLeft:"4px solid #4285f4",backgroundColor:"#f0f4ff",color:"#202124"},tip:{borderLeft:"4px solid #34a853",backgroundColor:"#f0f9ff",color:"#202124"},warning:{borderLeft:"4px solid #fbbc04",backgroundColor:"#fef7e0",color:"#202124"},danger:{borderLeft:"4px solid #ea4335",backgroundColor:"#fce8e6",color:"#202124"}},r={note:"\u2139\ufe0f",tip:"\ud83d\udca1",warning:"\u26a0\ufe0f",danger:"\u274c"},a=s[e]||s.note,l=r[e]||r.note;return(0,t.jsx)("div",{className:`callout callout-${e} ${o}`,style:{border:"1px solid",borderRadius:"4px",padding:"1rem",margin:"1rem 0",...a},children:(0,t.jsxs)("div",{style:{display:"flex",alignItems:"flex-start"},children:[(0,t.jsx)("span",{style:{fontSize:"1.2rem",marginRight:"0.5rem"},children:l}),(0,t.jsxs)("div",{children:[n&&(0,t.jsx)("h5",{style:{margin:"0 0 0.5rem 0",fontSize:"1rem",fontWeight:"bold",textTransform:"uppercase",letterSpacing:"0.5px"},children:n}),(0,t.jsx)("div",{children:i})]})]})})}}}]);