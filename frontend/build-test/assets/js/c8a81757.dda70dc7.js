"use strict";(globalThis.webpackChunktextbook_physical_ai=globalThis.webpackChunktextbook_physical_ai||[]).push([[585],{1302(e,n,i){i(6540),i(4848)},4871(e,n,i){i.r(n),i.d(n,{assets:()=>m,contentTitle:()=>d,default:()=>h,frontMatter:()=>c,metadata:()=>t,toc:()=>u});const t=JSON.parse('{"id":"ai-robot-brain/edge-inference-jetson","title":"Edge AI Inference","description":"Real-time inference optimization on NVIDIA Jetson platforms for humanoid robotics","source":"@site/docs/ai-robot-brain/04-edge-inference-jetson.mdx","sourceDirName":"ai-robot-brain","slug":"/ai-robot-brain/edge-inference-jetson","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/ai-robot-brain/edge-inference-jetson","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ai-robot-brain/04-edge-inference-jetson.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"edge-inference-jetson","title":"Edge AI Inference","description":"Real-time inference optimization on NVIDIA Jetson platforms for humanoid robotics","personalization":true,"translation":"ur","learning_outcomes":["Optimize AI models using TensorRT for efficient inference on Jetson platforms","Implement real-time inference pipelines with predictable performance characteristics","Apply model quantization techniques to reduce computational requirements","Optimize power consumption for extended humanoid robot operation"],"software_stack":["NVIDIA JetPack SDK","TensorRT 8.6+","CUDA 12.0+ with cuDNN","ROS 2 Humble Hawksbill (LTS)","Python 3.10+ with rclpy","Isaac ROS GEMs for perception integration"],"hardware_recommendations":["NVIDIA Jetson AGX Orin (primary)","NVIDIA Jetson Orin NX (alternative)","NVIDIA Jetson Orin Nano for minimal configurations","NVIDIA RTX 4090 for model training and optimization"],"hardware_alternatives":["NVIDIA Jetson Orin Nano (budget option)","Laptop with discrete GPU for development","Cloud-based optimization with edge deployment"],"prerequisites":["Module 1: ROS 2 proficiency","Module 2: Simulation experience","Module 3 intro: AI-Robot brain concepts","Module 3.1: Synthetic data generation","Module 3.2: Isaac ROS GEMs implementation","Module 3.3: Nav2 bipedal navigation","Basic understanding of deep learning and neural networks"],"assessment_recommendations":["Inference optimization: Deploy optimized models on Jetson platform","Performance benchmark: Measure inference latency and throughput"],"dependencies":["03-ai-robot-brain/intro","03-ai-robot-brain/01-synthetic-data-generation","03-ai-robot-brain/02-isaac-ros-gems","03-ai-robot-brain/03-nav2-bipedal-navigation"]},"sidebar":"tutorialSidebar","previous":{"title":"Navigation for Bipedal Systems","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/ai-robot-brain/nav2-bipedal-navigation"},"next":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/vla/intro"}}');var o=i(4848),r=i(8453),a=i(8844),s=i(7589),l=i(6212),p=i(7639);i(1302);const c={id:"edge-inference-jetson",title:"Edge AI Inference",description:"Real-time inference optimization on NVIDIA Jetson platforms for humanoid robotics",personalization:!0,translation:"ur",learning_outcomes:["Optimize AI models using TensorRT for efficient inference on Jetson platforms","Implement real-time inference pipelines with predictable performance characteristics","Apply model quantization techniques to reduce computational requirements","Optimize power consumption for extended humanoid robot operation"],software_stack:["NVIDIA JetPack SDK","TensorRT 8.6+","CUDA 12.0+ with cuDNN","ROS 2 Humble Hawksbill (LTS)","Python 3.10+ with rclpy","Isaac ROS GEMs for perception integration"],hardware_recommendations:["NVIDIA Jetson AGX Orin (primary)","NVIDIA Jetson Orin NX (alternative)","NVIDIA Jetson Orin Nano for minimal configurations","NVIDIA RTX 4090 for model training and optimization"],hardware_alternatives:["NVIDIA Jetson Orin Nano (budget option)","Laptop with discrete GPU for development","Cloud-based optimization with edge deployment"],prerequisites:["Module 1: ROS 2 proficiency","Module 2: Simulation experience","Module 3 intro: AI-Robot brain concepts","Module 3.1: Synthetic data generation","Module 3.2: Isaac ROS GEMs implementation","Module 3.3: Nav2 bipedal navigation","Basic understanding of deep learning and neural networks"],assessment_recommendations:["Inference optimization: Deploy optimized models on Jetson platform","Performance benchmark: Measure inference latency and throughput"],dependencies:["03-ai-robot-brain/intro","03-ai-robot-brain/01-synthetic-data-generation","03-ai-robot-brain/02-isaac-ros-gems","03-ai-robot-brain/03-nav2-bipedal-navigation"]},d="Edge AI Inference",m={},u=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"TensorRT Optimization for Jetson Platforms",id:"tensorrt-optimization-for-jetson-platforms",level:2},{value:"Diagram Descriptions",id:"diagram-descriptions",level:3},{value:"Concrete Examples",id:"concrete-examples",level:3},{value:"Real-time Inference Performance",id:"real-time-inference-performance",level:2},{value:"Concrete Examples",id:"concrete-examples-1",level:3},{value:"Model Quantization Techniques",id:"model-quantization-techniques",level:2},{value:"Diagram Descriptions",id:"diagram-descriptions-1",level:3},{value:"Concrete Examples",id:"concrete-examples-2",level:3},{value:"Power Consumption Optimization",id:"power-consumption-optimization",level:2},{value:"Concrete Examples",id:"concrete-examples-3",level:3},{value:"Forward References to Capstone Project",id:"forward-references-to-capstone-project",level:2},{value:"Ethical &amp; Safety Considerations",id:"ethical--safety-considerations",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function f(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"edge-ai-inference",children:"Edge AI Inference"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Optimize AI models using TensorRT for efficient inference on Jetson platforms"}),"\n",(0,o.jsx)(n.li,{children:"Implement real-time inference pipelines with predictable performance characteristics"}),"\n",(0,o.jsx)(n.li,{children:"Apply model quantization techniques to reduce computational requirements"}),"\n",(0,o.jsx)(n.li,{children:"Optimize power consumption for extended humanoid robot operation"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"tensorrt-optimization-for-jetson-platforms",children:"TensorRT Optimization for Jetson Platforms"}),"\n",(0,o.jsx)(n.p,{children:"TensorRT optimization is essential for achieving real-time AI inference performance on NVIDIA Jetson platforms used in humanoid robots. TensorRT provides a high-performance deep learning inference optimizer and runtime that delivers low latency and high throughput for AI applications. For humanoid robots, TensorRT optimization enables complex perception and decision-making models to run efficiently on power-constrained edge devices."}),"\n",(0,o.jsx)(a.A,{type:"tip",title:"TensorRT for Robotics",children:(0,o.jsx)(n.p,{children:"TensorRT optimization enables complex perception and decision-making models to run efficiently on power-constrained edge devices, which is essential for humanoid robots operating in real-world environments."})}),"\n",(0,o.jsx)(n.p,{children:"The TensorRT optimization process converts trained neural network models into optimized inference engines that maximize GPU utilization and minimize latency. For humanoid robots, this optimization is crucial for achieving real-time performance on perception tasks including object detection, depth estimation, and scene understanding. The optimization process includes layer fusion, kernel auto-tuning, and memory optimization techniques."}),"\n",(0,o.jsx)(p.A,{title:"TensorRT Optimization Process",description:"Diagram showing the TensorRT optimization process from trained model to optimized inference engine",caption:"TensorRT optimization process from trained model to optimized inference engine"}),"\n",(0,o.jsx)(n.p,{children:"Model serialization in TensorRT creates optimized inference engines that can be efficiently loaded and executed on Jetson platforms. For humanoid robots, the serialized models must maintain the accuracy required for safe operation and achieve the performance needed for real-time response. The serialization process also includes optimization for the specific Jetson hardware configuration, which maximizes performance."}),"\n",(0,o.jsx)(l.A,{title:"TensorRT Model Optimization",problem:"Implement TensorRT optimization for an AI model on a Jetson platform for humanoid robot perception.",hints:["Use TensorRT Python API for model conversion","Implement proper calibration for INT8 quantization","Validate accuracy after optimization"],solution:'# Example TensorRT optimization implementation\nimport tensorrt as trt\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nimport numpy as np\n\nclass TensorRTOptimizer:\n  def __init__(self):\n      self.logger = trt.Logger(trt.Logger.WARNING)\n      self.builder = trt.Builder(self.logger)\n      self.network = None\n      self.engine = None\n\n  def create_optimized_engine(self, onnx_model_path, precision=\'fp16\'):\n      """Create optimized TensorRT engine from ONNX model"""\n      # Create network definition\n      network_flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n      self.network = self.builder.create_network(network_flags)\n      config = self.builder.create_builder_config()\n\n      # Parse ONNX model\n      parser = trt.OnnxParser(self.network, self.logger)\n      with open(onnx_model_path, \'rb\') as model:\n          if not parser.parse(model.read()):\n              for error in range(parser.num_errors):\n                  print(parser.get_error(error))\n              return None\n\n      # Configure optimization settings\n      config.max_workspace_size = 2 << 30  # 2GB\n\n      if precision == \'fp16\':\n          if self.builder.platform_has_fast_fp16:\n              config.set_flag(trt.BuilderFlag.FP16)\n      elif precision == \'int8\':\n          if self.builder.platform_has_fast_int8:\n              config.set_flag(trt.BuilderFlag.INT8)\n              # Set up INT8 calibration\n              config.int8_calibrator = self.create_calibrator()\n\n      # Build engine\n      self.engine = self.builder.build_engine(self.network, config)\n      return self.engine\n\n  def create_calibrator(self):\n      """Create INT8 calibrator for quantization"""\n      # Implementation of calibration data for INT8 quantization\n      pass\n\n  def optimize_model(self, model_path, output_path, precision=\'fp16\'):\n      """Optimize model and save as TensorRT engine"""\n      engine = self.create_optimized_engine(model_path, precision)\n\n      if engine is None:\n          print("Failed to create TensorRT engine")\n          return False\n\n      # Serialize engine to file\n      with open(output_path, \'wb\') as f:\n          f.write(engine.serialize())\n\n      print(f"Optimized engine saved to {output_path}")\n      return True\n\n# Example usage\noptimizer = TensorRTOptimizer()\noptimizer.optimize_model(\'model.onnx\', \'optimized_model.engine\', precision=\'int8\')'}),"\n",(0,o.jsx)(n.p,{children:"Dynamic shape support in TensorRT enables models to handle variable input sizes. This is important for humanoid robot applications where sensor data dimensions may vary. For example, different camera resolutions or variable point cloud sizes can be handled by the same optimized model. This flexibility is crucial for humanoid robots that may operate with different sensor configurations."}),"\n",(0,o.jsx)(s.A,{question:"What is a key benefit of TensorRT optimization for humanoid robots?",options:["Reduced model accuracy","Enabling complex perception models to run efficiently on power-constrained edge devices","Increased memory requirements","Slower inference performance"],correctAnswer:"Enabling complex perception models to run efficiently on power-constrained edge devices",explanation:"TensorRT optimization enables complex perception and decision-making models to run efficiently on power-constrained edge devices, which is essential for humanoid robots operating in real-world environments."}),"\n",(0,o.jsx)(n.p,{children:"Precision optimization in TensorRT includes support for various precision formats including FP32, FP16, INT8, and sparse operations. For humanoid robots, the choice of precision format involves balancing accuracy requirements with performance gains. INT8 quantization can provide significant performance improvements with minimal accuracy loss, which makes it suitable for many robotic applications."}),"\n",(0,o.jsx)(n.h3,{id:"diagram-descriptions",children:"Diagram Descriptions"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Diagram: Precision optimization showing different formats (FP32, FP16, INT8) and their trade-offs"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"concrete-examples",children:"Concrete Examples"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Example: Optimizing YOLO object detection model using TensorRT for real-time performance"}),"\n",(0,o.jsx)(n.li,{children:"Example: Converting depth estimation model to INT8 precision for improved inference speed"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"real-time-inference-performance",children:"Real-time Inference Performance"}),"\n",(0,o.jsx)(n.p,{children:"Real-time inference performance on Jetson platforms requires careful consideration of computational resources, memory management, and pipeline optimization to meet the timing requirements of humanoid robot applications. The performance optimization must balance accuracy, speed, and power consumption to enable sustained operation."}),"\n",(0,o.jsx)(a.A,{type:"note",title:"Real-time Performance",children:(0,o.jsx)(n.p,{children:"Real-time inference performance requires balancing accuracy, speed, and power consumption to meet the timing requirements of humanoid robot applications while enabling sustained operation."})}),"\n",(0,o.jsx)(n.p,{children:"Inference pipeline optimization involves the coordination of multiple processing stages to maximize throughput while minimizing end-to-end latency. For humanoid robots, the perception pipeline must process sensor data through multiple stages including preprocessing, inference, and post-processing while maintaining real-time performance. The optimization may include parallel processing and asynchronous execution to maximize resource utilization."}),"\n",(0,o.jsx)(p.A,{title:"Inference Pipeline Optimization",description:"Diagram showing inference pipeline optimization with multiple processing stages",caption:"Inference pipeline optimization with multiple processing stages for real-time performance"}),"\n",(0,o.jsx)(n.p,{children:"Memory management optimization ensures efficient use of GPU memory and system RAM for real-time processing. For humanoid robots, the inference pipeline must handle multiple data streams simultaneously while maintaining consistent performance. The optimization includes memory pooling, data pre-allocation, and efficient data transfer between CPU and GPU, which minimizes overhead."}),"\n",(0,o.jsx)(l.A,{title:"Inference Pipeline Optimization",problem:"Implement an optimized inference pipeline for real-time object detection on a Jetson platform.",hints:["Use asynchronous processing for pipeline stages","Implement memory pooling to reduce allocation overhead","Optimize data transfer between CPU and GPU"],solution:'# Example real-time inference pipeline\nimport numpy as np\nimport threading\nimport queue\nimport time\nfrom collections import deque\n\nclass RealTimeInferencePipeline:\n  def __init__(self, model_path):\n      self.model_path = model_path\n      self.input_queue = queue.Queue(maxsize=5)\n      self.output_queue = queue.Queue(maxsize=5)\n\n      # Memory pools for efficient allocation\n      self.memory_pool = deque(maxlen=10)\n      self.gpu_memory_pool = deque(maxlen=10)\n\n      # Pipeline threads\n      self.preprocessing_thread = None\n      self.inference_thread = None\n      self.postprocessing_thread = None\n      self.pipeline_running = False\n\n      # Performance metrics\n      self.frame_count = 0\n      self.start_time = time.time()\n      self.fps_history = deque(maxlen=100)\n\n  def initialize_pipeline(self):\n      """Initialize the inference pipeline"""\n      # Load optimized model\n      self.load_optimized_model()\n\n      # Initialize memory pools\n      self.initialize_memory_pools()\n\n      return True\n\n  def load_optimized_model(self):\n      """Load the TensorRT optimized model"""\n      # In real implementation, this would load the TensorRT engine\n      print(f"Loading optimized model from {self.model_path}")\n      # self.engine = self.load_tensorrt_engine(self.model_path)\n\n  def initialize_memory_pools(self):\n      """Initialize memory pools for efficient allocation"""\n      for _ in range(5):\n          # Create reusable memory buffers\n          input_buffer = np.zeros((1, 3, 640, 640), dtype=np.float32)  # Example input size\n          output_buffer = np.zeros((1, 100, 85), dtype=np.float32)     # Example output size\n\n          self.memory_pool.append({\n              \'input\': input_buffer,\n              \'output\': output_buffer\n          })\n\n  def preprocessing_stage(self, input_data):\n      """Preprocessing stage of the pipeline"""\n      # Normalize and format input data\n      processed_data = self.normalize_input(input_data)\n      return processed_data\n\n  def normalize_input(self, input_data):\n      """Normalize input data for the model"""\n      # Example normalization (adjust based on your model requirements)\n      input_data = input_data.astype(np.float32) / 255.0\n      input_data = np.transpose(input_data, (2, 0, 1))  # HWC to CHW\n      input_data = np.expand_dims(input_data, axis=0)   # Add batch dimension\n      return input_data\n\n  def inference_stage(self, processed_data):\n      """Inference stage using the optimized model"""\n      # In real implementation, this would run inference on GPU\n      # result = self.run_tensorrt_inference(processed_data)\n\n      # Simulate inference with timing\n      start_time = time.time()\n      # Simulate model execution\n      time.sleep(0.01)  # Simulate 10ms inference time\n      inference_time = time.time() - start_time\n\n      # Simulate output\n      result = np.random.random((1, 100, 85)).astype(np.float32)  # Example output\n\n      return result, inference_time\n\n  def postprocessing_stage(self, inference_result):\n      """Postprocessing stage of the pipeline"""\n      # Process inference results\n      detections = self.process_detections(inference_result)\n      return detections\n\n  def process_detections(self, inference_result):\n      """Process detection results from the model"""\n      # Example postprocessing (adjust based on your model)\n      # Apply NMS, thresholding, etc.\n      detections = []\n\n      # Simulate processing\n      for i in range(min(10, len(inference_result[0]))):  # Process top 10 detections\n          detection = {\n              \'bbox\': [0, 0, 100, 100],  # Example bounding box\n              \'confidence\': float(np.random.random()),\n              \'class_id\': int(np.random.randint(0, 80))\n          }\n          detections.append(detection)\n\n      return detections\n\n  def pipeline_worker(self):\n      """Main pipeline worker that processes data through all stages"""\n      while self.pipeline_running:\n          try:\n              # Get input data\n              input_data = self.input_queue.get(timeout=1.0)\n\n              # Process through pipeline stages\n              start_time = time.time()\n\n              # Preprocessing\n              processed_data = self.preprocessing_stage(input_data)\n\n              # Inference\n              inference_result, inference_time = self.inference_stage(processed_data)\n\n              # Postprocessing\n              final_result = self.postprocessing_stage(inference_result)\n\n              # Calculate total processing time\n              total_time = time.time() - start_time\n\n              # Calculate FPS\n              self.frame_count += 1\n              current_time = time.time()\n              elapsed_time = current_time - self.start_time\n              current_fps = self.frame_count / elapsed_time if elapsed_time > 0 else 0\n              self.fps_history.append(current_fps)\n\n              # Put result in output queue\n              output_item = {\n                  \'detections\': final_result,\n                  \'processing_time\': total_time,\n                  \'inference_time\': inference_time,\n                  \'fps\': current_fps\n              }\n\n              if not self.output_queue.full():\n                  self.output_queue.put(output_item)\n\n          except queue.Empty:\n              continue\n          except Exception as e:\n              print(f"Pipeline error: {e}")\n              continue\n\n  def start_pipeline(self):\n      """Start the inference pipeline"""\n      self.pipeline_running = True\n      self.preprocessing_thread = threading.Thread(target=self.pipeline_worker)\n      self.preprocessing_thread.start()\n\n  def stop_pipeline(self):\n      """Stop the inference pipeline"""\n      self.pipeline_running = False\n      if self.preprocessing_thread:\n          self.preprocessing_thread.join(timeout=2.0)\n\n  def process_frame(self, frame_data):\n      """Process a single frame through the pipeline"""\n      if not self.input_queue.full():\n          self.input_queue.put(frame_data)\n          return True\n      return False\n\n  def get_results(self):\n      """Get results from the pipeline"""\n      if not self.output_queue.empty():\n          return self.output_queue.get()\n      return None\n\n  def get_performance_metrics(self):\n      """Get performance metrics"""\n      if self.fps_history:\n          avg_fps = sum(self.fps_history) / len(self.fps_history)\n      else:\n          avg_fps = 0\n\n      return {\n          \'current_fps\': self.fps_history[-1] if self.fps_history else 0,\n          \'average_fps\': avg_fps,\n          \'frame_count\': self.frame_count,\n          \'memory_usage\': len(self.memory_pool)\n      }\n\n# Example usage\npipeline = RealTimeInferencePipeline(\'optimized_model.engine\')\npipeline.initialize_pipeline()\npipeline.start_pipeline()\n\n# Simulate processing frames\nfor i in range(10):\n  # Simulate input frame (random RGB image)\n  input_frame = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)\n  pipeline.process_frame(input_frame)\n\n  # Get results if available\n  results = pipeline.get_results()\n  if results:\n      print(f"Frame {i}: FPS={results[\'fps\']:.2f}, Processing time={results[\'processing_time\']:.4f}s")\n\n# Get performance metrics\nmetrics = pipeline.get_performance_metrics()\nprint(f"Performance: {metrics[\'average_fps\']:.2f} FPS average")\n\npipeline.stop_pipeline()'}),"\n",(0,o.jsx)(n.p,{children:"Multi-model inference optimization enables humanoid robots to run multiple AI models simultaneously that perform different tasks such as perception, planning, and control. The optimization involves efficient scheduling and resource allocation that maximizes the utilization of the Jetson platform while maintaining the performance requirements of each model. For humanoid robots, this may include prioritizing safety-critical models over less time-sensitive tasks."}),"\n",(0,o.jsx)(n.p,{children:"Performance monitoring and profiling tools enable the measurement and optimization of inference performance on Jetson platforms. For humanoid robots, continuous monitoring of inference performance helps identify bottlenecks and optimizes the system for sustained operation. The profiling tools provide insights into GPU utilization, memory bandwidth, and computational efficiency."}),"\n",(0,o.jsx)(n.h3,{id:"concrete-examples-1",children:"Concrete Examples"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Example: Optimizing inference pipeline for real-time object detection and tracking"}),"\n",(0,o.jsx)(n.li,{children:"Example: Multi-model inference running perception and planning models simultaneously"}),"\n"]}),"\n",(0,o.jsx)(s.A,{question:"What is a key consideration for real-time inference performance on Jetson platforms?",options:["Maximizing memory requirements","Balancing accuracy, speed, and power consumption to meet timing requirements","Reducing computational resources","Increasing model complexity"],correctAnswer:"Balancing accuracy, speed, and power consumption to meet timing requirements",explanation:"Real-time inference performance requires balancing accuracy, speed, and power consumption to meet the timing requirements of humanoid robot applications while enabling sustained operation."}),"\n",(0,o.jsx)(n.h2,{id:"model-quantization-techniques",children:"Model Quantization Techniques"}),"\n",(0,o.jsx)(n.p,{children:"Model quantization techniques reduce the computational requirements and memory footprint of deep learning models while maintaining acceptable accuracy for humanoid robot applications. Quantization converts high-precision models to lower precision representations that can execute more efficiently on edge platforms."}),"\n",(0,o.jsx)(a.A,{type:"warning",title:"Quantization Considerations",children:(0,o.jsx)(n.p,{children:"Model quantization reduces computational requirements and memory footprint while maintaining acceptable accuracy, but must be carefully validated to ensure safety-critical functions are not compromised."})}),"\n",(0,o.jsx)(n.p,{children:"INT8 quantization provides significant performance improvements with minimal accuracy loss, converting 32-bit floating-point models to 8-bit integer representations. For humanoid robots, INT8 quantization can double inference throughput while reducing power consumption. The quantization process requires calibration with representative data to maintain accuracy in the reduced precision format."}),"\n",(0,o.jsx)(p.A,{title:"Model Quantization Process",description:"Diagram showing the model quantization process showing precision reduction from FP32 to INT8",caption:"Model quantization process showing precision reduction from FP32 to INT8"}),"\n",(0,o.jsx)(n.p,{children:"Post-training quantization enables quantization of pre-trained models without requiring retraining. This makes it suitable for deploying existing models on Jetson platforms. For humanoid robots, post-training quantization allows for rapid deployment of optimized models that maintain the performance characteristics of the original model. The calibration process uses representative data from the robot's operating environment."}),"\n",(0,o.jsx)(l.A,{title:"Model Quantization Implementation",problem:"Implement INT8 quantization for an AI model using TensorRT for deployment on Jetson platform.",hints:["Create a calibration dataset representative of robot's operating environment","Implement TensorRT INT8 calibrator","Validate accuracy after quantization"],solution:'# Example INT8 quantization implementation\nimport tensorrt as trt\nimport numpy as np\nimport os\n\nclass Int8Calibrator(trt.IInt8EntropyCalibrator2):\n  def __init__(self, calibration_files, batch_size, input_shape):\n      trt.IInt8EntropyCalibrator2.__init__(self)\n\n      self.calibration_files = calibration_files\n      self.batch_size = batch_size\n      self.input_shape = input_shape\n      self.current_index = 0\n\n      # Create a buffer for the calibration data\n      self.device_input = cuda.mem_alloc(trt.volume(self.input_shape) * self.batch_size * np.dtype(np.float32).itemsize)\n\n  def get_batch_size(self):\n      return self.batch_size\n\n  def get_batch(self, names):\n      if self.current_index + self.batch_size > len(self.calibration_files):\n          return None\n\n      batch = np.zeros((self.batch_size, *self.input_shape[1:]), dtype=np.float32)\n\n      for i in range(self.batch_size):\n          # Load calibration data (this would be your actual data loading logic)\n          # For example, loading images for vision models\n          file_path = self.calibration_files[self.current_index + i]\n          # Load and preprocess your calibration data here\n          # image = self.load_and_preprocess_image(file_path)\n          # batch[i] = image\n\n          # For demonstration, using random data\n          batch[i] = np.random.random(self.input_shape[1:]).astype(np.float32)\n\n      # Copy to device\n      cuda.memcpy_htod(self.device_input, batch)\n      self.current_index += self.batch_size\n\n      return [int(self.device_input)]\n\n  def read_calibration_cache(self):\n      # Try to read calibration cache from file\n      cache_file = "calibration.cache"\n      if os.path.exists(cache_file):\n          with open(cache_file, "rb") as f:\n              return f.read()\n      return None\n\n  def write_calibration_cache(self, cache):\n      # Write calibration cache to file\n      with open("calibration.cache", "wb") as f:\n          f.write(cache)\n\ndef quantize_model_to_int8(onnx_model_path, output_engine_path, calibration_files):\n  """Quantize a model to INT8 using TensorRT"""\n\n  # Initialize TensorRT\n  logger = trt.Logger(trt.Logger.WARNING)\n  builder = trt.Builder(logger)\n\n  # Create network\n  network_flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n  network = builder.create_network(network_flags)\n\n  # Parse ONNX model\n  parser = trt.OnnxParser(network, logger)\n  with open(onnx_model_path, \'rb\') as model:\n      if not parser.parse(model.read()):\n          print("Failed to parse ONNX model")\n          for error in range(parser.num_errors):\n              print(parser.get_error(error))\n          return False\n\n  # Create builder config\n  config = builder.create_builder_config()\n\n  # Enable INT8 precision\n  if builder.platform_has_fast_int8:\n      config.set_flag(trt.BuilderFlag.INT8)\n\n      # Set up INT8 calibrator\n      input_shape = (1, 3, 640, 640)  # Example input shape (adjust as needed)\n      calibrator = Int8Calibrator(calibration_files, batch_size=1, input_shape=input_shape)\n      config.int8_calibrator = calibrator\n\n      # Set workspace size\n      config.max_workspace_size = 2 << 30  # 2GB\n  else:\n      print("INT8 not supported on this platform")\n      return False\n\n  # Build engine\n  print("Building INT8 engine...")\n  engine = builder.build_engine(network, config)\n\n  if engine is None:\n      print("Failed to build INT8 engine")\n      return False\n\n  # Serialize engine to file\n  with open(output_engine_path, \'wb\') as f:\n      f.write(engine.serialize())\n\n  print(f"INT8 quantized engine saved to {output_engine_path}")\n  return True\n\n# Example usage\ncalibration_dataset = [\n  # Add paths to your calibration images/data here\n  # \'calibration_image_1.jpg\',\n  # \'calibration_image_2.jpg\',\n  # ...\n]\n\n# quantize_model_to_int8(\'model.onnx\', \'int8_model.engine\', calibration_dataset)'}),"\n",(0,o.jsx)(n.p,{children:"Quantization-aware training incorporates quantization effects during the training process, which results in models that are optimized for low-precision inference. For humanoid robots, quantization-aware training can maintain higher accuracy compared to post-training quantization, especially for models with complex architectures or sensitive operations. The training process simulates the quantization effects and optimizes the model for the target precision."}),"\n",(0,o.jsx)(n.p,{children:"Mixed precision quantization allows different parts of a model to use different precision formats based on their sensitivity to quantization effects. For humanoid robots, this approach can maintain high accuracy in critical parts of the model while achieving performance gains in less sensitive components. The mixed precision approach optimizes the trade-off between accuracy and performance."}),"\n",(0,o.jsx)(n.h3,{id:"diagram-descriptions-1",children:"Diagram Descriptions"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Diagram: Mixed precision quantization with different formats for different model parts"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"concrete-examples-2",children:"Concrete Examples"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Example: INT8 quantization of object detection model for improved inference speed"}),"\n",(0,o.jsx)(n.li,{children:"Example: Post-training quantization of depth estimation model for Jetson deployment"}),"\n"]}),"\n",(0,o.jsx)(s.A,{question:"What is a key advantage of INT8 quantization for humanoid robots?",options:["Increased memory requirements","Significant performance improvements with minimal accuracy loss","Reduced computational capabilities","Slower inference speed"],correctAnswer:"Significant performance improvements with minimal accuracy loss",explanation:"INT8 quantization provides significant performance improvements with minimal accuracy loss, converting 32-bit floating-point models to 8-bit integer representations, which can double inference throughput while reducing power consumption."}),"\n",(0,o.jsx)(n.h2,{id:"power-consumption-optimization",children:"Power Consumption Optimization"}),"\n",(0,o.jsx)(n.p,{children:"Power consumption optimization is critical for humanoid robots that require extended autonomous operation on battery power. The optimization involves balancing computational performance with power efficiency to maximize operational time while maintaining the required AI capabilities."}),"\n",(0,o.jsx)(a.A,{type:"danger",title:"Power Optimization",children:(0,o.jsx)(n.p,{children:"Power consumption optimization is critical for humanoid robots requiring extended autonomous operation on battery power, balancing computational performance with power efficiency to maximize operational time."})}),"\n",(0,o.jsx)(n.p,{children:"Dynamic voltage and frequency scaling (DVFS) in Jetson platforms enables power optimization that adjusts the operating frequency and voltage based on computational requirements. For humanoid robots, DVFS can reduce power consumption during periods of lower computational demand while maintaining performance when needed. The power management must consider the robot's operational patterns and performance requirements."}),"\n",(0,o.jsx)(p.A,{title:"Power Consumption Optimization",description:"Diagram showing power consumption optimization with DVFS and model pruning techniques",caption:"Power consumption optimization with DVFS and model pruning techniques"}),"\n",(0,o.jsx)(n.p,{children:"Model pruning techniques remove redundant or less important connections in neural networks, which reduces computational requirements and power consumption. For humanoid robots, structured pruning can maintain model accuracy while significantly reducing the computational load. The pruning process must consider the impact on safety-critical perception and control tasks."}),"\n",(0,o.jsx)(l.A,{title:"Power Optimization System",problem:"Implement a power optimization system for AI inference on Jetson platform for humanoid robot.",hints:["Monitor power consumption in real-time","Implement dynamic model switching based on power availability","Use DVFS to adjust performance based on requirements"],solution:"# Example power optimization system\nimport subprocess\nimport time\nimport threading\nfrom collections import deque\nimport psutil\n\nclass PowerOptimizationSystem:\n  def __init__(self):\n      self.power_thresholds = {\n          'low_power': 15.0,    # watts\n          'normal': 20.0,       # watts\n          'high_power': 25.0    # watts\n      }\n\n      self.battery_thresholds = {\n          'critical': 10,       # percent\n          'low': 20,            # percent\n          'normal': 80          # percent\n      }\n\n      self.current_power_mode = 'normal'\n      self.optimization_active = True\n      self.power_monitoring_thread = None\n      self.power_history = deque(maxlen=100)\n\n      # Model complexity levels\n      self.model_complexity = {\n          'high': {'fps': 30, 'power': 25.0, 'accuracy': 0.95},\n          'normal': {'fps': 20, 'power': 18.0, 'accuracy': 0.92},\n          'low': {'fps': 10, 'power': 12.0, 'accuracy': 0.85}\n      }\n\n  def get_jetson_power_consumption(self):\n      \"\"\"Get current power consumption from Jetson platform\"\"\"\n      try:\n          # On Jetson platforms, power consumption can be read from system files\n          # This is a simplified example - actual implementation would vary by Jetson model\n          power_path = \"/sys/devices/7000c400.i2c/i2c-1/1-0040/iio:device0/in_power0_input\"\n          if subprocess.run(['ls', power_path], capture_output=True).returncode == 0:\n              with open(power_path, 'r') as f:\n                  power_mw = int(f.read().strip())\n                  return power_mw / 1000.0  # Convert mW to W\n          else:\n              # Fallback to CPU power estimation\n              cpu_percent = psutil.cpu_percent()\n              estimated_power = 5.0 + (cpu_percent / 100.0) * 15.0  # Estimate\n              return estimated_power\n      except:\n          # Return a default estimate if power reading fails\n          return 15.0\n\n  def get_battery_level(self):\n      \"\"\"Get current battery level\"\"\"\n      try:\n          battery = psutil.sensors_battery()\n          if battery:\n              return battery.percent\n          else:\n              # If no battery sensor, return a default value\n              return 100  # Assume plugged in\n      except:\n          return 100\n\n  def adjust_power_mode(self, current_power, battery_level):\n      \"\"\"Adjust power mode based on current consumption and battery level\"\"\"\n      new_mode = 'normal'\n\n      # Check battery level first (highest priority)\n      if battery_level <= self.battery_thresholds['critical']:\n          new_mode = 'low'\n      elif battery_level <= self.battery_thresholds['low']:\n          if current_power > self.power_thresholds['normal']:\n              new_mode = 'low'\n\n      # Check power consumption\n      elif current_power > self.power_thresholds['high_power']:\n          new_mode = 'low'\n      elif current_power > self.power_thresholds['normal'] and battery_level < self.battery_thresholds['normal']:\n          new_mode = 'normal'\n      elif current_power < self.power_thresholds['low_power'] and battery_level > self.battery_thresholds['normal']:\n          new_mode = 'high'\n\n      if new_mode != self.current_power_mode:\n          self.set_power_mode(new_mode)\n          return True  # Mode changed\n\n      return False\n\n  def set_power_mode(self, mode):\n      \"\"\"Set the power mode and apply corresponding optimizations\"\"\"\n      print(f\"Switching to {mode} power mode\")\n\n      if mode == 'high':\n          # Higher performance, more power\n          self.apply_performance_settings()\n      elif mode == 'low':\n          # Lower performance, less power\n          self.apply_power_saving_settings()\n      else:  # normal\n          # Balanced performance and power\n          self.apply_balanced_settings()\n\n      self.current_power_mode = mode\n\n  def apply_performance_settings(self):\n      \"\"\"Apply high-performance settings\"\"\"\n      # Set higher CPU/GPU frequencies\n      # Use more complex models\n      # Increase inference frequency\n      print(\"Applying high-performance settings\")\n      # In real implementation, this would adjust system settings\n\n  def apply_power_saving_settings(self):\n      \"\"\"Apply power-saving settings\"\"\"\n      # Set lower CPU/GPU frequencies\n      # Use simpler models\n      # Reduce inference frequency\n      # Increase sleep intervals\n      print(\"Applying power-saving settings\")\n      # In real implementation, this would adjust system settings\n\n  def apply_balanced_settings(self):\n      \"\"\"Apply balanced settings\"\"\"\n      # Moderate CPU/GPU frequencies\n      # Use medium-complexity models\n      # Balanced inference frequency\n      print(\"Applying balanced settings\")\n      # In real implementation, this would adjust system settings\n\n  def get_optimal_model_config(self):\n      \"\"\"Get the optimal model configuration based on current power mode\"\"\"\n      return self.model_complexity[self.current_power_mode]\n\n  def monitor_power_consumption(self):\n      \"\"\"Continuous power monitoring loop\"\"\"\n      while self.optimization_active:\n          try:\n              current_power = self.get_jetson_power_consumption()\n              battery_level = self.get_battery_level()\n\n              # Store in history\n              self.power_history.append({\n                  'power': current_power,\n                  'battery': battery_level,\n                  'timestamp': time.time()\n              })\n\n              # Adjust power mode if needed\n              mode_changed = self.adjust_power_mode(current_power, battery_level)\n\n              if mode_changed:\n                  print(f\"Power mode changed to {self.current_power_mode}\")\n                  print(f\"Current power: {current_power:.2f}W, Battery: {battery_level:.1f}%\")\n\n              # Sleep for 1 second\n              time.sleep(1.0)\n\n          except Exception as e:\n              print(f\"Power monitoring error: {e}\")\n              time.sleep(1.0)\n              continue\n\n  def start_power_optimization(self):\n      \"\"\"Start the power optimization system\"\"\"\n      print(\"Starting power optimization system...\")\n\n      self.power_monitoring_thread = threading.Thread(\n          target=self.monitor_power_consumption,\n          daemon=True\n      )\n      self.power_monitoring_thread.start()\n\n  def stop_power_optimization(self):\n      \"\"\"Stop the power optimization system\"\"\"\n      self.optimization_active = False\n      if self.power_monitoring_thread:\n          self.power_monitoring_thread.join(timeout=2.0)\n\n  def get_power_efficiency_metrics(self):\n      \"\"\"Get power efficiency metrics\"\"\"\n      if not self.power_history:\n          return {\n              'average_power': 0,\n              'min_power': 0,\n              'max_power': 0,\n              'battery_depletion_rate': 0\n          }\n\n      powers = [entry['power'] for entry in self.power_history]\n      battery_levels = [entry['battery'] for entry in self.power_history]\n\n      avg_power = sum(powers) / len(powers)\n      min_power = min(powers)\n      max_power = max(powers)\n\n      # Calculate battery depletion rate if we have enough history\n      if len(battery_levels) > 1:\n          time_span = self.power_history[-1]['timestamp'] - self.power_history[0]['timestamp']\n          battery_change = battery_levels[0] - battery_levels[-1]\n          if time_span > 0:\n              depletion_rate = (battery_change / time_span) * 3600  # % per hour\n          else:\n              depletion_rate = 0\n      else:\n          depletion_rate = 0\n\n      return {\n          'average_power': avg_power,\n          'min_power': min_power,\n          'max_power': max_power,\n          'battery_depletion_rate': depletion_rate\n      }\n\n# Example usage\npower_system = PowerOptimizationSystem()\npower_system.start_power_optimization()\n\nprint(\"Power optimization system running...\")\n\ntry:\n  # Let it run for a while\n  time.sleep(10)\n\n  # Check metrics\n  metrics = power_system.get_power_efficiency_metrics()\n  print(f\"Power metrics: {metrics}\")\n\nexcept KeyboardInterrupt:\n  print(\"Stopping power optimization system...\")\n\npower_system.stop_power_optimization()\nprint(\"Power optimization system stopped.\")"}),"\n",(0,o.jsx)(n.p,{children:"Efficient model architectures such as MobileNets, EfficientNets, and other lightweight networks are designed for edge deployment and have reduced computational requirements. For humanoid robots, selecting appropriate model architectures from the beginning of development can provide significant power savings. The architecture choice involves balancing accuracy, speed, and power consumption for the specific robot application."}),"\n",(0,o.jsx)(n.p,{children:"Power monitoring and management systems enable humanoid robots to optimize their AI workload based on available power and operational requirements. The system can dynamically adjust the complexity of AI tasks, reduce frame rates, and switch to lower-precision models when power conservation is needed. For humanoid robots operating in the field, intelligent power management extends operational time."}),"\n",(0,o.jsx)(p.A,{title:"Power Management System",description:"Diagram showing power management system with AI workload optimization based on available power",caption:"Power management system showing AI workload optimization based on available power"}),"\n",(0,o.jsx)(n.h3,{id:"concrete-examples-3",children:"Concrete Examples"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Example: Using DVFS to reduce power consumption during idle periods of robot operation"}),"\n",(0,o.jsx)(n.li,{children:"Example: Implementing model pruning for efficient perception models on Jetson platform"}),"\n"]}),"\n",(0,o.jsx)(s.A,{question:"Why is power consumption optimization critical for humanoid robots?",options:["To increase computational complexity","Because humanoid robots require extended autonomous operation on battery power","To reduce model accuracy","To increase memory requirements"],correctAnswer:"Because humanoid robots require extended autonomous operation on battery power",explanation:"Power consumption optimization is critical for humanoid robots requiring extended autonomous operation on battery power, balancing computational performance with power efficiency to maximize operational time."}),"\n",(0,o.jsx)(n.h2,{id:"forward-references-to-capstone-project",children:"Forward References to Capstone Project"}),"\n",(0,o.jsx)(n.p,{children:"The edge inference optimization techniques covered in this chapter are essential. These are for deploying your Autonomous Humanoid capstone project's AI systems on the Jetson platform."}),"\n",(0,o.jsx)(n.p,{children:"The TensorRT optimization will enable real-time performance for your perception and control systems. The quantization techniques will reduce computational requirements for sustained operation. The power optimization strategies will ensure your humanoid robot can operate effectively during extended missions."}),"\n",(0,o.jsx)(n.h2,{id:"ethical--safety-considerations",children:"Ethical & Safety Considerations"}),"\n",(0,o.jsx)(n.p,{children:"The deployment of AI inference systems on edge platforms for humanoid robots raises important ethical and safety considerations. These relate to system reliability and performance degradation."}),"\n",(0,o.jsx)(a.A,{type:"danger",title:"Safety in Optimization",children:(0,o.jsx)(n.p,{children:"The optimization process must not compromise safety-critical functions of the robot, and power management systems must ensure safety-critical AI tasks continue to operate reliably even under power constraints."})}),"\n",(0,o.jsx)(n.p,{children:"The optimization process must not compromise the safety-critical functions of the robot. The power management systems must ensure that safety-critical AI tasks continue to operate reliably. This occurs even under power constraints. Additionally, the quantization and optimization processes must be validated to ensure they do not introduce unexpected behaviors that could compromise safety."}),"\n",(0,o.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"TensorRT optimization enables efficient AI inference on Jetson platforms for humanoid robots"}),"\n",(0,o.jsx)(n.li,{children:"Real-time performance requires pipeline optimization and efficient resource management"}),"\n",(0,o.jsx)(n.li,{children:"Model quantization reduces computational requirements while maintaining accuracy"}),"\n",(0,o.jsx)(n.li,{children:"Power optimization is critical for extended humanoid robot operation"}),"\n",(0,o.jsx)(n.li,{children:"Mixed precision approaches balance accuracy and performance requirements"}),"\n",(0,o.jsx)(n.li,{children:"Performance monitoring enables continuous optimization of inference systems"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(f,{...e})}):f(e)}},6212(e,n,i){i.d(n,{A:()=>r});var t=i(6540),o=i(4848);const r=({title:e,problem:n,solution:i,hints:r=[],initialCode:a="",className:s=""})=>{const[l,p]=(0,t.useState)(a),[c,d]=(0,t.useState)(!1),[m,u]=(0,t.useState)(!1),[f,h]=(0,t.useState)(!1),[g,_]=(0,t.useState)(null);return(0,o.jsxs)("div",{className:`exercise-component ${s}`,style:{border:"1px solid #ddd",borderRadius:"8px",padding:"1rem",margin:"1rem 0",backgroundColor:"#fff"},children:[(0,o.jsx)("h4",{style:{margin:"0 0 1rem 0"},children:e}),(0,o.jsxs)("div",{style:{marginBottom:"1rem"},children:[(0,o.jsx)("h5",{style:{margin:"0.5rem 0",color:"#202124"},children:"Problem:"}),(0,o.jsx)("div",{style:{padding:"0.5rem",backgroundColor:"#f9f9f9",borderRadius:"4px"},children:n})]}),r.length>0&&(0,o.jsxs)("div",{style:{marginBottom:"1rem"},children:[(0,o.jsx)("button",{onClick:()=>u(!m),style:{padding:"0.5rem 1rem",backgroundColor:"#fbbc04",color:"white",border:"none",borderRadius:"4px",cursor:"pointer",marginBottom:"0.5rem"},children:m?"Hide Hint":"Show Hint"}),m&&(0,o.jsxs)("div",{style:{padding:"0.5rem",backgroundColor:"#fef7e0",borderRadius:"4px",border:"1px solid #fbbc04"},children:[(0,o.jsx)("strong",{children:"Hint:"})," ",r[0]]})]}),(0,o.jsxs)("div",{style:{marginBottom:"1rem"},children:[(0,o.jsx)("h5",{style:{margin:"0.5rem 0",color:"#202124"},children:"Your Solution:"}),(0,o.jsx)("textarea",{value:l,onChange:e=>p(e.target.value),style:{width:"100%",minHeight:"150px",padding:"0.5rem",fontFamily:"monospace",border:"1px solid #ddd",borderRadius:"4px",fontSize:"0.9rem"},placeholder:"Write your solution here..."})]}),(0,o.jsxs)("div",{style:{display:"flex",gap:"0.5rem",marginBottom:"1rem"},children:[(0,o.jsx)("button",{onClick:()=>{_({success:!0,message:"Code executed successfully! Check your logic against the solution."}),h(!0)},style:{padding:"0.5rem 1rem",backgroundColor:"#4caf50",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Run Code"}),(0,o.jsx)("button",{onClick:()=>d(!c),style:{padding:"0.5rem 1rem",backgroundColor:"#2196f3",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:c?"Hide Solution":"Show Solution"}),(0,o.jsx)("button",{onClick:()=>{p(a),d(!1),u(!1),h(!1),_(null)},style:{padding:"0.5rem 1rem",backgroundColor:"#9e9e9e",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Reset"})]}),f&&g&&(0,o.jsxs)("div",{style:{padding:"0.75rem",borderRadius:"4px",backgroundColor:g.success?"#e8f5e9":"#ffebee",border:"1px solid "+(g.success?"#4caf50":"#f44336"),marginBottom:"1rem"},children:[(0,o.jsx)("strong",{children:"Status:"})," ",g.message]}),c&&(0,o.jsxs)("div",{style:{padding:"0.5rem",backgroundColor:"#e8f5e9",borderRadius:"4px",border:"1px solid #4caf50"},children:[(0,o.jsx)("h5",{style:{margin:"0.5rem 0",color:"#202124"},children:"Solution:"}),(0,o.jsx)("pre",{style:{padding:"0.5rem",backgroundColor:"#f1f8e9",borderRadius:"4px",overflowX:"auto",whiteSpace:"pre-wrap"},children:i})]})]})}},7589(e,n,i){i.d(n,{A:()=>r});var t=i(6540),o=i(4848);const r=({question:e,options:n,correctAnswer:i,explanation:r,className:a=""})=>{const s=Array.isArray(n)?n:n&&"string"==typeof n?n.split("||"):[],[l,p]=(0,t.useState)(null),[c,d]=(0,t.useState)(!1),[m,u]=(0,t.useState)(!1),f=e=>{if(c)return;p(e);u(e===i),d(!0)},h=e=>c?e===i?{padding:"0.75rem",margin:"0.5rem 0",border:"1px solid #4caf50",borderRadius:"4px",backgroundColor:"#e8f5e9",fontWeight:"bold"}:e===l&&e!==i?{padding:"0.75rem",margin:"0.5rem 0",border:"1px solid #f44336",borderRadius:"4px",backgroundColor:"#ffebee"}:{padding:"0.75rem",margin:"0.5rem 0",border:"1px solid #ddd",borderRadius:"4px",backgroundColor:"#f5f5f5"}:{padding:"0.75rem",margin:"0.5rem 0",cursor:"pointer",border:"1px solid #ddd",borderRadius:"4px",backgroundColor:l===e?"#e3f2fd":"#fff"};return(0,o.jsxs)("div",{className:`quiz-component ${a}`,style:{border:"1px solid #ddd",borderRadius:"8px",padding:"1rem",margin:"1rem 0",backgroundColor:"#fff"},children:[(0,o.jsx)("h4",{style:{margin:"0 0 1rem 0"},children:e}),(0,o.jsx)("div",{children:s.map((n,i)=>(0,o.jsxs)("div",{style:h(n),onClick:()=>f(n),children:[(0,o.jsx)("input",{type:"radio",name:`quiz-${e}`,value:n,checked:l===n,onChange:()=>{},disabled:c,style:{marginRight:"0.5rem"}}),n]},i))}),c&&(0,o.jsxs)("div",{style:{marginTop:"1rem",padding:"0.75rem",borderRadius:"4px",backgroundColor:m?"#e8f5e9":"#ffebee",border:"1px solid "+(m?"#4caf50":"#f44336")},children:[(0,o.jsx)("p",{style:{margin:"0.5rem 0",fontWeight:"bold"},children:m?"\u2705 Correct!":"\u274c Incorrect"}),r&&(0,o.jsxs)("p",{style:{margin:"0.5rem 0"},children:[(0,o.jsx)("strong",{children:"Explanation:"})," ",r]})]}),c?(0,o.jsx)("button",{onClick:()=>{p(null),d(!1),u(!1)},style:{marginTop:"1rem",padding:"0.5rem 1rem",backgroundColor:"#2196f3",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Try Again"}):l&&(0,o.jsx)("button",{onClick:()=>f(l),style:{marginTop:"1rem",padding:"0.5rem 1rem",backgroundColor:"#4caf50",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Submit Answer"})]})}},7639(e,n,i){i.d(n,{A:()=>o});i(6540);var t=i(4848);const o=({title:e,description:n,src:i,alt:o,caption:r,className:a=""})=>(0,t.jsxs)("div",{className:`diagram-component ${a}`,style:{textAlign:"center",margin:"1.5rem 0",padding:"1rem",border:"1px solid #eee",borderRadius:"8px",backgroundColor:"#fafafa"},children:[e&&(0,t.jsx)("h5",{style:{margin:"0 0 1rem 0",color:"#202124",fontSize:"1rem",fontWeight:"bold"},children:e}),(0,t.jsx)("div",{style:{display:"flex",justifyContent:"center",alignItems:"center",margin:"0 auto",maxWidth:"100%"},children:i?(0,t.jsx)("img",{src:i,alt:o||e||"Diagram",style:{maxWidth:"100%",height:"auto",border:"1px solid #ddd",borderRadius:"4px"}}):(0,t.jsx)("div",{style:{width:"100%",height:"200px",display:"flex",alignItems:"center",justifyContent:"center",backgroundColor:"#f5f5f5",border:"2px dashed #ccc",borderRadius:"4px",color:"#666"},children:"Diagram placeholder"})}),(n||r)&&(0,t.jsxs)("div",{style:{marginTop:"0.5rem",fontSize:"0.9rem",color:"#5f6368",textAlign:"left",padding:"0.5rem"},children:[n&&(0,t.jsx)("p",{style:{margin:"0.5rem 0"},children:n}),r&&(0,t.jsxs)("p",{style:{margin:"0.5rem 0",fontStyle:"italic"},children:[(0,t.jsx)("strong",{children:"Figure:"})," ",r]})]})]})},8453(e,n,i){i.d(n,{R:()=>a,x:()=>s});var t=i(6540);const o={},r=t.createContext(o);function a(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(r.Provider,{value:n},e.children)}},8844(e,n,i){i.d(n,{A:()=>o});i(6540);var t=i(4848);const o=({type:e="note",title:n,children:i,className:o=""})=>{const r={note:{borderLeft:"4px solid #4285f4",backgroundColor:"#f0f4ff",color:"#202124"},tip:{borderLeft:"4px solid #34a853",backgroundColor:"#f0f9ff",color:"#202124"},warning:{borderLeft:"4px solid #fbbc04",backgroundColor:"#fef7e0",color:"#202124"},danger:{borderLeft:"4px solid #ea4335",backgroundColor:"#fce8e6",color:"#202124"}},a={note:"\u2139\ufe0f",tip:"\ud83d\udca1",warning:"\u26a0\ufe0f",danger:"\u274c"},s=r[e]||r.note,l=a[e]||a.note;return(0,t.jsx)("div",{className:`callout callout-${e} ${o}`,style:{border:"1px solid",borderRadius:"4px",padding:"1rem",margin:"1rem 0",...s},children:(0,t.jsxs)("div",{style:{display:"flex",alignItems:"flex-start"},children:[(0,t.jsx)("span",{style:{fontSize:"1.2rem",marginRight:"0.5rem"},children:l}),(0,t.jsxs)("div",{children:[n&&(0,t.jsx)("h5",{style:{margin:"0 0 0.5rem 0",fontSize:"1rem",fontWeight:"bold",textTransform:"uppercase",letterSpacing:"0.5px"},children:n}),(0,t.jsx)("div",{children:i})]})]})})}}}]);