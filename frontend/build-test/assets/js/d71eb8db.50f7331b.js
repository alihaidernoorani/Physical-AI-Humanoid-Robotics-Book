"use strict";(globalThis.webpackChunktextbook_physical_ai=globalThis.webpackChunktextbook_physical_ai||[]).push([[786],{1285(e,i,n){n.r(i),n.d(i,{assets:()=>p,contentTitle:()=>m,default:()=>g,frontMatter:()=>d,metadata:()=>t,toc:()=>h});const t=JSON.parse('{"id":"digital-twin/sensor-simulation","title":"Sensor Simulation","description":"Learning Objectives","source":"@site/docs/digital-twin/02-sensor-simulation.mdx","sourceDirName":"digital-twin","slug":"/digital-twin/sensor-simulation","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/digital-twin/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/digital-twin/02-sensor-simulation.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Sensor Simulation","sidebar_label":"Sensor Simulation"},"sidebar":"tutorialSidebar","previous":{"title":"Rigid Body Dynamics","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/digital-twin/rigid-body-dynamics-gazebo"},"next":{"title":"Unity Environments","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/digital-twin/unity-high-fidelity-env"}}');var o=n(4848),a=n(8453),r=n(8844),s=n(7589),l=n(6212),c=n(7639);const d={title:"Sensor Simulation",sidebar_label:"Sensor Simulation"},m=void 0,p={},h=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"LiDAR Simulation and Point Cloud Generation",id:"lidar-simulation-and-point-cloud-generation",level:2},{value:"RGB-D Camera Models and Depth Perception",id:"rgb-d-camera-models-and-depth-perception",level:2},{value:"IMU and Inertial Sensor Simulation",id:"imu-and-inertial-sensor-simulation",level:2},{value:"Sensor Noise Modeling and Calibration",id:"sensor-noise-modeling-and-calibration",level:2},{value:"Practical Applications in Humanoid Robotics",id:"practical-applications-in-humanoid-robotics",level:2},{value:"Ethical &amp; Safety Considerations",id:"ethical--safety-considerations",level:2},{value:"Summary",id:"summary",level:2}];function u(e){const i={h2:"h2",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(i.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(i.p,{children:"By the end of this chapter, you will:"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Implement realistic LiDAR simulation with accurate point cloud generation"}),"\n",(0,o.jsx)(i.li,{children:"Configure RGB-D camera models with realistic depth perception and noise characteristics"}),"\n",(0,o.jsx)(i.li,{children:"Simulate IMU and inertial sensors with appropriate noise models and dynamics"}),"\n",(0,o.jsx)(i.li,{children:"Model sensor noise and calibration parameters for realistic perception systems"}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"lidar-simulation-and-point-cloud-generation",children:"LiDAR Simulation and Point Cloud Generation"}),"\n",(0,o.jsx)(i.p,{children:"LiDAR (Light Detection and Ranging) simulation in Gazebo provides realistic 3D point cloud data that accurately mimics real-world laser range finders, which are essential for humanoid robot navigation, mapping, and obstacle detection. The simulation must accurately model the physical principles of LiDAR operation, including beam propagation, reflection, and measurement uncertainties to ensure realistic sensor behavior."}),"\n",(0,o.jsx)(r.A,{type:"note",title:"LiDAR Importance",children:(0,o.jsx)(i.p,{children:"LiDAR simulation is essential for humanoid robot navigation, mapping, and obstacle detection in complex human environments."})}),"\n",(0,o.jsx)(i.p,{children:"Gazebo's LiDAR sensor implementation models the scanning pattern of real LiDAR devices and generates point clouds with appropriate density and accuracy characteristics. For humanoid robots, LiDAR simulation must account for the robot's height and typical operating scenarios to ensure that the generated point clouds reflect the expected sensor data in real-world environments. The simulation includes parameters for beam divergence, detection range, and angular resolution that precisely match the physical sensor specifications."}),"\n",(0,o.jsx)(i.p,{children:"Point cloud generation in LiDAR simulation involves ray tracing from the sensor origin to detect intersections with objects in the environment. For humanoid robots operating in human environments, the simulation must handle complex indoor scenes including furniture, architectural features, and dynamic obstacles. The point cloud density and quality directly impact the performance of perception algorithms developed in simulation."}),"\n",(0,o.jsx)(i.p,{children:"Range and intensity modeling in LiDAR simulation accounts for the physical properties of laser reflection, including material properties and surface characteristics. For humanoid robots, this modeling affects the robot's ability to distinguish between different surface types and materials, which is important for navigation and safety considerations. The intensity values in simulated point clouds can help identify reflective surfaces, glass, or other materials that might pose navigation challenges."}),"\n",(0,o.jsx)(s.A,{question:"What is the primary purpose of intensity modeling in LiDAR simulation?",options:["To improve rendering quality","To help identify different surface materials and navigation challenges","To increase the number of points in the cloud","To reduce simulation performance"],correctAnswer:1,explanation:"Intensity modeling in LiDAR simulation helps identify different surface materials and navigation challenges such as reflective surfaces, glass, or other materials that might affect robot navigation."}),"\n",(0,o.jsx)(i.p,{children:"LiDAR sensors are configured with parameters that define their scanning pattern and measurement capabilities. For humanoid robots, these parameters must be carefully set to match the expected operating environment and navigation requirements."}),"\n",(0,o.jsx)(l.A,{title:"LiDAR Sensor Configuration",problem:"Configure a Gazebo LiDAR sensor with appropriate parameters for humanoid robot navigation.",hints:["Use the <sensor> tag with type='ray'","Configure the horizontal and vertical scan ranges","Set appropriate resolution and range parameters"],solution:'<sensor name="lidar_sensor" type="ray">\n<pose>0.5 0 0.2 0 0 0</pose>\n<ray>\n  <scan>\n    <horizontal>\n      <samples>1080</samples>\n      <resolution>1</resolution>\n      <min_angle>-1.570796</min_angle>\n      <max_angle>1.570796</max_angle>\n    </horizontal>\n    <vertical>\n      <samples>16</samples>\n      <resolution>1</resolution>\n      <min_angle>-0.261799</min_angle>\n      <max_angle>0.261799</max_angle>\n    </vertical>\n  </scan>\n  <range>\n    <min>0.1</min>\n    <max>30.0</max>\n    <resolution>0.01</resolution>\n  </range>\n</ray>\n<plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n  <ros>\n    <namespace>lidar</namespace>\n    <remapping>~/out:=scan</remapping>\n  </ros>\n  <output_type>sensor_msgs/LaserScan</output_type>\n</plugin>\n</sensor>'}),"\n",(0,o.jsx)(i.h2,{id:"rgb-d-camera-models-and-depth-perception",children:"RGB-D Camera Models and Depth Perception"}),"\n",(0,o.jsx)(i.p,{children:"RGB-D camera simulation in Gazebo combines color (RGB) and depth (D) information to provide comprehensive visual perception capabilities for humanoid robots. The simulation must accurately model both the color imaging and depth sensing components, including their respective noise characteristics and limitations that affect real-world performance."}),"\n",(0,o.jsx)(c.A,{title:"RGB-D Camera Data Fusion",description:"Diagram showing how color and depth information are aligned and fused in RGB-D simulation",width:"700",height:"400"}),"\n",(0,o.jsx)(i.p,{children:"Color camera simulation models the optical properties of real cameras, including focal length, field of view, and lens distortion characteristics. For humanoid robots, RGB camera simulation must provide realistic color reproduction and image quality that matches the expected performance of physical cameras. The simulation includes parameters for exposure time, ISO sensitivity, and various noise sources that affect image quality in real-world conditions."}),"\n",(0,o.jsx)(i.p,{children:"Depth camera simulation models the active or passive depth sensing mechanisms of RGB-D cameras and generates depth maps that correspond to the RGB image data. For humanoid robots, depth simulation must accurately represent distances, surface normals, and object boundaries that are critical for navigation, manipulation, and human-robot interaction. The depth accuracy and range limitations must match the physical sensor specifications to ensure realistic performance."}),"\n",(0,o.jsx)(r.A,{type:"tip",title:"Depth Accuracy",children:(0,o.jsx)(i.p,{children:"For humanoid robots, depth simulation must accurately represent distances, surface normals, and object boundaries that are critical for navigation, manipulation, and human-robot interaction."})}),"\n",(0,o.jsx)(i.p,{children:"Stereo vision and structured light modeling in RGB-D simulation accounts for the specific depth sensing technologies used in different camera models. For humanoid robots, this includes simulation of stereo cameras, time-of-flight sensors, and structured light systems, each with its own accuracy characteristics and limitations. The simulation must handle challenging scenarios such as specular reflections, transparent surfaces, and depth discontinuities that are common in human environments."}),"\n",(0,o.jsx)(i.p,{children:"RGB-D cameras in simulation provide both visual and geometric information that is essential for humanoid robot perception. The combination of color and depth data enables sophisticated perception algorithms for object recognition, scene understanding, and navigation that are crucial for autonomous humanoid robot operation."}),"\n",(0,o.jsx)(s.A,{question:"Which of the following is NOT a depth sensing technology modeled in RGB-D simulation?",options:["Stereo vision","Time-of-flight (ToF)","Structured light","GPS positioning"],correctAnswer:3,explanation:"RGB-D simulation models stereo vision, time-of-flight (ToF), and structured light technologies for depth sensing. GPS positioning is not a depth sensing technology used in RGB-D cameras."}),"\n",(0,o.jsx)(i.h2,{id:"imu-and-inertial-sensor-simulation",children:"IMU and Inertial Sensor Simulation"}),"\n",(0,o.jsx)(i.p,{children:"Inertial Measurement Unit (IMU) simulation in Gazebo provides realistic measurements of linear acceleration and angular velocity that are essential for humanoid robot balance control, motion estimation, and state estimation. The simulation must include appropriate noise models and dynamic response characteristics that accurately match physical IMU sensors to ensure realistic behavior."}),"\n",(0,o.jsx)(c.A,{title:"IMU Sensor Axes and Coordinate System",description:"Diagram showing the IMU sensor axes and coordinate system for humanoid robot balance control",width:"700",height:"400"}),"\n",(0,o.jsx)(i.p,{children:"IMU sensor modeling includes three-axis accelerometers and gyroscopes with realistic noise characteristics, including bias, drift, and random walk components. For humanoid robots, IMU simulation must accurately represent the sensor's response to the robot's dynamic motion, including the high-frequency vibrations and impacts typical of bipedal locomotion. The noise models must reflect the actual performance characteristics of physical IMU sensors used in humanoid robots to ensure accurate simulation results."}),"\n",(0,o.jsx)(i.p,{children:"IMU sensors are critical for humanoid robot stability and motion control, providing essential feedback about the robot's orientation and acceleration that is crucial for maintaining balance during locomotion and manipulation tasks."}),"\n",(0,o.jsx)(l.A,{title:"IMU Sensor Configuration",problem:"Configure a Gazebo IMU sensor with appropriate noise parameters for humanoid balance control.",hints:["Use the <sensor> tag with type='imu'","Configure noise parameters for accelerometer and gyroscope","Set appropriate bias and drift values"],solution:'<sensor name="imu_sensor" type="imu">\n<pose>0 0 0.5 0 0 0</pose>\n<imu>\n  <angular_velocity>\n    <x>\n      <noise type="gaussian">\n        <mean>0.0</mean>\n        <stddev>0.0017</stddev>\n        <bias_mean>0.0</bias_mean>\n        <bias_stddev>0.001</bias_stddev>\n      </noise>\n    </x>\n    <y>\n      <noise type="gaussian">\n        <mean>0.0</mean>\n        <stddev>0.0017</stddev>\n        <bias_mean>0.0</bias_mean>\n        <bias_stddev>0.001</bias_stddev>\n      </noise>\n    </y>\n    <z>\n      <noise type="gaussian">\n        <mean>0.0</mean>\n        <stddev>0.0017</stddev>\n        <bias_mean>0.0</bias_mean>\n        <bias_stddev>0.001</bias_stddev>\n      </noise>\n    </z>\n  </angular_velocity>\n  <linear_acceleration>\n    <x>\n      <noise type="gaussian">\n        <mean>0.0</mean>\n        <stddev>0.017</stddev>\n        <bias_mean>0.0</bias_mean>\n        <bias_stddev>0.001</bias_stddev>\n      </noise>\n    </x>\n    <y>\n      <noise type="gaussian">\n        <mean>0.0</mean>\n        <stddev>0.017</stddev>\n        <bias_mean>0.0</bias_mean>\n        <bias_stddev>0.001</bias_stddev>\n      </noise>\n    </y>\n    <z>\n      <noise type="gaussian">\n        <mean>0.0</mean>\n        <stddev>0.017</stddev>\n        <bias_mean>0.0</bias_mean>\n        <bias_stddev>0.001</bias_stddev>\n      </noise>\n    </z>\n  </linear_acceleration>\n</imu>\n<plugin name="imu_plugin" filename="libgazebo_ros_imu_sensor.so">\n  <ros>\n    <namespace>imu</namespace>\n    <remapping>~/out:=imu_data</remapping>\n  </ros>\n  <initial_orientation_as_reference>false</initial_orientation_as_reference>\n</plugin>\n</sensor>'}),"\n",(0,o.jsx)(i.p,{children:"Bias and drift modeling in IMU simulation accounts for the time-varying characteristics of real inertial sensors, including temperature effects and long-term stability. For humanoid robots, these effects can significantly impact balance control and motion estimation algorithms, making accurate modeling essential for developing robust control systems. The simulation includes parameters for initial bias, bias drift, and noise density that match physical sensor specifications."}),"\n",(0,o.jsx)(i.p,{children:"Integration with robot dynamics ensures that IMU measurements accurately reflect the robot's motion as computed by the physics engine. For humanoid robots, this integration must handle the complex multi-body dynamics and contact forces that affect the robot's motion. The IMU simulation must provide measurements that are consistent with the robot's actual acceleration and rotation as determined by the physics simulation."}),"\n",(0,o.jsx)(r.A,{type:"danger",title:"IMU Criticality",children:(0,o.jsx)(i.p,{children:"For humanoid robots, accurate IMU simulation is critical for balance control and motion estimation algorithms, as errors can lead to instability and falls."})}),"\n",(0,o.jsx)(i.h2,{id:"sensor-noise-modeling-and-calibration",children:"Sensor Noise Modeling and Calibration"}),"\n",(0,o.jsx)(i.p,{children:"Sensor noise modeling in Gazebo provides realistic imperfections that reflect the limitations of physical sensors and enable the development of robust perception and control algorithms. For humanoid robots, accurate noise modeling is essential to create algorithms that can handle the uncertainties and errors inherent in real-world sensor data."}),"\n",(0,o.jsx)(c.A,{title:"Sensor Noise Model Parameters",description:"Diagram showing different types of sensor noise and their effects on measurements",width:"700",height:"400"}),"\n",(0,o.jsx)(i.p,{children:"Gaussian noise modeling represents the random measurement errors typical of most sensors using parameters that match the physical sensor characteristics. For humanoid robots, Gaussian noise models must be carefully calibrated to reflect the actual sensor performance, including factors such as signal-to-noise ratio, quantization effects, and thermal noise. The noise parameters directly impact the performance of perception and control algorithms developed in simulation."}),"\n",(0,o.jsx)(i.p,{children:"Sensor noise modeling is crucial for developing algorithms that are robust to real-world conditions. Without proper noise simulation, algorithms may work well in simulation but fail when deployed on physical robots with noisy sensor data."}),"\n",(0,o.jsx)(s.A,{question:"What is the primary purpose of Gaussian noise modeling in sensor simulation?",options:["To improve sensor performance","To represent random measurement errors typical of physical sensors","To eliminate sensor errors","To increase sensor accuracy"],correctAnswer:1,explanation:"Gaussian noise modeling represents the random measurement errors typical of most physical sensors, using parameters that match the actual sensor characteristics."}),"\n",(0,o.jsx)(i.p,{children:"Calibration parameter simulation includes both intrinsic and extrinsic calibration parameters that affect sensor measurements. For humanoid robots, this includes camera intrinsic parameters (focal length, principal point, distortion coefficients), extrinsic parameters (position and orientation relative to robot base), and sensor-specific calibration factors. The calibration simulation enables the development of calibration procedures and validates sensor alignment in the robot system."}),"\n",(0,o.jsx)(l.A,{title:"Camera Calibration Parameters",problem:"Define camera intrinsic and extrinsic calibration parameters for a humanoid robot's head-mounted camera.",hints:["Include focal length and principal point in intrinsic parameters","Specify distortion coefficients","Define position and orientation in extrinsic parameters"],solution:'\x3c!-- Intrinsic parameters --\x3e\n<camera>\n<horizontal_fov>1.089</horizontal_fov>\n<image>\n  <width>640</width>\n  <height>480</height>\n  <format>R8G8B8</format>\n</image>\n<clip>\n  <near>0.1</near>\n  <far>100</far>\n</clip>\n<distortion>\n  <k1>0.0</k1>\n  <k2>0.0</k2>\n  <k3>0.0</k3>\n  <p1>0.0</p1>\n  <p2>0.0</p2>\n  <center>0.5 0.5</center>\n</distortion>\n</camera>\n\n\x3c!-- Extrinsic parameters in the joint connecting camera to robot --\x3e\n<joint name="head_camera_joint" type="fixed">\n<parent link="head_link"/>\n<child link="camera_link"/>\n<origin xyz="0.05 0 0.1" rpy="0 0 0"/>\n</joint>'}),"\n",(0,o.jsx)(i.p,{children:"Dynamic noise modeling accounts for sensor performance variations that occur under different operating conditions such as temperature, vibration, and electromagnetic interference. For humanoid robots, these effects can vary significantly during operation, particularly during locomotion or when interacting with the environment. The dynamic noise models must reflect how sensor performance changes under realistic operating conditions."}),"\n",(0,o.jsx)(i.h2,{id:"practical-applications-in-humanoid-robotics",children:"Practical Applications in Humanoid Robotics"}),"\n",(0,o.jsx)(i.p,{children:"In humanoid robotics, sensor simulation is essential for developing and testing perception systems before deploying them on real robots. The simulated sensors must provide realistic data that allows for the development of robust algorithms that can handle the noise and limitations of real-world sensors, ensuring successful transfer from simulation to reality."}),"\n",(0,o.jsx)(i.p,{children:"When setting up a sensor suite for a humanoid robot in simulation, several critical considerations ensure effective and realistic results:"}),"\n",(0,o.jsxs)(i.ol,{children:["\n",(0,o.jsx)(i.li,{children:"Selecting the appropriate sensor types for your robot's intended tasks and application requirements"}),"\n",(0,o.jsx)(i.li,{children:"Implementing realistic noise models that precisely match the physical sensors used in the actual robot"}),"\n",(0,o.jsx)(i.li,{children:"Configuring proper calibration parameters that accurately reflect the physical robot's specifications"}),"\n",(0,o.jsx)(i.li,{children:"Creating seamless integration between different sensors to enable effective sensor fusion"}),"\n"]}),"\n",(0,o.jsx)(i.p,{children:"These elements work together synergistically to create a high-fidelity simulation environment where you can develop and test perception algorithms that will transfer effectively to the physical robot with minimal adaptation required."}),"\n",(0,o.jsx)(i.h2,{id:"ethical--safety-considerations",children:"Ethical & Safety Considerations"}),"\n",(0,o.jsx)(i.p,{children:"The accuracy of sensor simulation in humanoid robotics has important ethical and safety implications that directly affect robot deployment in human environments. These considerations are fundamental to responsible robotics development and deployment."}),"\n",(0,o.jsx)(i.p,{children:"Inaccurate sensor simulation can lead to perception algorithms that perform well in simulation but fail to detect critical obstacles or hazards in the real world. This poses significant risks to human safety and property, making accurate simulation a critical ethical responsibility for humanoid robot developers."}),"\n",(0,o.jsx)(i.p,{children:"Proper modeling of sensor limitations and noise characteristics is essential to ensure that safety-critical perception systems are robust to real-world sensor imperfections. This comprehensive approach to simulation accuracy helps prevent potentially dangerous situations during real-world robot operation."}),"\n",(0,o.jsx)(i.p,{children:"Additionally, the realistic simulation of sensor performance enables comprehensive safety validation before physical deployment, which significantly reduces risks to humans and property. This proactive approach to safety testing is an ethical imperative in humanoid robotics development."}),"\n",(0,o.jsx)(r.A,{type:"danger",title:"Safety Critical",children:(0,o.jsx)(i.p,{children:"Inaccurate sensor simulation can lead to perception algorithms that fail to detect critical obstacles or hazards in the real world, potentially causing unsafe robot behavior."})}),"\n",(0,o.jsx)(i.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(i.p,{children:"In this chapter, we've explored the fundamental concepts of sensor simulation in Gazebo and their critical applications in humanoid robotics:"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"LiDAR simulation"})," provides realistic 3D point cloud data for navigation, mapping, and obstacle detection with accurate modeling of physical principles and noise characteristics"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"RGB-D camera simulation"})," combines color and depth information with realistic noise characteristics, enabling comprehensive visual perception for humanoid robots"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"IMU simulation"})," includes appropriate noise models and dynamic response for balance control, providing essential feedback for maintaining robot stability"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Sensor noise modeling"})," enables development of robust perception and control algorithms that can handle real-world sensor imperfections and uncertainties"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Calibration parameter simulation"})," ensures accurate sensor integration in robot systems by modeling both intrinsic and extrinsic parameters that affect sensor measurements"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Realistic sensor simulation"})," is critical for safe transfer of algorithms from simulation to reality, ensuring that perception and control systems perform reliably in real-world conditions"]}),"\n"]}),"\n",(0,o.jsx)(i.p,{children:"The sensor simulation concepts covered in this chapter are essential for developing the perception systems of your Autonomous Humanoid capstone project. LiDAR simulation will enable development of navigation and mapping algorithms that can operate effectively in complex environments. RGB-D camera simulation will support object recognition and manipulation planning with both visual and geometric information. IMU simulation will be crucial for balance control and state estimation, providing essential feedback for maintaining robot stability. Noise modeling will ensure that your perception algorithms are robust to real-world sensor imperfections, enabling reliable operation in challenging conditions. Understanding these principles is fundamental to creating humanoid robots that can safely and effectively perceive and interact with their environment."})]})}function g(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,o.jsx)(i,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}},6212(e,i,n){n.d(i,{A:()=>a});var t=n(6540),o=n(4848);const a=({title:e,problem:i,solution:n,hints:a=[],initialCode:r="",className:s=""})=>{const[l,c]=(0,t.useState)(r),[d,m]=(0,t.useState)(!1),[p,h]=(0,t.useState)(!1),[u,g]=(0,t.useState)(!1),[f,b]=(0,t.useState)(null);return(0,o.jsxs)("div",{className:`exercise-component ${s}`,style:{border:"1px solid #ddd",borderRadius:"8px",padding:"1rem",margin:"1rem 0",backgroundColor:"#fff"},children:[(0,o.jsx)("h4",{style:{margin:"0 0 1rem 0"},children:e}),(0,o.jsxs)("div",{style:{marginBottom:"1rem"},children:[(0,o.jsx)("h5",{style:{margin:"0.5rem 0",color:"#202124"},children:"Problem:"}),(0,o.jsx)("div",{style:{padding:"0.5rem",backgroundColor:"#f9f9f9",borderRadius:"4px"},children:i})]}),a.length>0&&(0,o.jsxs)("div",{style:{marginBottom:"1rem"},children:[(0,o.jsx)("button",{onClick:()=>h(!p),style:{padding:"0.5rem 1rem",backgroundColor:"#fbbc04",color:"white",border:"none",borderRadius:"4px",cursor:"pointer",marginBottom:"0.5rem"},children:p?"Hide Hint":"Show Hint"}),p&&(0,o.jsxs)("div",{style:{padding:"0.5rem",backgroundColor:"#fef7e0",borderRadius:"4px",border:"1px solid #fbbc04"},children:[(0,o.jsx)("strong",{children:"Hint:"})," ",a[0]]})]}),(0,o.jsxs)("div",{style:{marginBottom:"1rem"},children:[(0,o.jsx)("h5",{style:{margin:"0.5rem 0",color:"#202124"},children:"Your Solution:"}),(0,o.jsx)("textarea",{value:l,onChange:e=>c(e.target.value),style:{width:"100%",minHeight:"150px",padding:"0.5rem",fontFamily:"monospace",border:"1px solid #ddd",borderRadius:"4px",fontSize:"0.9rem"},placeholder:"Write your solution here..."})]}),(0,o.jsxs)("div",{style:{display:"flex",gap:"0.5rem",marginBottom:"1rem"},children:[(0,o.jsx)("button",{onClick:()=>{b({success:!0,message:"Code executed successfully! Check your logic against the solution."}),g(!0)},style:{padding:"0.5rem 1rem",backgroundColor:"#4caf50",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Run Code"}),(0,o.jsx)("button",{onClick:()=>m(!d),style:{padding:"0.5rem 1rem",backgroundColor:"#2196f3",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:d?"Hide Solution":"Show Solution"}),(0,o.jsx)("button",{onClick:()=>{c(r),m(!1),h(!1),g(!1),b(null)},style:{padding:"0.5rem 1rem",backgroundColor:"#9e9e9e",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Reset"})]}),u&&f&&(0,o.jsxs)("div",{style:{padding:"0.75rem",borderRadius:"4px",backgroundColor:f.success?"#e8f5e9":"#ffebee",border:"1px solid "+(f.success?"#4caf50":"#f44336"),marginBottom:"1rem"},children:[(0,o.jsx)("strong",{children:"Status:"})," ",f.message]}),d&&(0,o.jsxs)("div",{style:{padding:"0.5rem",backgroundColor:"#e8f5e9",borderRadius:"4px",border:"1px solid #4caf50"},children:[(0,o.jsx)("h5",{style:{margin:"0.5rem 0",color:"#202124"},children:"Solution:"}),(0,o.jsx)("pre",{style:{padding:"0.5rem",backgroundColor:"#f1f8e9",borderRadius:"4px",overflowX:"auto",whiteSpace:"pre-wrap"},children:n})]})]})}},7589(e,i,n){n.d(i,{A:()=>a});var t=n(6540),o=n(4848);const a=({question:e,options:i,correctAnswer:n,explanation:a,className:r=""})=>{const s=Array.isArray(i)?i:i&&"string"==typeof i?i.split("||"):[],[l,c]=(0,t.useState)(null),[d,m]=(0,t.useState)(!1),[p,h]=(0,t.useState)(!1),u=e=>{if(d)return;c(e);h(e===n),m(!0)},g=e=>d?e===n?{padding:"0.75rem",margin:"0.5rem 0",border:"1px solid #4caf50",borderRadius:"4px",backgroundColor:"#e8f5e9",fontWeight:"bold"}:e===l&&e!==n?{padding:"0.75rem",margin:"0.5rem 0",border:"1px solid #f44336",borderRadius:"4px",backgroundColor:"#ffebee"}:{padding:"0.75rem",margin:"0.5rem 0",border:"1px solid #ddd",borderRadius:"4px",backgroundColor:"#f5f5f5"}:{padding:"0.75rem",margin:"0.5rem 0",cursor:"pointer",border:"1px solid #ddd",borderRadius:"4px",backgroundColor:l===e?"#e3f2fd":"#fff"};return(0,o.jsxs)("div",{className:`quiz-component ${r}`,style:{border:"1px solid #ddd",borderRadius:"8px",padding:"1rem",margin:"1rem 0",backgroundColor:"#fff"},children:[(0,o.jsx)("h4",{style:{margin:"0 0 1rem 0"},children:e}),(0,o.jsx)("div",{children:s.map((i,n)=>(0,o.jsxs)("div",{style:g(i),onClick:()=>u(i),children:[(0,o.jsx)("input",{type:"radio",name:`quiz-${e}`,value:i,checked:l===i,onChange:()=>{},disabled:d,style:{marginRight:"0.5rem"}}),i]},n))}),d&&(0,o.jsxs)("div",{style:{marginTop:"1rem",padding:"0.75rem",borderRadius:"4px",backgroundColor:p?"#e8f5e9":"#ffebee",border:"1px solid "+(p?"#4caf50":"#f44336")},children:[(0,o.jsx)("p",{style:{margin:"0.5rem 0",fontWeight:"bold"},children:p?"\u2705 Correct!":"\u274c Incorrect"}),a&&(0,o.jsxs)("p",{style:{margin:"0.5rem 0"},children:[(0,o.jsx)("strong",{children:"Explanation:"})," ",a]})]}),d?(0,o.jsx)("button",{onClick:()=>{c(null),m(!1),h(!1)},style:{marginTop:"1rem",padding:"0.5rem 1rem",backgroundColor:"#2196f3",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Try Again"}):l&&(0,o.jsx)("button",{onClick:()=>u(l),style:{marginTop:"1rem",padding:"0.5rem 1rem",backgroundColor:"#4caf50",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Submit Answer"})]})}},7639(e,i,n){n.d(i,{A:()=>o});n(6540);var t=n(4848);const o=({title:e,description:i,src:n,alt:o,caption:a,className:r=""})=>(0,t.jsxs)("div",{className:`diagram-component ${r}`,style:{textAlign:"center",margin:"1.5rem 0",padding:"1rem",border:"1px solid #eee",borderRadius:"8px",backgroundColor:"#fafafa"},children:[e&&(0,t.jsx)("h5",{style:{margin:"0 0 1rem 0",color:"#202124",fontSize:"1rem",fontWeight:"bold"},children:e}),(0,t.jsx)("div",{style:{display:"flex",justifyContent:"center",alignItems:"center",margin:"0 auto",maxWidth:"100%"},children:n?(0,t.jsx)("img",{src:n,alt:o||e||"Diagram",style:{maxWidth:"100%",height:"auto",border:"1px solid #ddd",borderRadius:"4px"}}):(0,t.jsx)("div",{style:{width:"100%",height:"200px",display:"flex",alignItems:"center",justifyContent:"center",backgroundColor:"#f5f5f5",border:"2px dashed #ccc",borderRadius:"4px",color:"#666"},children:"Diagram placeholder"})}),(i||a)&&(0,t.jsxs)("div",{style:{marginTop:"0.5rem",fontSize:"0.9rem",color:"#5f6368",textAlign:"left",padding:"0.5rem"},children:[i&&(0,t.jsx)("p",{style:{margin:"0.5rem 0"},children:i}),a&&(0,t.jsxs)("p",{style:{margin:"0.5rem 0",fontStyle:"italic"},children:[(0,t.jsx)("strong",{children:"Figure:"})," ",a]})]})]})},8453(e,i,n){n.d(i,{R:()=>r,x:()=>s});var t=n(6540);const o={},a=t.createContext(o);function r(e){const i=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function s(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(a.Provider,{value:i},e.children)}},8844(e,i,n){n.d(i,{A:()=>o});n(6540);var t=n(4848);const o=({type:e="note",title:i,children:n,className:o=""})=>{const a={note:{borderLeft:"4px solid #4285f4",backgroundColor:"#f0f4ff",color:"#202124"},tip:{borderLeft:"4px solid #34a853",backgroundColor:"#f0f9ff",color:"#202124"},warning:{borderLeft:"4px solid #fbbc04",backgroundColor:"#fef7e0",color:"#202124"},danger:{borderLeft:"4px solid #ea4335",backgroundColor:"#fce8e6",color:"#202124"}},r={note:"\u2139\ufe0f",tip:"\ud83d\udca1",warning:"\u26a0\ufe0f",danger:"\u274c"},s=a[e]||a.note,l=r[e]||r.note;return(0,t.jsx)("div",{className:`callout callout-${e} ${o}`,style:{border:"1px solid",borderRadius:"4px",padding:"1rem",margin:"1rem 0",...s},children:(0,t.jsxs)("div",{style:{display:"flex",alignItems:"flex-start"},children:[(0,t.jsx)("span",{style:{fontSize:"1.2rem",marginRight:"0.5rem"},children:l}),(0,t.jsxs)("div",{children:[i&&(0,t.jsx)("h5",{style:{margin:"0 0 0.5rem 0",fontSize:"1rem",fontWeight:"bold",textTransform:"uppercase",letterSpacing:"0.5px"},children:i}),(0,t.jsx)("div",{children:n})]})]})})}}}]);