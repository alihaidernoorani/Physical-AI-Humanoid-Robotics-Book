"use strict";(globalThis.webpackChunktextbook_physical_ai=globalThis.webpackChunktextbook_physical_ai||[]).push([[286],{1302(e,n,t){t(6540),t(4848)},2078(e,n,t){t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>d,default:()=>g,frontMatter:()=>m,metadata:()=>i,toc:()=>u});const i=JSON.parse('{"id":"vla/capstone-end-to-end","title":"Capstone: End-to-End Autonomous Humanoid","description":"Integration of all modules: voice command to action execution in simulation with comprehensive system validation","source":"@site/docs/vla/04-capstone-end-to-end.mdx","sourceDirName":"vla","slug":"/vla/capstone-end-to-end","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/vla/capstone-end-to-end","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla/04-capstone-end-to-end.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"capstone-end-to-end","title":"Capstone: End-to-End Autonomous Humanoid","description":"Integration of all modules: voice command to action execution in simulation with comprehensive system validation","personalization":true,"translation":"ur","learning_outcomes":["Integrate all modules (ROS 2, Digital Twin, AI-Brain, VLA) into a complete autonomous humanoid system","Implement end-to-end pipeline from voice command to action execution with full system validation","Design comprehensive testing and validation strategies for autonomous humanoid systems","Optimize system performance and debug complex multi-module interactions"],"software_stack":["ROS 2 Humble Hawksbill (LTS)","NVIDIA Isaac Sim 2024.2+","NVIDIA Isaac ROS GEMs","Navigation2 (Nav2) stack","OpenAI Whisper or NVIDIA Riva ASR","Large Language Models (LLM) integration","Python 3.10+ with rclpy","CUDA 12.0+ with cuDNN"],"hardware_recommendations":["NVIDIA Jetson AGX Orin for integrated processing","NVIDIA RTX 4090 for development and training","High-quality microphone array for voice input","RGB-D camera for perception","IMU for balance feedback"],"hardware_alternatives":["Laptop with discrete GPU for development","NVIDIA Jetson Orin Nano for minimal configurations","Simulated environment for development and testing"],"prerequisites":["Module 1-4: Complete understanding of all previous modules and their integration","Experience with system integration and debugging complex multi-module systems","Knowledge of comprehensive testing and validation methodologies","Understanding of performance optimization techniques for multi-component systems"],"assessment_recommendations":["Integration test: Complete end-to-end voice command to action execution","System validation: Comprehensive testing of all integrated components","Performance benchmark: End-to-end system performance evaluation"],"dependencies":["04-vla/intro","04-vla/01-voice-to-text-whisper","04-vla/02-llm-task-decomposition","04-vla/03-grounding-language-ros2"]},"sidebar":"tutorialSidebar","previous":{"title":"Language-Action Grounding","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/vla/grounding-language-ros2"}}');var o=t(4848),s=t(8453),a=t(8844),r=t(7589),l=t(6212),c=t(7639);t(1302);const m={id:"capstone-end-to-end",title:"Capstone: End-to-End Autonomous Humanoid",description:"Integration of all modules: voice command to action execution in simulation with comprehensive system validation",personalization:!0,translation:"ur",learning_outcomes:["Integrate all modules (ROS 2, Digital Twin, AI-Brain, VLA) into a complete autonomous humanoid system","Implement end-to-end pipeline from voice command to action execution with full system validation","Design comprehensive testing and validation strategies for autonomous humanoid systems","Optimize system performance and debug complex multi-module interactions"],software_stack:["ROS 2 Humble Hawksbill (LTS)","NVIDIA Isaac Sim 2024.2+","NVIDIA Isaac ROS GEMs","Navigation2 (Nav2) stack","OpenAI Whisper or NVIDIA Riva ASR","Large Language Models (LLM) integration","Python 3.10+ with rclpy","CUDA 12.0+ with cuDNN"],hardware_recommendations:["NVIDIA Jetson AGX Orin for integrated processing","NVIDIA RTX 4090 for development and training","High-quality microphone array for voice input","RGB-D camera for perception","IMU for balance feedback"],hardware_alternatives:["Laptop with discrete GPU for development","NVIDIA Jetson Orin Nano for minimal configurations","Simulated environment for development and testing"],prerequisites:["Module 1-4: Complete understanding of all previous modules and their integration","Experience with system integration and debugging complex multi-module systems","Knowledge of comprehensive testing and validation methodologies","Understanding of performance optimization techniques for multi-component systems"],assessment_recommendations:["Integration test: Complete end-to-end voice command to action execution","System validation: Comprehensive testing of all integrated components","Performance benchmark: End-to-end system performance evaluation"],dependencies:["04-vla/intro","04-vla/01-voice-to-text-whisper","04-vla/02-llm-task-decomposition","04-vla/03-grounding-language-ros2"]},d="Capstone: End-to-End Autonomous Humanoid",p={},u=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Integration of All Previous Modules",id:"integration-of-all-previous-modules",level:2},{value:"Concrete Examples",id:"concrete-examples",level:3},{value:"End-to-End Pipeline Implementation",id:"end-to-end-pipeline-implementation",level:2},{value:"Concrete Examples",id:"concrete-examples-1",level:3},{value:"System Validation and Testing",id:"system-validation-and-testing",level:2},{value:"Concrete Examples",id:"concrete-examples-2",level:3},{value:"Performance Optimization and Debugging",id:"performance-optimization-and-debugging",level:2},{value:"Concrete Examples",id:"concrete-examples-3",level:3},{value:"Forward References to Capstone Project",id:"forward-references-to-capstone-project",level:2},{value:"Ethical &amp; Safety Considerations",id:"ethical--safety-considerations",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function f(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"capstone-end-to-end-autonomous-humanoid",children:"Capstone: End-to-End Autonomous Humanoid"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Integrate all modules (ROS 2, Digital Twin, AI-Brain, VLA) into a complete autonomous humanoid system"}),"\n",(0,o.jsx)(n.li,{children:"Implement end-to-end pipeline from voice command to action execution with full system validation"}),"\n",(0,o.jsx)(n.li,{children:"Design comprehensive testing and validation strategies for autonomous humanoid systems"}),"\n",(0,o.jsx)(n.li,{children:"Optimize system performance and debug complex multi-module interactions"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"integration-of-all-previous-modules",children:"Integration of All Previous Modules"}),"\n",(0,o.jsx)(n.p,{children:"The integration of all previous modules into a complete autonomous humanoid system represents the culmination of the knowledge and skills developed throughout the course. This integration brings together the ROS 2 nervous system, digital twin simulation environment, AI-powered robot brain, and vision-language-action capabilities to create a unified autonomous system."}),"\n",(0,o.jsx)(a.A,{type:"note",title:"System Integration",children:(0,o.jsx)(n.p,{children:"Complete system integration combines all course modules into a unified autonomous humanoid system, bringing together ROS 2, digital twin simulation, AI-powered robot brain, and vision-language-action capabilities."})}),"\n",(0,o.jsx)(n.p,{children:"System architecture design for the integrated humanoid robot must account for the complex interactions between all components while maintaining modularity and maintainability. The architecture should provide clear interfaces between subsystems, enable independent development and testing of components, and support the real-time performance requirements of autonomous operation. For humanoid robots, the architecture must also handle the safety-critical nature of human-robot interaction."}),"\n",(0,o.jsx)(c.A,{title:"Integrated System Architecture",description:"Diagram showing the integrated system architecture connecting all modules: ROS 2, Digital Twin, AI-Brain, and VLA",caption:"Integrated system architecture connecting all modules: ROS 2, Digital Twin, AI-Brain, and VLA"}),"\n",(0,o.jsx)(n.p,{children:"Data flow integration connects the various processing pipelines including voice-to-text, language understanding, task decomposition, navigation planning, perception processing, and action execution. The integration must handle the timing constraints of real-time operation while providing appropriate buffering and synchronization between components. For humanoid robots, the data flow must support both high-bandwidth sensor data and high-frequency control commands."}),"\n",(0,o.jsx)(l.A,{title:"System Integration Architecture",problem:"Design an integrated system architecture that connects all modules: ROS 2, Digital Twin, AI-Brain, and VLA for an autonomous humanoid robot.",hints:["Define clear interfaces between subsystems","Consider real-time performance requirements","Plan for safety-critical functions","Design modular components for maintainability"],solution:"# Example integrated system architecture\nclass IntegratedHumanoidSystem:\n  def __init__(self):\n      # Initialize all subsystems\n      self.ros2_nervous_system = self.initialize_ros2_system()\n      self.digital_twin = self.initialize_digital_twin()\n      self.ai_robot_brain = self.initialize_ai_brain()\n      self.vla_system = self.initialize_vla_system()\n\n      # Initialize communication layer\n      self.communication_layer = self.initialize_communication()\n\n      # Initialize safety and monitoring systems\n      self.safety_system = self.initialize_safety_system()\n      self.monitoring_system = self.initialize_monitoring_system()\n\n  def initialize_ros2_system(self):\n      # Initialize ROS 2 nodes and communication infrastructure\n      import rclpy\n      from rclpy.node import Node\n\n      class HumanoidROS2Node(Node):\n          def __init__(self):\n              super().__init__('humanoid_integrated_node')\n\n              # Create publishers and subscribers for all subsystems\n              self.voice_cmd_pub = self.create_publisher(String, 'voice_commands', 10)\n              self.action_cmd_pub = self.create_publisher(String, 'action_commands', 10)\n              self.sensor_data_sub = self.create_subscription(\n                  String, 'sensor_data', self.sensor_callback, 10\n              )\n\n          def sensor_callback(self, msg):\n              # Handle sensor data from perception system\n              pass\n\n      return HumanoidROS2Node()\n\n  def initialize_digital_twin(self):\n      # Initialize digital twin simulation environment\n      # This would connect to Isaac Sim or Gazebo simulation\n      return {\n          'simulation_engine': 'isaac_sim',  # or 'gazebo'\n          'synchronization_layer': 'real_time_sync',\n          'validation_tools': 'comprehensive_test_suite'\n      }\n\n  def initialize_ai_brain(self):\n      # Initialize AI-powered perception, navigation, and control systems\n      return {\n          'perception': self.initialize_perception_system(),\n          'navigation': self.initialize_navigation_system(),\n          'control': self.initialize_control_system()\n      }\n\n  def initialize_perception_system(self):\n      # Initialize Isaac ROS GEMs for perception\n      return {\n          'visual_slam': 'enabled',\n          'object_detection': 'enabled',\n          'depth_estimation': 'enabled'\n      }\n\n  def initialize_navigation_system(self):\n      # Initialize Nav2 for navigation\n      return {\n          'path_planner': 'nav2',\n          'recovery_behaviors': 'custom_humanoid',\n          'localization': 'amcl'\n      }\n\n  def initialize_control_system(self):\n      # Initialize humanoid-specific control systems\n      return {\n          'balance_control': 'enabled',\n          'motion_planning': 'humanoid_specific',\n          'actuator_control': 'real_time'\n      }\n\n  def initialize_vla_system(self):\n      # Initialize Vision-Language-Action system\n      return {\n          'voice_to_text': 'whisper_integration',\n          'llm_task_decomposition': 'transformer_based',\n          'language_action_grounding': 'ros2_action_servers'\n      }\n\n  def initialize_communication(self):\n      # Initialize inter-subsystem communication\n      return {\n          'middleware': 'ros2',\n          'data_sync': 'real_time',\n          'error_handling': 'comprehensive'\n      }\n\n  def initialize_safety_system(self):\n      # Initialize safety and emergency systems\n      return {\n          'emergency_stop': 'enabled',\n          'collision_avoidance': 'active',\n          'human_safety': 'priority_one'\n      }\n\n  def initialize_monitoring_system(self):\n      # Initialize system monitoring and logging\n      return {\n          'performance_monitoring': 'enabled',\n          'logging': 'comprehensive',\n          'debugging_tools': 'integrated'\n      }\n\n  def run_system(self):\n      # Main system execution loop\n      try:\n          while True:\n              # Process voice commands through VLA system\n              voice_commands = self.vla_system.process_voice_input()\n\n              # Decompose tasks using AI brain\n              tasks = self.ai_robot_brain.llm_task_decomposition(voice_commands)\n\n              # Plan actions using navigation and control systems\n              actions = self.ai_robot_brain.navigation.plan_actions(tasks)\n\n              # Execute actions through ROS 2 nervous system\n              self.ros2_nervous_system.execute_actions(actions)\n\n              # Monitor and validate execution\n              self.monitoring_system.validate_execution(actions)\n\n              # Update digital twin with real-time data\n              self.digital_twin.sync_with_real_robot()\n\n              # Check safety conditions\n              if not self.safety_system.check_safe_conditions():\n                  self.safety_system.emergency_stop()\n                  break\n\n      except KeyboardInterrupt:\n          print(\"System shutdown initiated\")\n      finally:\n          self.shutdown_system()\n\n  def shutdown_system(self):\n      # Graceful shutdown of all subsystems\n      self.safety_system.emergency_stop()\n      # Additional cleanup code here"}),"\n",(0,o.jsx)(n.p,{children:"Resource management integration coordinates the computational resources across all subsystems to ensure that critical components receive appropriate priority and resources. For humanoid robots operating on resource-constrained platforms, this includes managing GPU memory for AI inference, CPU cycles for control systems, and communication bandwidth for sensor and actuator data. The resource management must adapt to changing operational conditions and prioritize safety-critical functions."}),"\n",(0,o.jsx)(r.A,{question:"What is a key consideration for system architecture design in integrated humanoid robots?",options:"Reducing the number of components||Accounting for complex interactions while maintaining modularity and supporting real-time performance||Focusing only on computational efficiency||Minimizing memory usage",correctAnswer:"Accounting for complex interactions while maintaining modularity and supporting real-time performance",explanation:"System architecture design must account for complex interactions between components while maintaining modularity and supporting real-time performance requirements of autonomous operation, especially for safety-critical human-robot interaction."}),"\n",(0,o.jsx)(n.h3,{id:"concrete-examples",children:"Concrete Examples"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'Example: Complete "Bring me the red cup" command processed through all modules to robot action execution'}),"\n",(0,o.jsx)(n.li,{children:"Example: Resource management prioritizing safety-critical functions during simultaneous AI inference tasks"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"end-to-end-pipeline-implementation",children:"End-to-End Pipeline Implementation"}),"\n",(0,o.jsx)(n.p,{children:"The end-to-end pipeline implementation creates a complete system that processes voice commands through the entire chain of processing to generate appropriate robot actions. This pipeline integrates voice recognition, language understanding, task decomposition, action planning, and execution in a seamless flow that enables natural human-robot interaction."}),"\n",(0,o.jsx)(a.A,{type:"tip",title:"End-to-End Pipeline",children:(0,o.jsx)(n.p,{children:"The end-to-end pipeline processes voice commands through the entire chain of processing to generate robot actions, integrating voice recognition, language understanding, task decomposition, action planning, and execution in a seamless flow."})}),"\n",(0,o.jsx)(n.p,{children:"Voice command processing begins with speech recognition and flows through language understanding to task decomposition, then to action planning and execution. Each stage must maintain appropriate timing and data integrity while handling the uncertainties and variations inherent in natural language and real-world operation. The pipeline must also provide appropriate feedback and error handling throughout the process."}),"\n",(0,o.jsx)(c.A,{title:"End-to-End Pipeline Flow",description:"Diagram showing the end-to-end pipeline flow from voice command to action execution",caption:"End-to-end pipeline flow from voice command to action execution"}),"\n",(0,o.jsx)(n.p,{children:"State management across the pipeline maintains context and coherence throughout complex multi-step interactions. For humanoid robots, this includes tracking the robot's current state, the progress of ongoing tasks, and the context of the human-robot interaction. The state management system must handle interruptions, task switching, and context recovery to maintain natural interaction."}),"\n",(0,o.jsx)(l.A,{title:"End-to-End Pipeline Implementation",problem:"Implement an end-to-end pipeline that processes voice commands through all stages to robot action execution.",hints:["Design the pipeline flow from voice input to action output","Implement state management for multi-step interactions","Include error handling and feedback mechanisms","Ensure real-time performance requirements are met"],solution:"# Example end-to-end pipeline implementation\nimport asyncio\nimport threading\nfrom dataclasses import dataclass\nfrom typing import Optional, Dict, Any\nimport time\n\n@dataclass\nclass VoiceCommand:\n  text: str\n  timestamp: float\n  confidence: float\n\n@dataclass\nclass ParsedCommand:\n  action: str\n  objects: list\n  locations: list\n  confidence: float\n\n@dataclass\nclass TaskPlan:\n  steps: list\n  priority: str\n  safety_critical: bool\n\nclass EndToEndPipeline:\n  def __init__(self):\n      self.voice_recognizer = self.initialize_voice_recognizer()\n      self.language_understanding = self.initialize_language_understanding()\n      self.task_decomposer = self.initialize_task_decomposer()\n      self.action_planner = self.initialize_action_planner()\n      self.action_executor = self.initialize_action_executor()\n\n      # State management\n      self.current_state = {\n          'robot_position': [0, 0, 0],\n          'carrying_object': None,\n          'current_task': None,\n          'task_progress': 0.0,\n          'interaction_context': {}\n      }\n\n      # Pipeline state\n      self.pipeline_active = False\n      self.pipeline_lock = threading.Lock()\n\n  def initialize_voice_recognizer(self):\n      # Initialize voice recognition system (Whisper, etc.)\n      return {\n          'type': 'whisper',\n          'model': 'tiny',\n          'active': True\n      }\n\n  def initialize_language_understanding(self):\n      # Initialize LLM-based language understanding\n      return {\n          'type': 'transformer',\n          'model': 'llm_task_decomposer',\n          'active': True\n      }\n\n  def initialize_task_decomposer(self):\n      # Initialize task decomposition system\n      return {\n          'type': 'hierarchical_planner',\n          'active': True\n      }\n\n  def initialize_action_planner(self):\n      # Initialize action planning system\n      return {\n          'type': 'ros2_action_planner',\n          'active': True\n      }\n\n  def initialize_action_executor(self):\n      # Initialize action execution system\n      return {\n          'type': 'ros2_action_executor',\n          'active': True\n      }\n\n  async def process_voice_command(self, voice_input: str) -> bool:\n      \"\"\"Process a voice command through the entire pipeline\"\"\"\n      with self.pipeline_lock:\n          if self.pipeline_active:\n              print(\"Pipeline busy, rejecting new command\")\n              return False\n\n          self.pipeline_active = True\n\n      try:\n          # Step 1: Voice recognition\n          voice_cmd = await self.voice_recognition_step(voice_input)\n          if not voice_cmd or voice_cmd.confidence < 0.5:\n              print(\"Voice recognition failed or low confidence\")\n              return False\n\n          # Step 2: Language understanding\n          parsed_cmd = await self.language_understanding_step(voice_cmd.text)\n          if not parsed_cmd:\n              print(\"Language understanding failed\")\n              return False\n\n          # Step 3: Task decomposition\n          task_plan = await self.task_decomposition_step(parsed_cmd)\n          if not task_plan:\n              print(\"Task decomposition failed\")\n              return False\n\n          # Step 4: Action planning\n          action_sequence = await self.action_planning_step(task_plan)\n          if not action_sequence:\n              print(\"Action planning failed\")\n              return False\n\n          # Step 5: Action execution\n          execution_result = await self.action_execution_step(action_sequence)\n\n          return execution_result\n\n      except Exception as e:\n          print(f\"Pipeline execution error: {str(e)}\")\n          return False\n      finally:\n          with self.pipeline_lock:\n              self.pipeline_active = False\n\n  async def voice_recognition_step(self, voice_input: str) -> Optional[VoiceCommand]:\n      \"\"\"Step 1: Convert voice input to text with confidence\"\"\"\n      # Simulate voice recognition\n      # In real implementation, this would use Whisper or similar\n      import random\n      confidence = random.uniform(0.7, 0.95)  # Simulated confidence\n\n      return VoiceCommand(\n          text=voice_input,  # In real implementation, this would be the recognized text\n          timestamp=time.time(),\n          confidence=confidence\n      )\n\n  async def language_understanding_step(self, text: str) -> Optional[ParsedCommand]:\n      \"\"\"Step 2: Parse natural language command into structured format\"\"\"\n      # Simple parsing for demonstration\n      # In real implementation, this would use LLM-based understanding\n      import re\n\n      # Extract action\n      action_patterns = [\n          (r'bring|get|fetch', 'bring'),\n          (r'go to|navigate to|move to', 'navigate'),\n          (r'pick up|grasp|take', 'grasp'),\n          (r'place|put|set', 'place'),\n          (r'clean|tidy', 'clean')\n      ]\n\n      action = None\n      for pattern, action_type in action_patterns:\n          if re.search(pattern, text, re.IGNORECASE):\n              action = action_type\n              break\n\n      if not action:\n          return None\n\n      # Extract objects\n      object_patterns = [\n          r'(?:thes+|as+|ans+)?(w+s+w+)s+(?:cup|bottle|book|object)',  # \"red cup\"\n          r'(?:thes+|as+|ans+)?(w+)s+(?:cup|bottle|book|object)',        # \"cup\"\n      ]\n\n      objects = []\n      for pattern in object_patterns:\n          matches = re.findall(pattern, text.lower())\n          objects.extend(matches)\n\n      # Extract locations\n      location_patterns = [\n          r'tos+(?:thes+)?(w+s+w+)',  # \"go to kitchen table\"\n          r'tos+(?:thes+)?(w+)',        # \"go to kitchen\"\n          r'(?:in|at|near)s+(?:thes+)?(w+s+w+)',  # \"in kitchen area\"\n      ]\n\n      locations = []\n      for pattern in location_patterns:\n          matches = re.findall(pattern, text.lower())\n          locations.extend(matches)\n\n      return ParsedCommand(\n          action=action,\n          objects=objects,\n          locations=locations,\n          confidence=0.8  # Simulated confidence\n      )\n\n  async def task_decomposition_step(self, parsed_cmd: ParsedCommand) -> Optional[TaskPlan]:\n      \"\"\"Step 3: Decompose high-level command into executable task sequence\"\"\"\n      # Create task plan based on parsed command\n      steps = []\n\n      if parsed_cmd.action == 'bring':\n          # Bring action decomposes to: navigate -> grasp -> navigate -> place\n          steps = [\n              {'type': 'navigate', 'target': parsed_cmd.locations[0] if parsed_cmd.locations else 'object_location'},\n              {'type': 'grasp', 'object': parsed_cmd.objects[0] if parsed_cmd.objects else 'target_object'},\n              {'type': 'navigate', 'target': 'delivery_location'},\n              {'type': 'place', 'location': 'delivery_location'}\n          ]\n      elif parsed_cmd.action == 'navigate':\n          steps = [\n              {'type': 'navigate', 'target': parsed_cmd.locations[0] if parsed_cmd.locations else 'default_target'}\n          ]\n      elif parsed_cmd.action == 'clean':\n          steps = [\n              {'type': 'navigate', 'target': parsed_cmd.locations[0] if parsed_cmd.locations else 'room_to_clean'},\n              {'type': 'clean_area', 'area': parsed_cmd.locations[0] if parsed_cmd.locations else 'default_area'}\n          ]\n      else:\n          # Default simple action\n          steps = [{'type': parsed_cmd.action, 'params': {'objects': parsed_cmd.objects, 'locations': parsed_cmd.locations}}]\n\n      return TaskPlan(\n          steps=steps,\n          priority='normal',\n          safety_critical=False\n      )\n\n  async def action_planning_step(self, task_plan: TaskPlan) -> Optional[list]:\n      \"\"\"Step 4: Plan specific actions for each task step\"\"\"\n      action_sequence = []\n\n      for step in task_plan.steps:\n          if step['type'] == 'navigate':\n              # Plan navigation action\n              nav_action = {\n                  'action_type': 'navigation',\n                  'target_pose': self.get_location_pose(step['target']),\n                  'planner': 'nav2'\n              }\n              action_sequence.append(nav_action)\n\n          elif step['type'] == 'grasp':\n              # Plan grasping action\n              grasp_action = {\n                  'action_type': 'manipulation',\n                  'action': 'grasp',\n                  'target_object': step['object'],\n                  'planner': 'manipulation_planner'\n              }\n              action_sequence.append(grasp_action)\n\n          elif step['type'] == 'place':\n              # Plan placing action\n              place_action = {\n                  'action_type': 'manipulation',\n                  'action': 'place',\n                  'target_location': step['location'],\n                  'planner': 'manipulation_planner'\n              }\n              action_sequence.append(place_action)\n\n          elif step['type'] == 'clean_area':\n              # Plan cleaning action\n              clean_action = {\n                  'action_type': 'cleaning',\n                  'target_area': step['area'],\n                  'planner': 'cleaning_planner'\n              }\n              action_sequence.append(clean_action)\n\n      return action_sequence\n\n  async def action_execution_step(self, action_sequence: list) -> bool:\n      \"\"\"Step 5: Execute the planned action sequence\"\"\"\n      for i, action in enumerate(action_sequence):\n          print(f\"Executing action {i+1}/{len(action_sequence)}: {action['action_type']}\")\n\n          # Update task progress\n          self.current_state['task_progress'] = (i / len(action_sequence)) * 100.0\n\n          # Execute the action\n          success = await self.execute_single_action(action)\n\n          if not success:\n              print(f\"Action execution failed at step {i+1}\")\n              return False\n\n      print(\"All actions completed successfully\")\n      return True\n\n  async def execute_single_action(self, action: Dict[str, Any]) -> bool:\n      \"\"\"Execute a single action and return success status\"\"\"\n      # Simulate action execution\n      # In real implementation, this would interface with ROS 2 action servers\n      import random\n      import asyncio\n\n      # Simulate execution time\n      await asyncio.sleep(0.5)  # Simulate processing time\n\n      # Simulate success/failure\n      success = random.random() > 0.1  # 90% success rate for simulation\n\n      if success:\n          print(f\"Action completed: {action['action_type']}\")\n          # Update robot state based on action\n          self.update_robot_state(action)\n      else:\n          print(f\"Action failed: {action['action_type']}\")\n\n      return success\n\n  def get_location_pose(self, location_name: str) -> list:\n      \"\"\"Get pose for a named location\"\"\"\n      # In real implementation, this would query a location map\n      location_map = {\n          'kitchen': [2.0, 1.0, 0.0],\n          'living_room': [0.0, 2.0, 0.0],\n          'bedroom': [-1.0, 1.0, 0.0],\n          'dining_room': [1.5, -1.0, 0.0],\n          'default_target': [0.0, 0.0, 0.0]\n      }\n      return location_map.get(location_name, [0.0, 0.0, 0.0])\n\n  def update_robot_state(self, action: Dict[str, Any]):\n      \"\"\"Update robot state based on executed action\"\"\"\n      if action['action_type'] == 'navigation':\n          self.current_state['robot_position'] = action.get('target_pose', [0, 0, 0])\n      elif action['action_type'] == 'manipulation':\n          if action['action'] == 'grasp':\n              self.current_state['carrying_object'] = action.get('target_object')\n          elif action['action'] == 'place':\n              self.current_state['carrying_object'] = None\n\n# Example usage\nasync def main():\n  pipeline = EndToEndPipeline()\n\n  # Process a sample command\n  result = await pipeline.process_voice_command(\"Bring me the red cup from the kitchen\")\n  print(f\"Command execution result: {result}\")\n\nif __name__ == \"__main__\":\n  asyncio.run(main())"}),"\n",(0,o.jsx)(n.p,{children:"Performance optimization of the end-to-end pipeline ensures that the complete processing chain meets the real-time requirements for natural interaction while maintaining the accuracy and safety required for robot operation. This includes optimizing data flow, minimizing processing latencies, and ensuring that critical safety functions remain responsive. The optimization must balance performance with resource constraints and maintain system stability."}),"\n",(0,o.jsx)(n.h3,{id:"concrete-examples-1",children:"Concrete Examples"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'Example: "Clean the room" command decomposed into navigation, object detection, and manipulation sequence'}),"\n",(0,o.jsx)(n.li,{children:"Example: State management maintaining task context when interrupted by user during cleaning task"}),"\n"]}),"\n",(0,o.jsx)(r.A,{question:"What is a critical requirement for end-to-end pipeline implementation in humanoid robots?",options:"Minimizing memory usage only||Meeting real-time requirements while maintaining accuracy and safety||Reducing computational complexity||Increasing storage capacity",correctAnswer:"Meeting real-time requirements while maintaining accuracy and safety",explanation:"The end-to-end pipeline must meet real-time requirements for natural interaction while maintaining the accuracy and safety required for robot operation, balancing performance with resource constraints."}),"\n",(0,o.jsx)(n.h2,{id:"system-validation-and-testing",children:"System Validation and Testing"}),"\n",(0,o.jsx)(n.p,{children:"Comprehensive system validation ensures that the integrated autonomous humanoid system operates correctly and safely across all operational scenarios. The validation process must verify that individual components function correctly within the integrated system and that their interactions produce the expected behavior. For humanoid robots, validation must include safety-critical functions and human-robot interaction scenarios."}),"\n",(0,o.jsx)(a.A,{type:"warning",title:"System Validation",children:(0,o.jsx)(n.p,{children:"Comprehensive system validation is essential for integrated autonomous humanoid systems, ensuring correct operation and safety across all scenarios, including safety-critical functions and human-robot interaction."})}),"\n",(0,o.jsx)(n.p,{children:"Unit and integration testing validates individual components and their interactions within the larger system. For the integrated humanoid system, this includes testing the voice recognition pipeline, language understanding module, task decomposition system, navigation planning, and action execution individually and in combination. The testing must cover normal operation, edge cases, and failure scenarios."}),"\n",(0,o.jsx)(c.A,{title:"System Validation Process",description:"Diagram showing the system validation process with unit, integration, simulation, and real-world testing",caption:"System validation process with unit, integration, simulation, and real-world testing"}),"\n",(0,o.jsx)(n.p,{children:"Simulation-based validation uses the digital twin environment to test the complete system in a wide variety of scenarios before deployment to physical hardware. For humanoid robots, simulation allows for testing in diverse environments, with various obstacles and situations that might be difficult or dangerous to recreate in physical testing. The simulation must accurately model the physical and behavioral characteristics of the real robot."}),"\n",(0,o.jsx)(l.A,{title:"System Validation Framework",problem:"Design a comprehensive validation framework for the integrated autonomous humanoid system.",hints:["Include unit, integration, simulation, and real-world testing","Consider safety-critical validation scenarios","Design automated testing procedures","Plan for gradual validation progression"],solution:"# Example system validation framework\nimport unittest\nimport asyncio\nfrom typing import Dict, List, Any\nimport time\n\nclass SystemValidationFramework:\n  def __init__(self):\n      self.test_results = {}\n      self.validation_log = []\n      self.safety_protocols = self.initialize_safety_protocols()\n\n  def initialize_safety_protocols(self):\n      return {\n          'emergency_stop': True,\n          'collision_detection': True,\n          'human_safety': True,\n          'validation_boundaries': True\n      }\n\n  def run_comprehensive_validation(self):\n      \"\"\"Run all validation tests in sequence\"\"\"\n      print(\"Starting comprehensive system validation...\")\n\n      # 1. Unit testing\n      unit_results = self.run_unit_tests()\n\n      # 2. Integration testing\n      integration_results = self.run_integration_tests()\n\n      # 3. Simulation validation\n      simulation_results = self.run_simulation_validation()\n\n      # 4. Real-world validation (if safe)\n      real_world_results = self.run_real_world_validation()\n\n      # 5. Generate validation report\n      validation_report = self.generate_validation_report(\n          unit_results, integration_results,\n          simulation_results, real_world_results\n      )\n\n      return validation_report\n\n  def run_unit_tests(self):\n      \"\"\"Run unit tests for individual components\"\"\"\n      print(\"Running unit tests...\")\n\n      results = {\n          'voice_recognition': self.test_voice_recognition(),\n          'language_understanding': self.test_language_understanding(),\n          'task_decomposition': self.test_task_decomposition(),\n          'navigation_planning': self.test_navigation_planning(),\n          'action_execution': self.test_action_execution(),\n          'safety_systems': self.test_safety_systems()\n      }\n\n      return results\n\n  def test_voice_recognition(self):\n      \"\"\"Test voice recognition component\"\"\"\n      try:\n          # Simulate voice recognition with various inputs\n          test_inputs = [\n              (\"Hello robot\", 0.9),\n              (\"Bring me the cup\", 0.85),\n              (\"Clean the room\", 0.88)\n          ]\n\n          success_count = 0\n          for text, expected_confidence in test_inputs:\n              # Simulate recognition process\n              result = self.simulate_recognition(text)\n              if result['confidence'] >= 0.7:\n                  success_count += 1\n\n          success_rate = success_count / len(test_inputs)\n          return {'success_rate': success_rate, 'status': 'pass' if success_rate >= 0.8 else 'fail'}\n\n      except Exception as e:\n          return {'status': 'error', 'error': str(e)}\n\n  def test_language_understanding(self):\n      \"\"\"Test language understanding component\"\"\"\n      try:\n          test_commands = [\n              \"Bring me the red cup\",\n              \"Go to the kitchen\",\n              \"Pick up the book\"\n          ]\n\n          success_count = 0\n          for cmd in test_commands:\n              parsed = self.simulate_language_parsing(cmd)\n              if parsed and 'action' in parsed:\n                  success_count += 1\n\n          success_rate = success_count / len(test_commands)\n          return {'success_rate': success_rate, 'status': 'pass' if success_rate >= 0.8 else 'fail'}\n\n      except Exception as e:\n          return {'status': 'error', 'error': str(e)}\n\n  def test_task_decomposition(self):\n      \"\"\"Test task decomposition component\"\"\"\n      try:\n          complex_commands = [\n              \"Bring me the red cup from the kitchen\",\n              \"Clean the living room and then go to bedroom\"\n          ]\n\n          success_count = 0\n          for cmd in complex_commands:\n              task_plan = self.simulate_task_decomposition(cmd)\n              if task_plan and len(task_plan) > 0:\n                  success_count += 1\n\n          success_rate = success_count / len(complex_commands)\n          return {'success_rate': success_rate, 'status': 'pass' if success_rate >= 0.8 else 'fail'}\n\n      except Exception as e:\n          return {'status': 'error', 'error': str(e)}\n\n  def test_navigation_planning(self):\n      \"\"\"Test navigation planning component\"\"\"\n      try:\n          test_scenarios = [\n              {'start': [0, 0], 'goal': [5, 5], 'obstacles': []},\n              {'start': [0, 0], 'goal': [10, 10], 'obstacles': [[3, 3], [6, 6]]}\n          ]\n\n          success_count = 0\n          for scenario in test_scenarios:\n              path = self.simulate_navigation_planning(scenario)\n              if path and len(path) > 0:\n                  success_count += 1\n\n          success_rate = success_count / len(test_scenarios)\n          return {'success_rate': success_rate, 'status': 'pass' if success_rate >= 0.8 else 'fail'}\n\n      except Exception as e:\n          return {'status': 'error', 'error': str(e)}\n\n  def test_action_execution(self):\n      \"\"\"Test action execution component\"\"\"\n      try:\n          test_actions = [\n              {'type': 'navigate', 'target': [1, 1]},\n              {'type': 'grasp', 'object': 'cup'},\n              {'type': 'place', 'location': 'table'}\n          ]\n\n          success_count = 0\n          for action in test_actions:\n              success = self.simulate_action_execution(action)\n              if success:\n                  success_count += 1\n\n          success_rate = success_count / len(test_actions)\n          return {'success_rate': success_rate, 'status': 'pass' if success_rate >= 0.8 else 'fail'}\n\n      except Exception as e:\n          return {'status': 'error', 'error': str(e)}\n\n  def test_safety_systems(self):\n      \"\"\"Test safety systems\"\"\"\n      try:\n          safety_tests = [\n              self.test_collision_avoidance(),\n              self.test_emergency_stop(),\n              self.test_human_safety_protocols()\n          ]\n\n          success_count = sum(1 for test in safety_tests if test)\n          success_rate = success_count / len(safety_tests)\n          return {'success_rate': success_rate, 'status': 'pass' if success_rate >= 0.9 else 'fail'}\n\n      except Exception as e:\n          return {'status': 'error', 'error': str(e)}\n\n  def run_integration_tests(self):\n      \"\"\"Run integration tests for component interactions\"\"\"\n      print(\"Running integration tests...\")\n\n      # Test the complete pipeline\n      pipeline_success = self.test_end_to_end_pipeline()\n\n      # Test data flow between components\n      data_flow_success = self.test_data_flow_integrity()\n\n      # Test timing and synchronization\n      timing_success = self.test_timing_synchronization()\n\n      return {\n          'end_to_end_pipeline': pipeline_success,\n          'data_flow_integrity': data_flow_success,\n          'timing_synchronization': timing_success\n      }\n\n  def run_simulation_validation(self):\n      \"\"\"Run validation in simulation environment\"\"\"\n      print(\"Running simulation validation...\")\n\n      simulation_scenarios = [\n          {'name': 'object_fetch', 'environment': 'home', 'success_criteria': ['fetch_success', 'no_collision']},\n          {'name': 'room_navigation', 'environment': 'office', 'success_criteria': ['navigation_success', 'safe_motion']},\n          {'name': 'human_interaction', 'environment': 'living_room', 'success_criteria': ['safe_interaction', 'correct_response']}\n      ]\n\n      results = {}\n      for scenario in simulation_scenarios:\n          result = self.run_simulation_scenario(scenario)\n          results[scenario['name']] = result\n\n      return results\n\n  def run_real_world_validation(self):\n      \"\"\"Run safe real-world validation\"\"\"\n      print(\"Running real-world validation...\")\n\n      # Only run if safety protocols pass simulation\n      if not self.safety_protocols_pass_simulation():\n          print(\"Skipping real-world validation due to simulation failures\")\n          return {'status': 'skipped', 'reason': 'simulation failures'}\n\n      # Run controlled real-world tests\n      real_world_tests = [\n          self.test_real_world_navigation(),\n          self.test_real_world_manipulation(),\n          self.test_real_world_interaction()\n      ]\n\n      success_count = sum(1 for test in real_world_tests if test)\n      success_rate = success_count / len(real_world_tests)\n\n      return {'success_rate': success_rate, 'status': 'completed'}\n\n  def generate_validation_report(self, unit_results, integration_results,\n                               simulation_results, real_world_results):\n      \"\"\"Generate comprehensive validation report\"\"\"\n      report = {\n          'timestamp': time.time(),\n          'unit_test_results': unit_results,\n          'integration_test_results': integration_results,\n          'simulation_test_results': simulation_results,\n          'real_world_test_results': real_world_results,\n          'overall_status': self.calculate_overall_status(\n              unit_results, integration_results,\n              simulation_results, real_world_results\n          ),\n          'recommendations': self.generate_recommendations(\n              unit_results, integration_results,\n              simulation_results, real_world_results\n          )\n      }\n\n      return report\n\n  def calculate_overall_status(self, unit_results, integration_results,\n                              simulation_results, real_world_results):\n      \"\"\"Calculate overall validation status\"\"\"\n      # Simplified status calculation\n      # In real implementation, this would be more sophisticated\n      return 'pass' if all([\n          self.check_component_status(unit_results),\n          self.check_component_status(integration_results),\n          self.check_component_status(simulation_results)\n      ]) else 'fail'\n\n  def check_component_status(self, results):\n      \"\"\"Check if component results meet validation criteria\"\"\"\n      if isinstance(results, dict):\n          for key, value in results.items():\n              if isinstance(value, dict) and value.get('status') == 'fail':\n                  return False\n      return True\n\n  def generate_recommendations(self, unit_results, integration_results,\n                              simulation_results, real_world_results):\n      \"\"\"Generate validation recommendations\"\"\"\n      recommendations = []\n\n      # Add recommendations based on test results\n      if not self.check_component_status(unit_results):\n          recommendations.append(\"Address unit test failures before proceeding\")\n\n      if not self.check_component_status(integration_results):\n          recommendations.append(\"Fix integration issues\")\n\n      return recommendations\n\n  # Simulation methods (would interface with Isaac Sim or Gazebo)\n  def simulate_recognition(self, text):\n      \"\"\"Simulate voice recognition\"\"\"\n      import random\n      return {\n          'text': text,\n          'confidence': random.uniform(0.7, 0.95)\n      }\n\n  def simulate_language_parsing(self, command):\n      \"\"\"Simulate language parsing\"\"\"\n      return {'action': 'test_action', 'objects': ['test_object'], 'confidence': 0.8}\n\n  def simulate_task_decomposition(self, command):\n      \"\"\"Simulate task decomposition\"\"\"\n      return [{'action': 'test', 'params': {}}]\n\n  def simulate_navigation_planning(self, scenario):\n      \"\"\"Simulate navigation planning\"\"\"\n      return [[0, 0], [1, 1], [2, 2], [3, 3]]\n\n  def simulate_action_execution(self, action):\n      \"\"\"Simulate action execution\"\"\"\n      import random\n      return random.random() > 0.1  # 90% success rate\n\n  def test_collision_avoidance(self):\n      \"\"\"Test collision avoidance\"\"\"\n      return True\n\n  def test_emergency_stop(self):\n      \"\"\"Test emergency stop functionality\"\"\"\n      return True\n\n  def test_human_safety_protocols(self):\n      \"\"\"Test human safety protocols\"\"\"\n      return True\n\n  def test_end_to_end_pipeline(self):\n      \"\"\"Test complete end-to-end pipeline\"\"\"\n      return True\n\n  def test_data_flow_integrity(self):\n      \"\"\"Test data flow between components\"\"\"\n      return True\n\n  def test_timing_synchronization(self):\n      \"\"\"Test timing and synchronization\"\"\"\n      return True\n\n  def run_simulation_scenario(self, scenario):\n      \"\"\"Run a simulation scenario\"\"\"\n      return {'status': 'success', 'metrics': {'success_rate': 0.95, 'safety_score': 0.98}}\n\n  def safety_protocols_pass_simulation(self):\n      \"\"\"Check if safety protocols passed simulation\"\"\"\n      return True\n\n  def test_real_world_navigation(self):\n      \"\"\"Test real-world navigation in controlled environment\"\"\"\n      return True\n\n  def test_real_world_manipulation(self):\n      \"\"\"Test real-world manipulation in controlled environment\"\"\"\n      return True\n\n  def test_real_world_interaction(self):\n      \"\"\"Test real-world interaction in controlled environment\"\"\"\n      return True\n\n# Example usage\ndef main():\n  validator = SystemValidationFramework()\n  report = validator.run_comprehensive_validation()\n  print(f\"Validation completed with status: {report['overall_status']}\")\n  print(f\"Recommendations: {report['recommendations']}\")\n\nif __name__ == \"__main__\":\n  main()"}),"\n",(0,o.jsx)(n.p,{children:"Real-world validation on physical hardware verifies that the system operates correctly in actual operating conditions. This includes testing with real sensors, actuators, and environmental conditions that may differ from simulation. The real-world validation must include safety protocols and gradual progression from simple to complex scenarios."}),"\n",(0,o.jsx)(n.h3,{id:"concrete-examples-2",children:"Concrete Examples"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'Example: Testing "Bring me water" command in simulation before real-world execution with safety protocols'}),"\n",(0,o.jsx)(n.li,{children:"Example: Gradual validation progression from simple object pickup to complex multi-room navigation tasks"}),"\n"]}),"\n",(0,o.jsx)(r.A,{question:"What is the primary purpose of simulation-based validation for humanoid robots?",options:["To reduce computational requirements","To test the complete system in various scenarios before deployment to physical hardware","To improve audio quality","To increase network speed"],correctAnswer:"To test the complete system in various scenarios before deployment to physical hardware",explanation:"Simulation-based validation uses the digital twin environment to test the complete system in a wide variety of scenarios before deployment to physical hardware, allowing testing in diverse environments safely."}),"\n",(0,o.jsx)(n.h2,{id:"performance-optimization-and-debugging",children:"Performance Optimization and Debugging"}),"\n",(0,o.jsx)(n.p,{children:"Performance optimization of the integrated system addresses the computational and timing requirements of real-time autonomous operation. The optimization must balance the competing demands of different subsystems while ensuring that safety-critical functions remain responsive. For humanoid robots, this includes optimizing AI inference, sensor processing, and control loops to meet real-time requirements."}),"\n",(0,o.jsx)(a.A,{type:"danger",title:"Performance and Safety Balance",children:(0,o.jsx)(n.p,{children:"Performance optimization must balance competing demands of different subsystems while ensuring safety-critical functions remain responsive, optimizing AI inference, sensor processing, and control loops for real-time humanoid operation."})}),"\n",(0,o.jsx)(n.p,{children:"Multi-module debugging addresses the complexity of debugging integrated systems where problems may arise from interactions between components rather than individual component failures. For humanoid robots, this requires sophisticated logging, monitoring, and diagnostic tools that can trace problems across the entire system. The debugging infrastructure must support both development and operational environments."}),"\n",(0,o.jsx)(c.A,{title:"Multi-Module Debugging System",description:"Diagram showing the multi-module debugging system with logging, monitoring, and diagnostic tools",caption:"Multi-module debugging system with logging, monitoring, and diagnostic tools"}),"\n",(0,o.jsx)(n.p,{children:"Resource contention management resolves conflicts between different subsystems competing for shared resources such as CPU, GPU, memory, and communication bandwidth. For humanoid robots, this includes managing the trade-offs between perception accuracy, planning complexity, and control frequency. The resource management must adapt to changing operational conditions while maintaining system stability."}),"\n",(0,o.jsx)(l.A,{title:"Performance Optimization System",problem:"Implement a performance optimization and debugging system for the integrated humanoid robot.",hints:["Design resource allocation and contention management","Create comprehensive logging and monitoring","Implement adaptive performance optimization","Plan for debugging multi-module interactions"],solution:"# Example performance optimization and debugging system\nimport psutil\nimport time\nimport threading\nfrom collections import defaultdict, deque\nimport json\nfrom datetime import datetime\n\nclass PerformanceOptimizationSystem:\n  def __init__(self):\n      self.resource_manager = ResourceManager()\n      self.performance_monitor = PerformanceMonitor()\n      self.debugger = MultiModuleDebugger()\n      self.optimizer = AdaptiveOptimizer()\n\n      # Performance thresholds\n      self.thresholds = {\n          'cpu_usage': 85.0,  # percent\n          'gpu_usage': 90.0,  # percent\n          'memory_usage': 80.0,  # percent\n          'pipeline_latency': 2.0,  # seconds\n          'control_frequency': 50.0  # Hz\n      }\n\n      # Optimization state\n      self.optimization_active = True\n      self.monitoring_thread = None\n\n  def start_optimization_system(self):\n      \"\"\"Start the optimization and monitoring system\"\"\"\n      print(\"Starting performance optimization system...\")\n\n      # Start monitoring thread\n      self.monitoring_thread = threading.Thread(target=self.monitoring_loop, daemon=True)\n      self.monitoring_thread.start()\n\n      # Start optimization thread\n      optimization_thread = threading.Thread(target=self.optimization_loop, daemon=True)\n      optimization_thread.start()\n\n  def monitoring_loop(self):\n      \"\"\"Continuous monitoring loop\"\"\"\n      while self.optimization_active:\n          # Collect performance metrics\n          metrics = self.performance_monitor.collect_metrics()\n\n          # Log metrics\n          self.debugger.log_metrics(metrics)\n\n          # Check for threshold violations\n          violations = self.check_threshold_violations(metrics)\n          if violations:\n              self.handle_performance_violations(violations, metrics)\n\n          time.sleep(0.1)  # 10 Hz monitoring\n\n  def optimization_loop(self):\n      \"\"\"Continuous optimization loop\"\"\"\n      while self.optimization_active:\n          # Get current system state\n          system_state = self.get_system_state()\n\n          # Apply optimizations\n          optimizations = self.optimizer.calculate_optimizations(system_state)\n          self.apply_optimizations(optimizations)\n\n          time.sleep(0.5)  # 2 Hz optimization\n\n  def check_threshold_violations(self, metrics):\n      \"\"\"Check for performance threshold violations\"\"\"\n      violations = []\n\n      for metric_name, threshold in self.thresholds.items():\n          if metric_name in metrics:\n              if metric_name == 'control_frequency':\n                  # For frequency, lower is worse (less responsive)\n                  if metrics[metric_name] < threshold:\n                      violations.append({\n                          'metric': metric_name,\n                          'value': metrics[metric_name],\n                          'threshold': threshold,\n                          'severity': 'high'\n                      })\n              else:\n                  # For other metrics, higher is worse (more resource usage)\n                  if metrics[metric_name] > threshold:\n                      severity = 'high' if metrics[metric_name] > threshold * 1.2 else 'medium'\n                      violations.append({\n                          'metric': metric_name,\n                          'value': metrics[metric_name],\n                          'threshold': threshold,\n                          'severity': severity\n                      })\n\n      return violations\n\n  def handle_performance_violations(self, violations, metrics):\n      \"\"\"Handle performance threshold violations\"\"\"\n      for violation in violations:\n          print(f\"PERFORMANCE VIOLATION: {violation['metric']} = {violation['value']}, threshold = {violation['threshold']}\")\n\n          # Log violation\n          self.debugger.log_performance_violation(violation, metrics)\n\n          # Apply appropriate response based on severity\n          if violation['severity'] == 'high':\n              # Apply immediate optimizations\n              self.apply_emergency_optimizations(violation['metric'])\n          else:\n              # Apply standard optimizations\n              self.apply_standard_optimizations(violation['metric'])\n\n  def apply_emergency_optimizations(self, metric_name):\n      \"\"\"Apply emergency optimizations for high-severity violations\"\"\"\n      if metric_name in ['cpu_usage', 'gpu_usage', 'memory_usage']:\n          # Reduce computational load\n          self.optimizer.reduce_computational_load()\n      elif metric_name == 'pipeline_latency':\n          # Reduce pipeline complexity\n          self.optimizer.reduce_pipeline_complexity()\n      elif metric_name == 'control_frequency':\n          # Prioritize control systems\n          self.optimizer.prioritize_control_systems()\n\n  def apply_standard_optimizations(self, metric_name):\n      \"\"\"Apply standard optimizations for medium-severity violations\"\"\"\n      # Apply less aggressive optimizations\n      if metric_name in ['cpu_usage', 'gpu_usage']:\n          self.optimizer.adjust_compute_allocation()\n      elif metric_name == 'memory_usage':\n          self.optimizer.run_garbage_collection()\n\n  def get_system_state(self):\n      \"\"\"Get current system state for optimization\"\"\"\n      return {\n          'metrics': self.performance_monitor.collect_metrics(),\n          'resource_usage': self.resource_manager.get_resource_usage(),\n          'active_processes': self.resource_manager.get_active_processes(),\n          'system_load': psutil.cpu_percent(interval=1)\n      }\n\n  def apply_optimizations(self, optimizations):\n      \"\"\"Apply calculated optimizations\"\"\"\n      for optimization in optimizations:\n          if optimization['type'] == 'resource_allocation':\n              self.resource_manager.adjust_resource_allocation(\n                  optimization['process'],\n                  optimization['resources']\n              )\n          elif optimization['type'] == 'process_priority':\n              self.resource_manager.adjust_process_priority(\n                  optimization['process'],\n                  optimization['priority']\n              )\n          elif optimization['type'] == 'pipeline_optimization':\n              self.optimizer.optimize_pipeline(\n                  optimization['pipeline'],\n                  optimization['optimization']\n              )\n\n  def stop_optimization_system(self):\n      \"\"\"Stop the optimization system\"\"\"\n      self.optimization_active = False\n      if self.monitoring_thread:\n          self.monitoring_thread.join(timeout=1.0)\n\nclass ResourceManager:\n  def __init__(self):\n      self.resource_allocations = {}\n      self.process_priorities = {}\n      self.resource_limits = {\n          'cpu_percent': 80.0,\n          'memory_percent': 70.0,\n          'gpu_memory_percent': 85.0\n      }\n\n  def get_resource_usage(self):\n      \"\"\"Get current resource usage\"\"\"\n      return {\n          'cpu_percent': psutil.cpu_percent(interval=1),\n          'memory_percent': psutil.virtual_memory().percent,\n          'disk_usage': psutil.disk_usage('/').percent,\n          'process_count': len(psutil.pids())\n      }\n\n  def get_active_processes(self):\n      \"\"\"Get information about active processes\"\"\"\n      processes = []\n      for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent']):\n          try:\n              processes.append(proc.info)\n          except (psutil.NoSuchProcess, psutil.AccessDenied):\n              pass\n      return processes\n\n  def adjust_resource_allocation(self, process_name, resources):\n      \"\"\"Adjust resource allocation for a process\"\"\"\n      self.resource_allocations[process_name] = resources\n\n  def adjust_process_priority(self, process_name, priority):\n      \"\"\"Adjust process priority\"\"\"\n      self.process_priorities[process_name] = priority\n\nclass PerformanceMonitor:\n  def __init__(self):\n      self.metrics_history = defaultdict(lambda: deque(maxlen=100))  # Keep last 100 readings\n\n  def collect_metrics(self):\n      \"\"\"Collect current performance metrics\"\"\"\n      metrics = {\n          'timestamp': time.time(),\n          'cpu_percent': psutil.cpu_percent(interval=1),\n          'memory_percent': psutil.virtual_memory().percent,\n          'disk_usage': psutil.disk_usage('/').percent,\n          'process_count': len(psutil.pids()),\n          'network_io': psutil.net_io_counters(),\n          'sensors': self.get_sensor_metrics(),\n          'pipeline_latency': self.get_pipeline_latency(),\n          'control_frequency': self.get_control_frequency()\n      }\n\n      # Store in history\n      for key, value in metrics.items():\n          if isinstance(value, (int, float)):\n              self.metrics_history[key].append(value)\n\n      return metrics\n\n  def get_sensor_metrics(self):\n      \"\"\"Get sensor-related metrics\"\"\"\n      # Placeholder for sensor metrics\n      return {'sensor_frequency': 30.0, 'data_rate': 1000.0}\n\n  def get_pipeline_latency(self):\n      \"\"\"Get pipeline processing latency\"\"\"\n      # Placeholder - would measure actual pipeline latency\n      return 0.5  # seconds\n\n  def get_control_frequency(self):\n      \"\"\"Get control system frequency\"\"\"\n      # Placeholder - would measure actual control frequency\n      return 100.0  # Hz\n\n  def get_historical_average(self, metric_name):\n      \"\"\"Get historical average for a metric\"\"\"\n      if self.metrics_history[metric_name]:\n          return sum(self.metrics_history[metric_name]) / len(self.metrics_history[metric_name])\n      return 0.0\n\nclass MultiModuleDebugger:\n  def __init__(self):\n      self.log_buffer = deque(maxlen=1000)\n      self.error_buffer = deque(maxlen=100)\n      self.debug_level = 'info'  # debug, info, warning, error\n\n  def log_metrics(self, metrics):\n      \"\"\"Log performance metrics\"\"\"\n      log_entry = {\n          'timestamp': datetime.now().isoformat(),\n          'type': 'metrics',\n          'data': metrics,\n          'level': 'info'\n      }\n      self.log_buffer.append(log_entry)\n\n  def log_performance_violation(self, violation, metrics):\n      \"\"\"Log performance violation\"\"\"\n      log_entry = {\n          'timestamp': datetime.now().isoformat(),\n          'type': 'performance_violation',\n          'violation': violation,\n          'metrics': metrics,\n          'level': 'warning'\n      }\n      self.log_buffer.append(log_entry)\n      self.error_buffer.append(log_entry)\n\n  def get_detailed_logs(self, hours=1):\n      \"\"\"Get detailed logs for analysis\"\"\"\n      cutoff_time = time.time() - (hours * 3600)\n      recent_logs = [log for log in self.log_buffer\n                    if time.mktime(datetime.fromisoformat(log['timestamp']).timetuple()) > cutoff_time]\n      return recent_logs\n\nclass AdaptiveOptimizer:\n  def __init__(self):\n      self.optimization_history = []\n      self.current_optimization_level = 'normal'\n\n  def calculate_optimizations(self, system_state):\n      \"\"\"Calculate optimizations based on system state\"\"\"\n      optimizations = []\n\n      metrics = system_state['metrics']\n\n      # CPU optimization\n      if metrics['cpu_percent'] > 80:\n          optimizations.append({\n              'type': 'resource_allocation',\n              'process': 'perception',\n              'resources': {'cpu_quota': 0.6}  # Reduce CPU allocation\n          })\n\n      # Memory optimization\n      if metrics['memory_percent'] > 75:\n          optimizations.append({\n              'type': 'process_priority',\n              'process': 'non_critical_service',\n              'priority': 'low'\n          })\n\n      # Pipeline optimization\n      if metrics['pipeline_latency'] > 1.0:\n          optimizations.append({\n              'type': 'pipeline_optimization',\n              'pipeline': 'perception_pipeline',\n              'optimization': 'reduce_resolution'\n          })\n\n      return optimizations\n\n  def reduce_computational_load(self):\n      \"\"\"Reduce computational load across systems\"\"\"\n      # Reduce perception processing frequency\n      # Simplify planning algorithms\n      # Lower control frequency if safe\n      pass\n\n  def reduce_pipeline_complexity(self):\n      \"\"\"Reduce pipeline complexity\"\"\"\n      # Use simpler models\n      # Reduce processing resolution\n      # Skip non-critical processing steps\n      pass\n\n  def prioritize_control_systems(self):\n      \"\"\"Prioritize control systems\"\"\"\n      # Increase control system priority\n      # Allocate more resources to control\n      # Reduce other system priorities\n      pass\n\n  def adjust_compute_allocation(self):\n      \"\"\"Adjust compute allocation\"\"\"\n      # Rebalance CPU/GPU usage\n      # Adjust process priorities\n      pass\n\n  def run_garbage_collection(self):\n      \"\"\"Run garbage collection to free memory\"\"\"\n      import gc\n      gc.collect()\n\n  def optimize_pipeline(self, pipeline, optimization):\n      \"\"\"Optimize a specific pipeline\"\"\"\n      # Apply pipeline-specific optimizations\n      pass\n\n# Example usage\ndef main():\n  perf_system = PerformanceOptimizationSystem()\n  perf_system.start_optimization_system()\n\n  print(\"Performance optimization system running...\")\n  print(\"Monitoring and optimizing system performance...\")\n\n  try:\n      # Let it run for a while\n      time.sleep(10)\n  except KeyboardInterrupt:\n      print(\"Stopping performance optimization system...\")\n\n  perf_system.stop_optimization_system()\n  print(\"Performance optimization system stopped.\")\n\nif __name__ == \"__main__\":\n  main()"}),"\n",(0,o.jsx)(n.p,{children:"Fault tolerance and recovery mechanisms ensure that the system can continue operating safely when individual components fail or when unexpected conditions occur. For humanoid robots, this includes graceful degradation of capabilities, safe state transitions, and recovery procedures that maintain safety while minimizing disruption to operation. The fault tolerance must handle both temporary and permanent failures."}),"\n",(0,o.jsx)(n.h3,{id:"concrete-examples-3",children:"Concrete Examples"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Example: Optimizing GPU usage between perception and language processing during simultaneous tasks"}),"\n",(0,o.jsx)(n.li,{children:"Example: Fault tolerance mechanism safely stopping robot when navigation system fails during task execution"}),"\n"]}),"\n",(0,o.jsx)(r.A,{question:"What is a key challenge in multi-module debugging for integrated humanoid systems?",options:["Reducing the number of components","Addressing problems that arise from interactions between components rather than individual failures","Minimizing memory usage","Increasing computational speed"],correctAnswer:"Addressing problems that arise from interactions between components rather than individual failures",explanation:"Multi-module debugging addresses the complexity of debugging integrated systems where problems may arise from interactions between components rather than individual component failures, requiring sophisticated logging and diagnostic tools."}),"\n",(0,o.jsx)(n.h2,{id:"forward-references-to-capstone-project",children:"Forward References to Capstone Project"}),"\n",(0,o.jsx)(n.p,{children:"The end-to-end integration concepts covered in this chapter provide the foundation for completing your Autonomous Humanoid capstone project. The system integration techniques will enable you to connect all the components you've developed into a unified autonomous system. The validation strategies will ensure your robot operates safely and effectively, while the optimization techniques will ensure real-time performance for natural human-robot interaction."}),"\n",(0,o.jsx)(n.h2,{id:"ethical--safety-considerations",children:"Ethical & Safety Considerations"}),"\n",(0,o.jsx)(n.p,{children:"The integration of a complete autonomous humanoid system raises important ethical and safety considerations regarding autonomous decision-making and human-robot interaction in complex environments. The system must be designed with appropriate safety constraints and oversight mechanisms to ensure safe operation in human environments. Comprehensive validation and testing are essential to verify that the integrated system behaves safely across all operational scenarios. The system should include mechanisms for human override and clear communication of the robot's intentions and limitations to maintain trust and enable appropriate oversight."}),"\n",(0,o.jsx)(a.A,{type:"danger",title:"Safety and Oversight",children:(0,o.jsx)(n.p,{children:"Complete autonomous humanoid systems must include appropriate safety constraints, comprehensive validation, human override mechanisms, and clear communication of intentions and limitations to ensure safe operation and maintain trust in human environments."})}),"\n",(0,o.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Complete system integration combines all course modules into a unified autonomous humanoid system"}),"\n",(0,o.jsx)(n.li,{children:"End-to-end pipeline implementation connects voice command to action execution with proper state management"}),"\n",(0,o.jsx)(n.li,{children:"Comprehensive validation ensures safe and reliable operation across all scenarios"}),"\n",(0,o.jsx)(n.li,{children:"Performance optimization balances competing demands of different subsystems"}),"\n",(0,o.jsx)(n.li,{children:"Multi-module debugging addresses complexity of integrated system interactions"}),"\n",(0,o.jsx)(n.li,{children:"Fault tolerance mechanisms ensure safe operation despite component failures"}),"\n"]})]})}function g(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(f,{...e})}):f(e)}},6212(e,n,t){t.d(n,{A:()=>s});var i=t(6540),o=t(4848);const s=({title:e,problem:n,solution:t,hints:s=[],initialCode:a="",className:r=""})=>{const[l,c]=(0,i.useState)(a),[m,d]=(0,i.useState)(!1),[p,u]=(0,i.useState)(!1),[f,g]=(0,i.useState)(!1),[_,h]=(0,i.useState)(null);return(0,o.jsxs)("div",{className:`exercise-component ${r}`,style:{border:"1px solid #ddd",borderRadius:"8px",padding:"1rem",margin:"1rem 0",backgroundColor:"#fff"},children:[(0,o.jsx)("h4",{style:{margin:"0 0 1rem 0"},children:e}),(0,o.jsxs)("div",{style:{marginBottom:"1rem"},children:[(0,o.jsx)("h5",{style:{margin:"0.5rem 0",color:"#202124"},children:"Problem:"}),(0,o.jsx)("div",{style:{padding:"0.5rem",backgroundColor:"#f9f9f9",borderRadius:"4px"},children:n})]}),s.length>0&&(0,o.jsxs)("div",{style:{marginBottom:"1rem"},children:[(0,o.jsx)("button",{onClick:()=>u(!p),style:{padding:"0.5rem 1rem",backgroundColor:"#fbbc04",color:"white",border:"none",borderRadius:"4px",cursor:"pointer",marginBottom:"0.5rem"},children:p?"Hide Hint":"Show Hint"}),p&&(0,o.jsxs)("div",{style:{padding:"0.5rem",backgroundColor:"#fef7e0",borderRadius:"4px",border:"1px solid #fbbc04"},children:[(0,o.jsx)("strong",{children:"Hint:"})," ",s[0]]})]}),(0,o.jsxs)("div",{style:{marginBottom:"1rem"},children:[(0,o.jsx)("h5",{style:{margin:"0.5rem 0",color:"#202124"},children:"Your Solution:"}),(0,o.jsx)("textarea",{value:l,onChange:e=>c(e.target.value),style:{width:"100%",minHeight:"150px",padding:"0.5rem",fontFamily:"monospace",border:"1px solid #ddd",borderRadius:"4px",fontSize:"0.9rem"},placeholder:"Write your solution here..."})]}),(0,o.jsxs)("div",{style:{display:"flex",gap:"0.5rem",marginBottom:"1rem"},children:[(0,o.jsx)("button",{onClick:()=>{h({success:!0,message:"Code executed successfully! Check your logic against the solution."}),g(!0)},style:{padding:"0.5rem 1rem",backgroundColor:"#4caf50",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Run Code"}),(0,o.jsx)("button",{onClick:()=>d(!m),style:{padding:"0.5rem 1rem",backgroundColor:"#2196f3",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:m?"Hide Solution":"Show Solution"}),(0,o.jsx)("button",{onClick:()=>{c(a),d(!1),u(!1),g(!1),h(null)},style:{padding:"0.5rem 1rem",backgroundColor:"#9e9e9e",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Reset"})]}),f&&_&&(0,o.jsxs)("div",{style:{padding:"0.75rem",borderRadius:"4px",backgroundColor:_.success?"#e8f5e9":"#ffebee",border:"1px solid "+(_.success?"#4caf50":"#f44336"),marginBottom:"1rem"},children:[(0,o.jsx)("strong",{children:"Status:"})," ",_.message]}),m&&(0,o.jsxs)("div",{style:{padding:"0.5rem",backgroundColor:"#e8f5e9",borderRadius:"4px",border:"1px solid #4caf50"},children:[(0,o.jsx)("h5",{style:{margin:"0.5rem 0",color:"#202124"},children:"Solution:"}),(0,o.jsx)("pre",{style:{padding:"0.5rem",backgroundColor:"#f1f8e9",borderRadius:"4px",overflowX:"auto",whiteSpace:"pre-wrap"},children:t})]})]})}},7589(e,n,t){t.d(n,{A:()=>s});var i=t(6540),o=t(4848);const s=({question:e,options:n,correctAnswer:t,explanation:s,className:a=""})=>{const r=Array.isArray(n)?n:n&&"string"==typeof n?n.split("||"):[],[l,c]=(0,i.useState)(null),[m,d]=(0,i.useState)(!1),[p,u]=(0,i.useState)(!1),f=e=>{if(m)return;c(e);u(e===t),d(!0)},g=e=>m?e===t?{padding:"0.75rem",margin:"0.5rem 0",border:"1px solid #4caf50",borderRadius:"4px",backgroundColor:"#e8f5e9",fontWeight:"bold"}:e===l&&e!==t?{padding:"0.75rem",margin:"0.5rem 0",border:"1px solid #f44336",borderRadius:"4px",backgroundColor:"#ffebee"}:{padding:"0.75rem",margin:"0.5rem 0",border:"1px solid #ddd",borderRadius:"4px",backgroundColor:"#f5f5f5"}:{padding:"0.75rem",margin:"0.5rem 0",cursor:"pointer",border:"1px solid #ddd",borderRadius:"4px",backgroundColor:l===e?"#e3f2fd":"#fff"};return(0,o.jsxs)("div",{className:`quiz-component ${a}`,style:{border:"1px solid #ddd",borderRadius:"8px",padding:"1rem",margin:"1rem 0",backgroundColor:"#fff"},children:[(0,o.jsx)("h4",{style:{margin:"0 0 1rem 0"},children:e}),(0,o.jsx)("div",{children:r.map((n,t)=>(0,o.jsxs)("div",{style:g(n),onClick:()=>f(n),children:[(0,o.jsx)("input",{type:"radio",name:`quiz-${e}`,value:n,checked:l===n,onChange:()=>{},disabled:m,style:{marginRight:"0.5rem"}}),n]},t))}),m&&(0,o.jsxs)("div",{style:{marginTop:"1rem",padding:"0.75rem",borderRadius:"4px",backgroundColor:p?"#e8f5e9":"#ffebee",border:"1px solid "+(p?"#4caf50":"#f44336")},children:[(0,o.jsx)("p",{style:{margin:"0.5rem 0",fontWeight:"bold"},children:p?"\u2705 Correct!":"\u274c Incorrect"}),s&&(0,o.jsxs)("p",{style:{margin:"0.5rem 0"},children:[(0,o.jsx)("strong",{children:"Explanation:"})," ",s]})]}),m?(0,o.jsx)("button",{onClick:()=>{c(null),d(!1),u(!1)},style:{marginTop:"1rem",padding:"0.5rem 1rem",backgroundColor:"#2196f3",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Try Again"}):l&&(0,o.jsx)("button",{onClick:()=>f(l),style:{marginTop:"1rem",padding:"0.5rem 1rem",backgroundColor:"#4caf50",color:"white",border:"none",borderRadius:"4px",cursor:"pointer"},children:"Submit Answer"})]})}},7639(e,n,t){t.d(n,{A:()=>o});t(6540);var i=t(4848);const o=({title:e,description:n,src:t,alt:o,caption:s,className:a=""})=>(0,i.jsxs)("div",{className:`diagram-component ${a}`,style:{textAlign:"center",margin:"1.5rem 0",padding:"1rem",border:"1px solid #eee",borderRadius:"8px",backgroundColor:"#fafafa"},children:[e&&(0,i.jsx)("h5",{style:{margin:"0 0 1rem 0",color:"#202124",fontSize:"1rem",fontWeight:"bold"},children:e}),(0,i.jsx)("div",{style:{display:"flex",justifyContent:"center",alignItems:"center",margin:"0 auto",maxWidth:"100%"},children:t?(0,i.jsx)("img",{src:t,alt:o||e||"Diagram",style:{maxWidth:"100%",height:"auto",border:"1px solid #ddd",borderRadius:"4px"}}):(0,i.jsx)("div",{style:{width:"100%",height:"200px",display:"flex",alignItems:"center",justifyContent:"center",backgroundColor:"#f5f5f5",border:"2px dashed #ccc",borderRadius:"4px",color:"#666"},children:"Diagram placeholder"})}),(n||s)&&(0,i.jsxs)("div",{style:{marginTop:"0.5rem",fontSize:"0.9rem",color:"#5f6368",textAlign:"left",padding:"0.5rem"},children:[n&&(0,i.jsx)("p",{style:{margin:"0.5rem 0"},children:n}),s&&(0,i.jsxs)("p",{style:{margin:"0.5rem 0",fontStyle:"italic"},children:[(0,i.jsx)("strong",{children:"Figure:"})," ",s]})]})]})},8453(e,n,t){t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const o={},s=i.createContext(o);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(s.Provider,{value:n},e.children)}},8844(e,n,t){t.d(n,{A:()=>o});t(6540);var i=t(4848);const o=({type:e="note",title:n,children:t,className:o=""})=>{const s={note:{borderLeft:"4px solid #4285f4",backgroundColor:"#f0f4ff",color:"#202124"},tip:{borderLeft:"4px solid #34a853",backgroundColor:"#f0f9ff",color:"#202124"},warning:{borderLeft:"4px solid #fbbc04",backgroundColor:"#fef7e0",color:"#202124"},danger:{borderLeft:"4px solid #ea4335",backgroundColor:"#fce8e6",color:"#202124"}},a={note:"\u2139\ufe0f",tip:"\ud83d\udca1",warning:"\u26a0\ufe0f",danger:"\u274c"},r=s[e]||s.note,l=a[e]||a.note;return(0,i.jsx)("div",{className:`callout callout-${e} ${o}`,style:{border:"1px solid",borderRadius:"4px",padding:"1rem",margin:"1rem 0",...r},children:(0,i.jsxs)("div",{style:{display:"flex",alignItems:"flex-start"},children:[(0,i.jsx)("span",{style:{fontSize:"1.2rem",marginRight:"0.5rem"},children:l}),(0,i.jsxs)("div",{children:[n&&(0,i.jsx)("h5",{style:{margin:"0 0 0.5rem 0",fontSize:"1rem",fontWeight:"bold",textTransform:"uppercase",letterSpacing:"0.5px"},children:n}),(0,i.jsx)("div",{children:t})]})]})})}}}]);